{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-section",
   "metadata": {},
   "source": [
    "# Model Quality Metrics\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates advanced evaluation metrics for US cities demographic analysis using LLM-as-a-Judge methodology.\n",
    "\n",
    "## What We'll Cover\n",
    "1. **Programmatic Testing** - Verify model accuracy against ground truth city data\n",
    "2. **LLM as a Judge** - Assess response quality, accuracy, and analytical depth\n",
    "3. **Evaluation Analysis** - Compare performance across question types and complexity levels\n",
    "4. **Results Visualization** - Present findings and recommendations\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with Bedrock access\n",
    "- Python 3.10+\n",
    "- boto3 library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from time import sleep\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, List, Optional, Any\n",
    "import random\n",
    "\n",
    "\n",
    "bedrock = boto3.client(\"bedrock-runtime\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-section",
   "metadata": {},
   "source": [
    "## 1. Load the dataset \n",
    "\n",
    "We will be using the US Cities Population Dataset which contains comprehensive demographic and geographic information for the top 314 most populous cities in the United States. This dataset provides a detailed snapshot of American urban demographics, featuring cities with populations ranging from over 8.4 million (New York City) down to approximately 100,000 residents (Sunrise, FL).\n",
    "\n",
    "**Dataset features:**\n",
    "- **city**: Name of the city\n",
    "- **state**: Two-letter state abbreviation \n",
    "- **population**: Current population count (formatted with commas as strings)\n",
    "- **land_area_mi2**: Land area in square miles\n",
    "\n",
    "**Dataset characteristics:**\n",
    "- **Size**: 314 cities across all 50 US states plus Washington DC\n",
    "- **Population range**: 8,478,072 (New York) to 100,128 (Sunrise, FL)\n",
    "- **Geographic coverage**: Represents major metropolitan areas nationwide\n",
    "- **Data quality**: Some entries contain footnote references that may need cleaning\n",
    "\n",
    "This comprehensive dataset is ideal for analyzing urban demographics, population density patterns, regional growth trends, and the geographic distribution of America's largest cities. The inclusion of both population and land area data enables calculations of population density and comparative urban planning analysis.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c03dfb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (346, 4)\n",
      "Columns: ['city', 'state', 'population', 'land_area_mi2']\n",
      "\n",
      "First 3 rows:\n",
      "\n",
      "--- Row 1 ---\n",
      "City: New York[c]\n",
      "State: NY\n",
      "Population: 8,478,072\n",
      "Land Area (sq mi): 300.5\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Row 2 ---\n",
      "City: Los Angeles\n",
      "State: CA\n",
      "Population: 3,878,704\n",
      "Land Area (sq mi): 469.5\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Row 3 ---\n",
      "City: Chicago\n",
      "State: IL\n",
      "Population: 2,721,308\n",
      "Land Area (sq mi): 227.7\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"./city_pop.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# View first 3 rows with better formatting\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\n--- Row {i+1} ---\")\n",
    "    print(f\"City: {df.iloc[i]['city']}\")\n",
    "    print(f\"State: {df.iloc[i]['state']}\")\n",
    "    print(f\"Population: {df.iloc[i]['population']}\")\n",
    "    print(f\"Land Area (sq mi): {df.iloc[i]['land_area_mi2']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2cd90e",
   "metadata": {},
   "source": [
    "## 2. Programmatic Model Testing\n",
    "\n",
    "Now, we will perform programmatic testing on our dataset. We will ask the model a series of questions about specific cities and verify whether the model's responses match the data we have in our dataset. This approach allows us to systematically evaluate the model's accuracy against our ground truth data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93e496e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define our test questions\n",
    "test_questions = [\n",
    "    \"What is the land area of of New York?\",\n",
    "    \"What is the land area of Los Angeles in square miles?\",\n",
    "    \"What is the population of Chicago?\",\n",
    "    \"What is the land area of Houston?\",\n",
    "    \"Which city has a larger population: Phoenix or New York?\",\n",
    "    \"What is the land area of San Francisco?\",\n",
    "    \"What is the population of Seattle?\",\n",
    "    \"What is the total land area of Boston?\",\n",
    "    \"What is the land area of Las Vegas?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "289a47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bedrock_call(prompt: str, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Make a Bedrock call using Converse API with Nova Micro.\"\"\"\n",
    "    \n",
    "    structured_prompt = f\"\"\"\n",
    "    You will be asked questions about city populations and land areas.\n",
    "    \n",
    "    Answer the following question: {prompt}\n",
    "    \n",
    "    For direct questions about population, respond in this JSON format only:\n",
    "    {{\n",
    "        \"answer\": [numerical answer only, no commas or text],\n",
    "        \"city\": [city name],\n",
    "        \"metric\": \"population\"\n",
    "    }}\n",
    "\n",
    "    For direct questions about land area, respond in this JSON format only:\n",
    "    {{\n",
    "        \"answer\": [numerical answer as decimal, like 46.9],\n",
    "        \"city\": [city name],\n",
    "        \"metric\": \"land_area_mi2\"\n",
    "    }}\n",
    "\n",
    "    For comparison questions, respond in this JSON format only:\n",
    "    {{\n",
    "        \"answer\": [numerical answer for larger city],\n",
    "        \"city\": [name of larger city],\n",
    "        \"metric\": [what was compared],\n",
    "        \"comparison\": true\n",
    "    }}\n",
    "\n",
    "    Respond with the JSON only, no additional text.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = bedrock.converse(\n",
    "        # modelId='amazon.nova-micro-v1:0',\n",
    "        modelId='us.anthropic.claude-sonnet-4-20250514-v1:0',\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': [\n",
    "                    {\n",
    "                        'text': structured_prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        inferenceConfig={\n",
    "            'maxTokens': 300,\n",
    "            'temperature': 0\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response_text = response['output']['message']['content'][0]['text']\n",
    "    return json.loads(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca60abb",
   "metadata": {},
   "source": [
    "Here we will set up a function to programatically verify if the models response based on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ede85a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1/9: What is the land area of of New York?\n",
      "Testing 2/9: What is the land area of Los Angeles in square miles?\n",
      "Testing 3/9: What is the population of Chicago?\n",
      "Testing 4/9: What is the land area of Houston?\n",
      "Testing 5/9: Which city has a larger population: Phoenix or New York?\n",
      "Testing 6/9: What is the land area of San Francisco?\n",
      "Testing 7/9: What is the population of Seattle?\n",
      "Testing 8/9: What is the total land area of Boston?\n",
      "   Throttled, waiting 1.8s before retry...\n",
      "Testing 9/9: What is the land area of Las Vegas?\n",
      "   Throttled, waiting 1.7s before retry...\n",
      "\n",
      "Test Summary:\n",
      "Passed: 3/9\n",
      "Success Rate: 33.33%\n",
      "\n",
      "Detailed Results:\n",
      "\u274c FAIL - What is the land area of of New York?\n",
      "   Response: {\n",
      "  \"answer\": 302.6,\n",
      "  \"city\": \"New York\",\n",
      "  \"metric\": \"land_area_mi2\"\n",
      "}\n",
      "\u274c FAIL - What is the land area of Los Angeles in square miles?\n",
      "   Response: {\n",
      "  \"answer\": 502.7,\n",
      "  \"city\": \"Los Angeles\",\n",
      "  \"metric\": \"land_area_mi2\"\n",
      "}\n",
      "\u274c FAIL - What is the population of Chicago?\n",
      "   Response: {\n",
      "  \"answer\": 2746388,\n",
      "  \"city\": \"Chicago\",\n",
      "  \"metric\": \"population\"\n",
      "}\n",
      "\u274c FAIL - What is the land area of Houston?\n",
      "   Response: {\n",
      "  \"answer\": 670.2,\n",
      "  \"city\": \"Houston\",\n",
      "  \"metric\": \"land_area_mi2\"\n",
      "}\n",
      "\u2705 PASS - Which city has a larger population: Phoenix or New York?\n",
      "\u2705 PASS - What is the land area of San Francisco?\n",
      "\u274c FAIL - What is the population of Seattle?\n",
      "   Response: {\n",
      "  \"answer\": 749256,\n",
      "  \"city\": \"Seattle\",\n",
      "  \"metric\": \"population\"\n",
      "}\n",
      "\u2705 PASS - What is the total land area of Boston?\n",
      "\u274c FAIL - What is the land area of Las Vegas?\n",
      "   Response: {\n",
      "  \"answer\": 135.86,\n",
      "  \"city\": \"Las Vegas\",\n",
      "  \"metric\": \"land_area_mi2\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def verify_answer(response: Dict[str, Any], df: pd.DataFrame, question: str) -> bool:\n",
    "    \"\"\"Verify if the answer matches our dataset.\"\"\"\n",
    "    try:\n",
    "        city = response['city']\n",
    "        metric = response['metric']\n",
    "        \n",
    "        # Handle comparison questions\n",
    "        if response.get('comparison'):\n",
    "            cities = question.split(':')[1].strip().split(' or ')\n",
    "            city1, city2 = [c.strip() for c in cities]\n",
    "            \n",
    "            # Get values and handle both int and float\n",
    "            val1_raw = df[df['city'].str.contains(city1, case=False)][metric].values[0]\n",
    "            val2_raw = df[df['city'].str.contains(city2, case=False)][metric].values[0]\n",
    "            \n",
    "            if isinstance(val1_raw, str):\n",
    "                val1 = float(val1_raw.replace(',', ''))\n",
    "            else:\n",
    "                val1 = float(val1_raw)\n",
    "                \n",
    "            if isinstance(val2_raw, str):\n",
    "                val2 = float(val2_raw.replace(',', ''))\n",
    "            else:\n",
    "                val2 = float(val2_raw)\n",
    "            \n",
    "            expected_city = city1 if val1 > val2 else city2\n",
    "            return city.lower() in expected_city.lower() or expected_city.lower() in city.lower()\n",
    "        \n",
    "        # Handle direct questions - improved city matching\n",
    "        # First try exact match\n",
    "        matching_rows = df[df['city'].str.contains(city, case=False, regex=False)]\n",
    "        \n",
    "        # If no match, try without brackets/footnotes\n",
    "        if len(matching_rows) == 0:\n",
    "            city_clean = city.split('[')[0].strip()  # Remove footnote markers\n",
    "            matching_rows = df[df['city'].str.contains(city_clean, case=False, regex=False)]\n",
    "        \n",
    "        # If still no match, try the other way around (dataset city contains response city)\n",
    "        if len(matching_rows) == 0:\n",
    "            for idx, row in df.iterrows():\n",
    "                dataset_city_clean = row['city'].split('[')[0].strip()\n",
    "                if city.lower() in dataset_city_clean.lower() or dataset_city_clean.lower() in city.lower():\n",
    "                    matching_rows = df.iloc[[idx]]\n",
    "                    break\n",
    "        \n",
    "        if len(matching_rows) == 0:\n",
    "            print(f\"No match found for city: '{city}'\")\n",
    "            return False\n",
    "            \n",
    "        actual_value = matching_rows[metric].values[0]\n",
    "        \n",
    "        # Handle population (integer) vs land_area (float)\n",
    "        if metric == 'population':\n",
    "            if isinstance(actual_value, str):\n",
    "                actual_value = int(actual_value.replace(',', ''))\n",
    "            answer = int(response['answer'])\n",
    "            return answer == actual_value\n",
    "        else:  # land_area_mi2\n",
    "            if isinstance(actual_value, str):\n",
    "                actual_value = float(actual_value.replace(',', ''))\n",
    "            answer = float(response['answer'])\n",
    "            return abs(answer - actual_value) < 0.1  # Allow small floating point differences\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Verification error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def run_tests(questions: List[str], df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Run all test questions and collect results.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"Testing {i+1}/{len(questions)}: {question}\")\n",
    "        \n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = bedrock_call(question, df)\n",
    "                is_correct = verify_answer(response, df, question)\n",
    "                \n",
    "                results.append({\n",
    "                    \"question\": question,\n",
    "                    \"response\": response,\n",
    "                    \"passed\": is_correct\n",
    "                })\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"ThrottlingException\" in str(e) and attempt < max_retries - 1:\n",
    "                    wait_time = (2 ** attempt) + random.uniform(0, 1)  # Exponential backoff\n",
    "                    print(f\"   Throttled, waiting {wait_time:.1f}s before retry...\")\n",
    "                    sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"   Error: {str(e)}\")\n",
    "                    results.append({\n",
    "                        \"question\": question,\n",
    "                        \"error\": str(e),\n",
    "                        \"passed\": False\n",
    "                    })\n",
    "                    break\n",
    "        \n",
    "        # Base delay between requests\n",
    "        sleep(2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"./city_pop.csv\")\n",
    "\n",
    "# Run tests\n",
    "test_results = run_tests(test_questions, df)\n",
    "\n",
    "# Save results\n",
    "with open(\"test_results.json\", 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "passed_tests = sum(1 for result in test_results if result['passed'])\n",
    "print(f\"\\nTest Summary:\")\n",
    "print(f\"Passed: {passed_tests}/{len(test_questions)}\")\n",
    "print(f\"Success Rate: {(passed_tests/len(test_questions))*100:.2f}%\")\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nDetailed Results:\")\n",
    "for result in test_results:\n",
    "    status = \"\u2705 PASS\" if result['passed'] else \"\u274c FAIL\"\n",
    "    print(f\"{status} - {result['question']}\")\n",
    "    if 'error' in result:\n",
    "        print(f\"   Error: {result['error']}\")\n",
    "    elif not result['passed']:\n",
    "        print(f\"   Response: {json.dumps(result['response'], indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judge-prompt-section",
   "metadata": {},
   "source": [
    "## 2. LLM as A Judge Evaluation \n",
    "\n",
    "LLM as a Judge is an evaluation methodology where we use a powerful language model (often the same or different LLM) to act as an automated evaluator. Instead of human judges, the LLM analyzes AI responses against specific criteria, providing detailed feedback, scores, and reasoning for its assessments. \n",
    "\n",
    "This approach has gained significant traction in both research and industry applications due to its ability to provide consistent, scalable, and nuanced evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e248a238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response generation function loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#To begin we will ask our model questions based on the same dataset\n",
    "\n",
    "def generate_model_response(question: str, context_data: str = \"\") -> str:\n",
    "    \"\"\"Generate a model response to a cities question using Bedrock.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant with knowledge about US cities demographics. Answer the following question about US cities based on your knowledge.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    {f\"Context data from dataset: {context_data}\" if context_data else \"\"}\n",
    "    \n",
    "    Provide a clear, informative response. If the question involves calculations (like population density), show your work.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = bedrock.converse(\n",
    "            modelId='us.anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': [{'text': prompt}]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                'maxTokens': 500,\n",
    "                'temperature': 0.1\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response['output']['message']['content'][0]['text']\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "print(\"Model response generation function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3925bfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model responses...\n",
      "Generating response 1/6: What is the population of New York City?\n",
      "Generating response 2/6: Which city has a larger population: Los Angeles or Chicago?\n",
      "Generating response 3/6: What is the population density of San Francisco?\n",
      "Generating response 4/6: What is the land area of Houston in square miles?\n",
      "Generating response 5/6: List the top 3 most populous cities in the United States.\n",
      "Generating response 6/6: Calculate the population density of Chicago.\n",
      "\n",
      "Generated 6 model responses for evaluation\n",
      "\n",
      "Sample generated responses:\n",
      "\n",
      "--- Response 1 ---\n",
      "Question: What is the population of New York City?\n",
      "Model Response: The population of New York City is 8,478,072 according to the provided data....\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Response 2 ---\n",
      "Question: Which city has a larger population: Los Angeles or Chicago?\n",
      "Model Response: Based on the context data provided, Los Angeles has a larger population than Chicago. Los Angeles has a population of 3,878,704 people, while Chicago has a population of 2,721,308 people. The differen...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define test questions for cities evaluation\n",
    "cities_questions = [\n",
    "    {\n",
    "        'question': 'What is the population of New York City?',\n",
    "        'context': 'New York[c], NY: population=8,478,072, land_area_mi2=300.5'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Which city has a larger population: Los Angeles or Chicago?',\n",
    "        'context': 'Los Angeles, CA: population=3,878,704, land_area_mi2=469.5\\nChicago, IL: population=2,721,308, land_area_mi2=227.7'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the population density of San Francisco?',\n",
    "        'context': 'San Francisco, CA: population=815,201, land_area_mi2=46.9'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the land area of Houston in square miles?',\n",
    "        'context': 'Houston, TX: population=2,304,580, land_area_mi2=670.2'\n",
    "    },\n",
    "    {\n",
    "        'question': 'List the top 3 most populous cities in the United States.',\n",
    "        'context': 'Top cities: New York (8,478,072), Los Angeles (3,878,704), Chicago (2,721,308)'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Calculate the population density of Chicago.',\n",
    "        'context': 'Chicago, IL: population=2,721,308, land_area_mi2=227.7'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Generate model responses\n",
    "print(\"Generating model responses...\")\n",
    "cities_responses = []\n",
    "\n",
    "for i, item in enumerate(cities_questions):\n",
    "    print(f\"Generating response {i+1}/{len(cities_questions)}: {item['question']}\")\n",
    "    \n",
    "    # Generate model response\n",
    "    model_response = generate_model_response(item['question'], item['context'])\n",
    "    \n",
    "    # Create response data structure\n",
    "    response_data = {\n",
    "        'question': item['question'],\n",
    "        'model_response': model_response,\n",
    "        'context': item['context']\n",
    "    }\n",
    "    \n",
    "    cities_responses.append(response_data)\n",
    "    \n",
    "    # Add delay to avoid rate limiting\n",
    "    sleep(1)\n",
    "\n",
    "print(f\"\\nGenerated {len(cities_responses)} model responses for evaluation\")\n",
    "\n",
    "# Display sample responses\n",
    "print(\"\\nSample generated responses:\")\n",
    "for i, response in enumerate(cities_responses[:2]):\n",
    "    print(f\"\\n--- Response {i+1} ---\")\n",
    "    print(f\"Question: {response['question']}\")\n",
    "    print(f\"Model Response: {response['model_response'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4a3847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge prompt template loaded successfully\n",
      "\n",
      "Judge prompt preview:\n",
      "\n",
      "You will be given a question about US cities demographics and population data. \n",
      "Your task is to evaluate a model's response for accuracy, completeness, and analytical quality.\n",
      "\n",
      "Here is the question about US cities:\n",
      "<question>{QUESTION}</question>\n",
      "\n",
      "Here is the model's response:\n",
      "<model_response>{MODE...\n"
     ]
    }
   ],
   "source": [
    "#Load and Configure Judge Prompt\n",
    "judge_prompt_template = \"\"\"\n",
    "You will be given a question about US cities demographics and population data. \n",
    "Your task is to evaluate a model's response for accuracy, completeness, and analytical quality.\n",
    "\n",
    "Here is the question about US cities:\n",
    "<question>{QUESTION}</question>\n",
    "\n",
    "Here is the model's response:\n",
    "<model_response>{MODEL_RESPONSE}</model_response>\n",
    "\n",
    "Here is the context from the data:\n",
    "<dataset>{context}</dataset>\n",
    "\n",
    "**Dataset Context:** The response should be based on the US Cities Population Dataset containing 314 most populous US cities with the following features:\n",
    "- **city**: City name\n",
    "- **state**: Two-letter state abbreviation  \n",
    "- **population**: Population count (may include commas/formatting)\n",
    "- **land_area_mi2**: Land area in square miles\n",
    "- **Coverage**: Cities from 8.4M+ (NYC) down to ~100K residents\n",
    "\n",
    "First, analyze the question type and evaluate the model response based on:\n",
    "\n",
    "1. **Data Accuracy**: Are population figures, city names, and geographic information correct?\n",
    "2. **Calculation Correctness**: If calculations are involved (density, rankings, comparisons), are they mathematically sound?\n",
    "3. **Geographic Knowledge**: Does the response demonstrate proper understanding of US geography and state locations?\n",
    "4. **Analytical Depth**: For complex queries, does the response provide meaningful insights beyond basic data retrieval?\n",
    "5. **Data Handling**: Does the response appropriately handle data formatting issues (commas in numbers, footnotes, etc.)?\n",
    "\n",
    "Then, classify the question type:\n",
    "1. **Factual Lookup**: Simple data retrieval (population of specific city)\n",
    "2. **Ranking/Comparison**: Ordering cities by metrics or comparing multiple cities\n",
    "3. **Calculation-Based**: Requires mathematical operations (density, growth rates, etc.)\n",
    "4. **Geographic Analysis**: Regional patterns, state-level analysis, geographic distribution\n",
    "5. **Trend Analysis**: Population patterns, urban development insights\n",
    "\n",
    "Provide your evaluation in the following format:\n",
    "\n",
    "<analysis>\n",
    "[Your detailed analysis of the response quality, noting any factual errors, missing information, or analytical strengths/weaknesses]\n",
    "</analysis>\n",
    "\n",
    "<question_type>factual_lookup/ranking_comparison/calculation_based/geographic_analysis/trend_analysis</question_type>\n",
    "\n",
    "<complexity>Basic/Intermediate/Advanced</complexity>\n",
    "\n",
    "<score>X/10</score>\n",
    "\n",
    "<reasoning>\n",
    "[Explanation for the score based on accuracy, completeness, analytical quality, and appropriate handling of the dataset characteristics]\n",
    "</reasoning>\n",
    "\n",
    "<improvements>\n",
    "[Specific suggestions for how the response could be enhanced, if applicable]\n",
    "</improvements>\n",
    "\"\"\"\n",
    "\n",
    "print(\"Judge prompt template loaded successfully\")\n",
    "print(f\"\\nJudge prompt preview:\")\n",
    "print(judge_prompt_template[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-setup-section",
   "metadata": {},
   "source": [
    "## 3. Model-as-a-Judge Evaluation Setup\n",
    "\n",
    "Configure the evaluation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "judge-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated build_judge_prompt function loaded\n",
      "Judge model: us.anthropic.claude-3-7-sonnet-20250219-v1:0\n"
     ]
    }
   ],
   "source": [
    "# Judge model configuration\n",
    "JUDGE_MODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "\n",
    "def build_judge_prompt(question: str, model_response: str, context: str = \"\") -> str:\n",
    "    \"\"\"Build the judge prompt for evaluating US cities demographic analysis responses.\"\"\"\n",
    "    \n",
    "    # Replace placeholders in the judge prompt template\n",
    "    formatted_prompt = judge_prompt_template.replace(\"{QUESTION}\", question)\n",
    "    formatted_prompt = formatted_prompt.replace(\"{MODEL_RESPONSE}\", model_response)\n",
    "    formatted_prompt = formatted_prompt.replace(\"{context}\", context)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "print(\"Updated build_judge_prompt function loaded\")\n",
    "\n",
    "\n",
    "def call_judge_model(prompt: str) -> str:\n",
    "    \"\"\"Call the judge model to evaluate a response using boto3 directly.\"\"\"\n",
    "    try:\n",
    "        response = bedrock.converse(\n",
    "            modelId=JUDGE_MODEL_ID,\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "            inferenceConfig={\"temperature\": 0.1, \"maxTokens\": 1000}\n",
    "        )\n",
    "        \n",
    "        return response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def call_threaded_evaluation(prompts: List[str], max_workers=3) -> List[str]:\n",
    "    \"\"\"Process evaluation requests in parallel using boto3.\"\"\"\n",
    "    future_to_position = {}\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            future = executor.submit(call_judge_model, prompt)\n",
    "            future_to_position[future] = i\n",
    "        \n",
    "        responses = [None] * len(prompts)\n",
    "        \n",
    "        for future in as_completed(future_to_position):\n",
    "            position = future_to_position[future]\n",
    "            try:\n",
    "                response = future.result()\n",
    "                responses[position] = response\n",
    "            except Exception as exc:\n",
    "                print(f\"Request at position {position} generated an exception: {exc}\")\n",
    "                responses[position] = f\"Error: {str(exc)}\"\n",
    "        \n",
    "    return responses\n",
    "\n",
    "print(f\"Judge model: {JUDGE_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-evaluation-section",
   "metadata": {},
   "source": [
    "## 4. Run LLM-as-a-Judge Evaluation\n",
    "\n",
    "Evaluate the responses using the judge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "run-evaluation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation of 6 responses...\n",
      "Running evaluations (this may take a few minutes)...\n",
      "Completed 6 evaluations\n"
     ]
    }
   ],
   "source": [
    "if not cities_responses:\n",
    "    print(\"No cities responses to evaluate. Please run the model response generation cell first.\")\n",
    "else:\n",
    "    print(f\"Starting evaluation of {len(cities_responses)} responses...\")\n",
    "    \n",
    "    # Prepare evaluation prompts\n",
    "    evaluation_prompts = []\n",
    "    \n",
    "    for response_data in cities_responses:\n",
    "        # Extract the question and response\n",
    "        question = response_data['question']\n",
    "        model_response = response_data['model_response']\n",
    "        \n",
    "        # Extract relevant context data if available\n",
    "        context = response_data.get('context', '')\n",
    "        \n",
    "        judge_prompt = build_judge_prompt(\n",
    "            question=question,\n",
    "            model_response=model_response,\n",
    "            context=context\n",
    "        )\n",
    "        \n",
    "        evaluation_prompts.append(judge_prompt)\n",
    "    \n",
    "    # Run evaluations in parallel\n",
    "    print(\"Running evaluations (this may take a few minutes)...\")\n",
    "    evaluation_results = call_threaded_evaluation(evaluation_prompts)\n",
    "    \n",
    "    print(f\"Completed {len(evaluation_results)} evaluations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parse-results-section",
   "metadata": {},
   "source": [
    "## 5. Parse and Analyze Results\n",
    "\n",
    "Extract structured information from the judge evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "parse-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 6 evaluations\n",
      "\n",
      "Evaluation DataFrame created with columns:\n",
      "['question', 'model_response', 'context', 'evaluation_text', 'analysis', 'question_type', 'complexity', 'score', 'reasoning', 'improvements', 'numeric_score']\n",
      "\n",
      "Evaluation Summary:\n",
      "Average Score: 10.00\n",
      "Score Range: 10.0 - 10.0\n",
      "\n",
      "Question Type Distribution:\n",
      "question_type\n",
      "factual_lookup        2\n",
      "ranking_comparison    2\n",
      "calculation_based     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Complexity Distribution:\n",
      "complexity\n",
      "Basic    6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def extract_evaluation_components(evaluation_text: str) -> Dict:\n",
    "    \"\"\"Extract structured components from judge evaluation.\"\"\"\n",
    "    \n",
    "    # Regex patterns for extracting components - updated for cities evaluation format\n",
    "    patterns = {\n",
    "        'analysis': r'<analysis>(.*?)</analysis>',\n",
    "        'question_type': r'<question_type>(.*?)</question_type>',\n",
    "        'complexity': r'<complexity>(.*?)</complexity>',\n",
    "        'score': r'<score>(.*?)</score>',\n",
    "        'reasoning': r'<reasoning>(.*?)</reasoning>',\n",
    "        'improvements': r'<improvements>(.*?)</improvements>'\n",
    "    }\n",
    "    \n",
    "    extracted = {}\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, evaluation_text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            extracted[key] = match.group(1).strip()\n",
    "        else:\n",
    "            extracted[key] = None\n",
    "    \n",
    "    # Extract numeric score\n",
    "    if extracted['score']:\n",
    "        score_match = re.search(r'(\\d+(?:\\.\\d+)?)', extracted['score'])\n",
    "        if score_match:\n",
    "            try:\n",
    "                extracted['numeric_score'] = float(score_match.group(1))\n",
    "            except ValueError:\n",
    "                extracted['numeric_score'] = None\n",
    "        else:\n",
    "            extracted['numeric_score'] = None\n",
    "    else:\n",
    "        extracted['numeric_score'] = None\n",
    "    \n",
    "    return extracted\n",
    "\n",
    "if 'evaluation_results' in locals() and evaluation_results:\n",
    "    # Parse all evaluation results\n",
    "    parsed_evaluations = []\n",
    "    \n",
    "    for i, (response_data, evaluation_text) in enumerate(zip(cities_responses, evaluation_results)):\n",
    "        if not evaluation_text.startswith(\"Error:\"):\n",
    "            parsed_eval = extract_evaluation_components(evaluation_text)\n",
    "            \n",
    "            # Combine with original response data\n",
    "            combined_result = {\n",
    "                **response_data,\n",
    "                'evaluation_text': evaluation_text,\n",
    "                **parsed_eval\n",
    "            }\n",
    "            \n",
    "            parsed_evaluations.append(combined_result)\n",
    "        else:\n",
    "            print(f\"Evaluation error for response {i}: {evaluation_text}\")\n",
    "    \n",
    "    print(f\"Successfully parsed {len(parsed_evaluations)} evaluations\")\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df_evaluations = pd.DataFrame(parsed_evaluations)\n",
    "    \n",
    "    print(\"\\nEvaluation DataFrame created with columns:\")\n",
    "    print(list(df_evaluations.columns))\n",
    "    \n",
    "    # Display summary statistics for cities evaluation\n",
    "    if not df_evaluations.empty:\n",
    "        print(f\"\\nEvaluation Summary:\")\n",
    "        print(f\"Average Score: {df_evaluations['numeric_score'].mean():.2f}\")\n",
    "        print(f\"Score Range: {df_evaluations['numeric_score'].min():.1f} - {df_evaluations['numeric_score'].max():.1f}\")\n",
    "        \n",
    "        print(f\"\\nQuestion Type Distribution:\")\n",
    "        print(df_evaluations['question_type'].value_counts())\n",
    "        \n",
    "        print(f\"\\nComplexity Distribution:\")\n",
    "        print(df_evaluations['complexity'].value_counts())\n",
    "        \n",
    "else:\n",
    "    print(\"No evaluation results to parse.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "view-metrics-section",
   "metadata": {},
   "source": [
    "## 6. View Detailed Metrics Table\n",
    "\n",
    "Load and display the saved CSV file with all evaluation metrics for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "624252b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cities evaluation results...\n",
      "Detailed results saved to: cities_evaluation_results.json\n",
      "Summary CSV saved to: cities_evaluation_summary.csv\n",
      "\n",
      "Evaluation complete! 6 results processed.\n",
      "Run the next cell to view the detailed metrics table.\n"
     ]
    }
   ],
   "source": [
    "# Save cities evaluation results to files\n",
    "if 'df_evaluations' in locals() and not df_evaluations.empty:\n",
    "    print(\"Saving cities evaluation results...\")\n",
    "    \n",
    "    # Save detailed results to JSON\n",
    "    output_file = \"cities_evaluation_results.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(parsed_evaluations, f, indent=2, default=str)\n",
    "    print(f\"Detailed results saved to: {output_file}\")\n",
    "    \n",
    "    # Save summary CSV\n",
    "    summary_columns = ['question', 'numeric_score', 'question_type', 'complexity', \n",
    "                      'analysis', 'reasoning', 'improvements']\n",
    "    \n",
    "    available_columns = [col for col in summary_columns if col in df_evaluations.columns]\n",
    "    if available_columns:\n",
    "        summary_df = df_evaluations[available_columns]\n",
    "        summary_df.to_csv(\"cities_evaluation_summary.csv\", index=False)\n",
    "        print(f\"Summary CSV saved to: cities_evaluation_summary.csv\")\n",
    "    \n",
    "    print(f\"\\nEvaluation complete! {len(df_evaluations)} results processed.\")\n",
    "    print(\"Run the next cell to view the detailed metrics table.\")\n",
    "    \n",
    "else:\n",
    "    print(\"No evaluation data available to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "view-csv-metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metrics from: cities_evaluation_summary.csv\n",
      "\n",
      "CITIES EVALUATION METRICS TABLE (6 records)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>numeric_score</th>\n",
       "      <th>question_type</th>\n",
       "      <th>complexity</th>\n",
       "      <th>analysis</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>improvements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the population of New York City?</td>\n",
       "      <td>10.0</td>\n",
       "      <td>factual_lookup</td>\n",
       "      <td>Basic</td>\n",
       "      <td>The model's response is factually accurate. It...</td>\n",
       "      <td>The model deserves a perfect score because:\\n1...</td>\n",
       "      <td>The response is appropriate as is for this bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which city has a larger population: Los Angele...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>ranking_comparison</td>\n",
       "      <td>Basic</td>\n",
       "      <td>The model's response is factually accurate and...</td>\n",
       "      <td>The model deserves a perfect score because it:...</td>\n",
       "      <td>No significant improvements needed. The respon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the population density of San Francisco?</td>\n",
       "      <td>10.0</td>\n",
       "      <td>calculation_based</td>\n",
       "      <td>Basic</td>\n",
       "      <td>The model's response correctly calculates the ...</td>\n",
       "      <td>The model response deserves a perfect score be...</td>\n",
       "      <td>The response is comprehensive and accurate as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the land area of Houston in square miles?</td>\n",
       "      <td>10.0</td>\n",
       "      <td>factual_lookup</td>\n",
       "      <td>Basic</td>\n",
       "      <td>The model's response is factually accurate. It...</td>\n",
       "      <td>The model deserves a perfect score because:\\n1...</td>\n",
       "      <td>No improvements needed. The response is factua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>List the top 3 most populous cities in the Uni...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>ranking_comparison</td>\n",
       "      <td>Basic</td>\n",
       "      <td>The model's response is factually accurate and...</td>\n",
       "      <td>The model deserves a perfect score because:\\n1...</td>\n",
       "      <td>The response is already excellent and complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Calculate the population density of Chicago.</td>\n",
       "      <td>10.0</td>\n",
       "      <td>calculation_based</td>\n",
       "      <td>Basic</td>\n",
       "      <td>The model's response correctly calculates the ...</td>\n",
       "      <td>The model's response deserves a perfect score ...</td>\n",
       "      <td>While the response is already excellent, poten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  numeric_score  \\\n",
       "0           What is the population of New York City?           10.0   \n",
       "1  Which city has a larger population: Los Angele...           10.0   \n",
       "2   What is the population density of San Francisco?           10.0   \n",
       "3  What is the land area of Houston in square miles?           10.0   \n",
       "4  List the top 3 most populous cities in the Uni...           10.0   \n",
       "5       Calculate the population density of Chicago.           10.0   \n",
       "\n",
       "        question_type complexity  \\\n",
       "0      factual_lookup      Basic   \n",
       "1  ranking_comparison      Basic   \n",
       "2   calculation_based      Basic   \n",
       "3      factual_lookup      Basic   \n",
       "4  ranking_comparison      Basic   \n",
       "5   calculation_based      Basic   \n",
       "\n",
       "                                            analysis  \\\n",
       "0  The model's response is factually accurate. It...   \n",
       "1  The model's response is factually accurate and...   \n",
       "2  The model's response correctly calculates the ...   \n",
       "3  The model's response is factually accurate. It...   \n",
       "4  The model's response is factually accurate and...   \n",
       "5  The model's response correctly calculates the ...   \n",
       "\n",
       "                                           reasoning  \\\n",
       "0  The model deserves a perfect score because:\\n1...   \n",
       "1  The model deserves a perfect score because it:...   \n",
       "2  The model response deserves a perfect score be...   \n",
       "3  The model deserves a perfect score because:\\n1...   \n",
       "4  The model deserves a perfect score because:\\n1...   \n",
       "5  The model's response deserves a perfect score ...   \n",
       "\n",
       "                                        improvements  \n",
       "0  The response is appropriate as is for this bas...  \n",
       "1  No significant improvements needed. The respon...  \n",
       "2  The response is comprehensive and accurate as ...  \n",
       "3  No improvements needed. The response is factua...  \n",
       "4  The response is already excellent and complete...  \n",
       "5  While the response is already excellent, poten...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CITIES EVALUATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "SCORE STATISTICS:\n",
      "  Average Score: 10.00/10\n",
      "  Median Score: 10.00/10\n",
      "  Score Range: 10.0 - 10.0\n",
      "\n",
      "QUESTION TYPE PERFORMANCE:\n",
      "  Calculation Based: 10.00 avg (2 questions)\n",
      "  Factual Lookup: 10.00 avg (2 questions)\n",
      "  Ranking Comparison: 10.00 avg (2 questions)\n",
      "\n",
      "COMPLEXITY PERFORMANCE:\n",
      "  Basic: 10.00 avg (6 questions)\n"
     ]
    }
   ],
   "source": [
    "# Load and display the CSV metrics file\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_file = \"cities_evaluation_summary.csv\"\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    print(f\"Loading metrics from: {csv_file}\")\n",
    "    \n",
    "    # Load the CSV file\n",
    "    metrics_df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(f\"\\nCITIES EVALUATION METRICS TABLE ({len(metrics_df)} records)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Configure pandas display options for better viewing\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.width', None)\n",
    "    pd.set_option('display.max_colwidth', 50)\n",
    "    \n",
    "    # Display the dataframe\n",
    "    display(metrics_df)\n",
    "    \n",
    "    # Display additional cities-specific analysis\n",
    "    if not metrics_df.empty and 'numeric_score' in metrics_df.columns:\n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(\"CITIES EVALUATION ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(f\"\\nSCORE STATISTICS:\")\n",
    "        print(f\"  Average Score: {metrics_df['numeric_score'].mean():.2f}/10\")\n",
    "        print(f\"  Median Score: {metrics_df['numeric_score'].median():.2f}/10\")\n",
    "        print(f\"  Score Range: {metrics_df['numeric_score'].min():.1f} - {metrics_df['numeric_score'].max():.1f}\")\n",
    "        \n",
    "        if 'question_type' in metrics_df.columns:\n",
    "            print(f\"\\nQUESTION TYPE PERFORMANCE:\")\n",
    "            type_stats = metrics_df.groupby('question_type')['numeric_score'].agg(['mean', 'count'])\n",
    "            for question_type, stats in type_stats.iterrows():\n",
    "                print(f\"  {question_type.replace('_', ' ').title()}: {stats['mean']:.2f} avg ({int(stats['count'])} questions)\")\n",
    "        \n",
    "        if 'complexity' in metrics_df.columns:\n",
    "            print(f\"\\nCOMPLEXITY PERFORMANCE:\")\n",
    "            complexity_stats = metrics_df.groupby('complexity')['numeric_score'].agg(['mean', 'count'])\n",
    "            for complexity, stats in complexity_stats.iterrows():\n",
    "                print(f\"  {complexity}: {stats['mean']:.2f} avg ({int(stats['count'])} questions)\")\n",
    "    \n",
    "else:\n",
    "    print(f\"CSV file not found: {csv_file}\")\n",
    "    print(\"Please run the cities evaluation cells first to generate the metrics file.\")\n",
    "    print(\"\\nExpected workflow:\")\n",
    "    print(\"1. Generate model responses to cities questions\")\n",
    "    print(\"2. Run LLM-as-a-Judge evaluation\")\n",
    "    print(\"3. Parse and save evaluation results\")\n",
    "    print(\"4. View this metrics summary\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated advanced evaluation metrics for US cities demographic analysis using various quality metrics. Key achievements:\n",
    "\n",
    "- **Comprehensive Evaluation**: Used structured rubric to assess data accuracy, calculation correctness, and analytical depth\n",
    "- **Multi-faceted Testing**: Combined programmatic testing with qualitative judge evaluation\n",
    "- **Scalable Framework**: Created reusable evaluation pipeline for demographic data analysis\n",
    "- **Performance Insights**: Identified model strengths across different question types and complexity levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e655bd5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}