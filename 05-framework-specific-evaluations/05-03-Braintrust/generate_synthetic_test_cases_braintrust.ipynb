{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26059313-4e61-46a5-b9b4-9ede333453e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting braintrust\n",
      "  Downloading braintrust-0.2.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting autoevals\n",
      "  Downloading autoevals-0.0.129-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting GitPython (from braintrust)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from braintrust) (2.32.3)\n",
      "Collecting chevron (from braintrust)\n",
      "  Downloading chevron-0.14.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from braintrust) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.2.0 in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from braintrust) (1.2.2)\n",
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from braintrust) (1.1.1)\n",
      "Collecting sseclient-py (from braintrust)\n",
      "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting python-slugify (from braintrust)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.0 in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from braintrust) (4.12.2)\n",
      "Collecting polyleven (from autoevals)\n",
      "  Downloading polyleven-0.9.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from autoevals) (6.0.2)\n",
      "Collecting braintrust_core==0.0.59 (from autoevals)\n",
      "  Downloading braintrust_core-0.0.59-py3-none-any.whl.metadata (669 bytes)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from autoevals) (4.25.1)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython->braintrust)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from jsonschema->autoevals) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from jsonschema->autoevals) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from jsonschema->autoevals) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from jsonschema->autoevals) (0.27.0)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->braintrust)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from requests->braintrust) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from requests->braintrust) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from requests->braintrust) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytoch_3_12_5/lib/python3.12/site-packages (from requests->braintrust) (2024.12.14)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->braintrust)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading braintrust-0.2.4-py3-none-any.whl (185 kB)\n",
      "Downloading autoevals-0.0.129-py3-none-any.whl (53 kB)\n",
      "Downloading braintrust_core-0.0.59-py3-none-any.whl (4.4 kB)\n",
      "Downloading chevron-0.14.0-py3-none-any.whl (11 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading polyleven-0.9.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: text-unidecode, sseclient-py, chevron, smmap, python-slugify, polyleven, braintrust_core, gitdb, GitPython, braintrust, autoevals\n",
      "Successfully installed GitPython-3.1.45 autoevals-0.0.129 braintrust-0.2.4 braintrust_core-0.0.59 chevron-0.14.0 gitdb-4.0.12 polyleven-0.9.0 python-slugify-8.0.4 smmap-5.0.2 sseclient-py-1.8.0 text-unidecode-1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install braintrust autoevals pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f86e263-dccd-44cc-a9be-b93e52aa2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import braintrust as bt\n",
    "from braintrust import wrap_litellm\n",
    "from litellm import completion\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import json\n",
    "load_dotenv(override=True)\n",
    "MODEL_NAME=\"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38d3ec80-bad9-4442-bfb7-5323f131265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BT_PROJECT_NAME = \"customer-service-agent-eval\"\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "bt_project = bt.projects.create(name=BT_PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff42da-3cba-4fc4-93b4-2dc8677fdaa0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Understanding the Problem\n",
    "\n",
    "## Instructions:\n",
    "1. **Read the objective carefully**: We need ~100 diverse, realistic customer support requests\n",
    "2. **Key insight**: \"Naively generated queries tend to be generic, repetitive, and fail to capture real usage patterns\"\n",
    "3. **Solution**: Use a systematic approach with defined dimensions\n",
    "\n",
    "## Workshop Discussion Points:\n",
    "- Why is synthetic data important when real data isn't available?\n",
    "- What makes a query \"realistic\" vs \"generic\"?\n",
    "- How can we ensure diversity in our generated data?\n",
    "\n",
    "# Define Evaluation Dimensions\n",
    "\n",
    "A dimension is a way to categorize different parts of a user query. Each dimension represents ONE axis of variation. In our example customer service chatbot.\n",
    "\n",
    "Feature: What task or enquiry the user wants to perform e.g order cancelation\n",
    "Persona: What type of client e.g first time buyer, existing buyer\n",
    "scenario: How clear is the intent specified from the user e.g concise or verbose\n",
    "\n",
    "## Instructions:\n",
    "1. **Study each dimension carefully**:\n",
    "   - **Intent**: What the user wants to accomplish\n",
    "   - **Complexity**: How difficult the query is to handle\n",
    "   - **Persona**: What type of user is making the request\n",
    "   - **Language Style**: How the user communicates\n",
    "\n",
    "2. **Critical principle**: \"Choose dimensions that describe where the AI application is likely to fail\"\n",
    "\n",
    "3. **Review the Pydantic models**: Notice how we structure our data for validation\n",
    "\n",
    "## Workshop Activity:\n",
    "What other dimensions might be relevant for your specific use case?\n",
    "\n",
    "# Dimension Tuple Generation Function\n",
    "\n",
    "## Instructions:\n",
    "1. **Examine the prompt structure**: Notice how we:\n",
    "   - Provide clear instructions for balanced coverage\n",
    "   - Include realistic constraints (e.g., new customers rarely have returns)\n",
    "   - Request specific numbers of combinations\n",
    "\n",
    "2. **Understand the parallel processing**: We make multiple calls and deduplicate results\n",
    "\n",
    "3. **Run this cell**: It defines the function but doesn't execute it yet\n",
    "\n",
    "## Key Concept:\n",
    "Good prompt engineering includes constraints and examples to guide the LLM toward realistic outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ffacc38-b649-47b4-81f9-35b7f9cea728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define Pydantic Models for structured Output\n",
    "\n",
    "class DimensionTuple(BaseModel):\n",
    "    intent: str = Field(\n",
    "        description=\"The user's primary goal or task (e.g., product_inquiry, order_status_check, return_request, technical_support, account_management, general_info).\"\n",
    "    )\n",
    "    complexity: str = Field(\n",
    "        description=\"The difficulty and structure of the query (e.g., simple, multi-turn, ambiguous).\"\n",
    "    )\n",
    "    persona: str = Field(\n",
    "        description=\"The type of user based on their behavior or relationship with the store (e.g., new_customer, repeat_customer, frustrated_customer, loyalty_member).\"\n",
    "    )\n",
    "    language_style: str = Field(\n",
    "        description=\"Linguistic characteristics of the query (e.g., formal, informal, contains_slang, includes_typos, verbose, concise).\"\n",
    "    )\n",
    "\n",
    "class DimensionTuples(BaseModel):\n",
    "    tuples: List[DimensionTuple]\n",
    "\n",
    "class DimensionTuplesList(BaseModel):\n",
    "    tuples: List[DimensionTuple]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c30dedae-3335-4e87-b802-974bc4a69e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUPLES_GEN_PROMPT=\"\"\"\\\n",
    "        I am designing a customer support chatbot for a retail company and need to generate a diverse set of synthetic test data to evaluate its performance. I've provided you with the key dimensions that make up a customer's inquiry, along with a list of possible values for each.\n",
    "        \n",
    "        ## Instructions\n",
    "        \n",
    "        Generate {{{num_tuples_to_generate}}} unique combinations of dimension values based on the dimensions provided below.\n",
    "        \n",
    "        * Each combination should represent a distinct customer support scenario.\n",
    "        * Ensure **balanced coverage** across all dimensions; avoid over-representing any single value or combination.\n",
    "        * The generated tuples should be as realistic and varied as possible. For example, a frustrated customer is likely to use informal language and ask a complex question about a return.\n",
    "        * Never generate a tuple where the persona is 'new_customer' and the intent is 'return_request' or 'order_status_check' unless the complexity is multi-turn to simulate a scenario where they are new to this process.\n",
    "        \n",
    "        ## Dimensions\n",
    "        \n",
    "        * **intent**: What kind of inquiry are they making?\n",
    "            * product_inquiry\n",
    "            * order_status_check\n",
    "            * request_for_action_or_service\n",
    "            * return_request\n",
    "            * cancel_order\n",
    "            * technical_support\n",
    "            * account_management\n",
    "            * general_info\n",
    "        * **complexity**: The difficulty and structure of the query.\n",
    "            * simple\n",
    "            * multi-turn\n",
    "            * ambiguous\n",
    "        * **persona**: The type of user making the request\n",
    "            * new_customer\n",
    "            * repeat_customer\n",
    "            * frustrated_customer\n",
    "            * loyalty_member\n",
    "        * **language_style**: The linguistic characteristics of the query\n",
    "            * formal\n",
    "            * informal\n",
    "            * contains_slang\n",
    "            * includes_typos\n",
    "        \n",
    "        Generate {{{num_tuples_to_generate}}} unique dimension tuples.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6415a394-2034-49ef-b824-a24061c12fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_tuples_prompt():\n",
    "    try:\n",
    "        # Try to load existing prompt\n",
    "        prompt = bt.load_prompt(project=BT_PROJECT_NAME, slug=\"dimension-tuples-gen-prompt\")\n",
    "        # Validate it works\n",
    "        prompt.build(num_tuples_to_generate=20)\n",
    "        return prompt\n",
    "    except Exception:\n",
    "        # Create new prompt if loading/building fails\n",
    "        bt_project.prompts.create(\n",
    "            name=\"DimensionTuplesGenPrompt\",\n",
    "            slug=\"dimension-tuples-gen-prompt\", \n",
    "            description=\"Prompt for generating dimension tuples\",\n",
    "            model=\"claude-4-sonnet-20250514\",\n",
    "            messages=[{\"role\": \"user\", \"content\": TUPLES_GEN_PROMPT}],\n",
    "            if_exists=\"replace\",\n",
    "        )\n",
    "        bt_project.publish()\n",
    "        return bt.load_prompt(project=BT_PROJECT_NAME, slug=\"dimension-tuples-gen-prompt\")\n",
    "\n",
    "tuples_gen_prompt = get_or_create_tuples_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a7780d8-f634-4866-ba51-11c09aa148c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch existing prompts\n",
    "tuples_gen_prompt = bt.load_prompt(project=BT_PROJECT_NAME, slug=\"dimension-tuples-gen-prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2e22c5d-9c5f-4319-ae43-2bfa42aeadc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<braintrust.logger.Prompt at 0x7fbedeb23ec0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples_gen_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02acf1f0-6aed-4e44-b863-872dcf3c6e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"        I am designing a customer support chatbot for a retail company and need to generate a diverse set of synthetic test data to evaluate its performance. I've provided you with the key dimensions that make up a customer's inquiry, along with a list of possible values for each.\\n        \\n        ## Instructions\\n        \\n        Generate 20 unique combinations of dimension values based on the dimensions provided below.\\n        \\n        * Each combination should represent a distinct customer support scenario.\\n        * Ensure **balanced coverage** across all dimensions; avoid over-representing any single value or combination.\\n        * The generated tuples should be as realistic and varied as possible. For example, a frustrated customer is likely to use informal language and ask a complex question about a return.\\n        * Never generate a tuple where the persona is 'new_customer' and the intent is 'return_request' or 'order_status_check' unless the complexity is multi-turn to simulate a scenario where they are new to this process.\\n        \\n        ## Dimensions\\n        \\n        * **intent**: What kind of inquiry are they making?\\n            * product_inquiry\\n            * order_status_check\\n            * request_for_action_or_service\\n            * return_request\\n            * cancel_order\\n            * technical_support\\n            * account_management\\n            * general_info\\n        * **complexity**: The difficulty and structure of the query.\\n            * simple\\n            * multi-turn\\n            * ambiguous\\n        * **persona**: The type of user making the request\\n            * new_customer\\n            * repeat_customer\\n            * frustrated_customer\\n            * loyalty_member\\n        * **language_style**: The linguistic characteristics of the query\\n            * formal\\n            * informal\\n            * contains_slang\\n            * includes_typos\\n        \\n        Generate 20 unique dimension tuples.\\n    \", 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "_p = tuples_gen_prompt.build(num_tuples_to_generate=20)\n",
    "print(_p[\"messages\"])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![title](images/braintrust_playgrounds.png)",
   "id": "be23e871cd5b0e2f"
  },
  {
   "cell_type": "markdown",
   "id": "5bdb6d2a-ceba-40bd-8307-c6ef90b7f4a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Generate Dimension Combinations\n",
    "\n",
    "## Instructions:\n",
    "1. **Execute this cell**: It will generate diverse dimension combinations\n",
    "2. **Watch the output**: You should see parallel generation happening\n",
    "3. **Review the results**: Examine the generated tuples for:\n",
    "   - Realistic combinations\n",
    "   - Balanced coverage across dimensions\n",
    "   - Absence of impossible scenarios\n",
    "\n",
    "## Expected Output:\n",
    "- \"Generated X total tuples, Y unique\"\n",
    "- A list of DimensionTuple objects with varied combinations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9237d302-1aff-4df0-a188-e94b8b973856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synth_data_dimension_tuples(num_tuples: int = 20, model_kwargs: dict = {}):\n",
    "    \"\"\"Generate a list of dimension tuples based on the provided prompt.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    prompt = tuples_gen_prompt.build(num_tuples_to_generate=num_tuples)\n",
    "    rsp = completion(\n",
    "        model=\"bedrock/anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        messages=prompt[\"messages\"],\n",
    "        response_format=DimensionTuples,\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    \n",
    "    # Parse JSON content and validate with Pydantic\n",
    "    content = rsp.choices[0].message.content\n",
    "    tuples_dict = json.loads(content)\n",
    "    tuples_list: DimensionTuples = DimensionTuples(**tuples_dict)\n",
    "    \n",
    "    unique_tuples = []\n",
    "    seen = set()\n",
    "    \n",
    "    for tup in tuples_list.tuples:\n",
    "        tuple_str = tup.model_dump_json()\n",
    "        if tuple_str in seen:\n",
    "            continue\n",
    "        seen.add(tuple_str)\n",
    "        unique_tuples.append(tup)\n",
    "    \n",
    "    bt_experiment = bt.init(project=BT_PROJECT_NAME, experiment=f\"synth_tuples_it_{timestamp}\")\n",
    "    for uniq_tup in unique_tuples:\n",
    "        with bt_experiment.start_span(name=\"generate_dimension_tuples\") as span:\n",
    "            span.log(input=prompt[\"messages\"], output=uniq_tup, metadata=dict(model_kwargs=model_kwargs))\n",
    "    \n",
    "    summary = bt_experiment.summarize(summarize_scores=False)\n",
    "    return summary, tuples_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0d19c964-b4c9-4f29-825c-0df6cdaca21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data=generate_synth_data_dimension_tuples(num_tuples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "39701754-6282-4dc8-bf0e-97f9653b559f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ExperimentSummary(project_name='customer-service-agent-eval', project_id='8a70e092-c540-4db6-bb9d-2fa3da5d1e2e', experiment_id='714b8c07-9b0c-415d-ad93-c552212492fa', experiment_name='synth_tuples_it_20250826_0052', project_url='https://www.braintrust.dev/app/aiopsdream/p/customer-service-agent-eval', experiment_url='https://www.braintrust.dev/app/aiopsdream/p/customer-service-agent-eval/experiments/synth_tuples_it_20250826_0052', comparison_experiment_name=None, scores={}, metrics={}),\n",
       " DimensionTuples(tuples=[DimensionTuple(intent='technical_support', complexity='multi-turn', persona='frustrated_customer', language_style='informal'), DimensionTuple(intent='request_for_action_or_service', complexity='simple', persona='loyalty_member', language_style='formal')]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00c605-ee4d-4ed5-a9ac-8699bec0539f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Review Generated Tuples\n",
    "\n",
    "## Instructions:\n",
    "1. **Examine the output**: Look at the variety of combinations generated\n",
    "2. **Quality check**: Verify that combinations make sense (e.g., frustrated customers with complex queries)\n",
    "3. **Note the balance**: See how different dimensions are represented\n",
    "\n",
    "## Workshop Discussion:\n",
    "- Which combinations seem most realistic?\n",
    "- Are there any combinations that seem problematic?\n",
    "- How does this compare to manually brainstorming scenarios?\n",
    "\n",
    "## Key Takeaways\n",
    "- We use braintrust.init to manually create a new experiment.\n",
    "- We generate a trace in the form of a single span, adding information for input, output, and metadata.\n",
    "- We obtain the experiment summary via braintrust.summarize to review the experiment results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86484207-dbac-4661-93be-c4e98fd86a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
