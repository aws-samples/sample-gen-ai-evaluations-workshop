{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08dae7d3-dbad-4737-8bec-fe624ac21b12",
   "metadata": {},
   "source": [
    "# Basic Promptfoo Evaluation Setup and Execution\n",
    "\n",
    "## Welcome to GlobalMart's AI Innovation Team!\n",
    "\n",
    "Congratulations on joining GlobalMart's AI Innovation Team! As a leading e-commerce platform, GlobalMart receives thousands of customer emails daily, covering a wide range of topics from order inquiries to technical support requests. Your first major project is to develop and evaluate an AI-powered email classification system that will revolutionize our customer support operations.\n",
    "\n",
    "### Your Mission\n",
    "\n",
    "As the newest data scientist on the team, you've been tasked with setting up and running the initial evaluation for our email classification system using Promptfoo and Amazon Bedrock. This system aims to automatically categorize incoming customer emails into four main categories:\n",
    "\n",
    "1. Order Issues\n",
    "2. Product Inquiries\n",
    "3. Technical Support\n",
    "4. Returns/Refunds\n",
    "\n",
    "Your evaluation will help us understand the current performance of our AI model and identify areas for improvement. The insights you gather will be crucial in enhancing our customer service efficiency and response times.\n",
    "\n",
    "### What's at Stake\n",
    "\n",
    "The success of this project could lead to:\n",
    "- Faster response times to customer inquiries\n",
    "- More efficient allocation of customer support resources\n",
    "- Improved customer satisfaction rates\n",
    "- Potential cost savings in our customer service department\n",
    "\n",
    "Your work will directly impact GlobalMart's operational efficiency and customer experience. Are you ready to dive in and make a real difference?\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll set up and run our first evaluation for the GlobalMart Email Classification System using Promptfoo and Amazon Bedrock. We'll go through the process of defining our evaluation objectives, creating prompts and test cases, and analyzing the results.\n",
    "\n",
    "Let's get started on this exciting journey to optimize GlobalMart's customer support!\n",
    "\n",
    "## 1. Preparing for Evaluation\n",
    "\n",
    "### Review of Promptfoo configuration\n",
    "\n",
    "Before we begin, make sure you have Promptfoo installed and configured to work with Amazon Bedrock. \n",
    "\n",
    "### Defining evaluation objectives\n",
    "\n",
    "Our goal is to create an email classification system for GlobalMart that can accurately categorize customer emails into the following categories:\n",
    "1. Order Issues\n",
    "2. Product Inquiries\n",
    "3. Technical Support\n",
    "4. Returns/Refunds\n",
    "\n",
    "We'll evaluate our system's ability to correctly classify emails into these categories.\n",
    "\n",
    "## 2. Creating a Focused Evaluation\n",
    "\n",
    "### A. Designing a prompt template\n",
    "\n",
    "We'll create a prompt template in a separate Python file called `prompts.py`. This approach offers several advantages:\n",
    "1. Modularity: Keeps our prompts separate from configuration, making them easier to manage and update.\n",
    "2. Reusability: Allows us to easily reuse prompts across different evaluations or projects.\n",
    "3. Version control: Makes it easier to track changes to our prompts over time.\n",
    "\n",
    "Let's create our `prompts.py` file with the `classify_email` function below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60d913",
   "metadata": {},
   "source": [
    "First install Promptfoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install -g promptfoo --loglevel=error --no-fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b32f35-d94f-4725-9a73-b0b568d0f188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile prompts.py\n",
    "# prompts.py\n",
    "\n",
    "def classify_email(email_content):\n",
    "    return f\"\"\"You are an AI assistant for GlobalMart's customer support team. Your task is to classify the following email into one of these categories: Order Issues, Product Inquiries, Technical Support, or Returns/Refunds.\n",
    "\n",
    "Email content: {email_content}\n",
    "\n",
    "Provide your classification as a single word or phrase, choosing from the categories mentioned above. Do not include any explanation or additional text.\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "# You can add more prompt functions here in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938f97f-0e50-4aa0-824a-947b6b00f601",
   "metadata": {},
   "source": [
    "### B. Developing test cases\n",
    "\n",
    "We'll create our test cases in a CSV file named `dataset.csv`. Using a CSV file for test cases offers several benefits:\n",
    "1. Easy to read and edit: CSV files are human-readable and can be edited with various tools, including spreadsheet software.\n",
    "2. Scalability: CSV files can handle large numbers of test cases efficiently.\n",
    "3. Separation of concerns: Keeps test data separate from code and configuration.\n",
    "\n",
    "Let's update our `dataset.csv` file with the content below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94a4ae5-c8a4-4662-a915-4ea14d24122e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile dataset.csv\n",
    "email_content,__expected\n",
    "\"Hi, I ordered a laptop last week, but I haven't received any shipping update. Can you help?\",Order Issues\n",
    "\"I'm having trouble logging into my account. It keeps saying my password is incorrect even though I'm sure it's right.\",Technical Support\n",
    "\"Do you have the latest iPhone model in stock? I couldn't find it on your website.\",Product Inquiries\n",
    "\"I received my order yesterday, but the shirt is the wrong size. I'd like to return it for a refund.\",Returns/Refunds\n",
    "\"Can you tell me when the next sale is? I'm looking to buy a new TV.\",Product Inquiries\n",
    "\"My order arrived damaged. What should I do?\",Returns/Refunds\n",
    "\"How do I track my recent order?\",Order Issues\n",
    "\"I bought a blender from your store, but it's not working. Is there a warranty?\",Technical Support\n",
    "\"I want to change the shipping address for my recent order. Is that possible?\",Order Issues\n",
    "\"What's your return policy for electronics?\",Returns/Refunds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ace669-31eb-408e-91ca-972689116bb0",
   "metadata": {},
   "source": [
    "### C. Create your propmptfoo `promptfooconfig.yaml` config file\n",
    "Your configuration file brings everything together, specifying how the evaluation should run: description: \"GlobalMart Email Classification Evaluation\"\n",
    "\n",
    "The Promptfoo config file below has 4 key items:\n",
    "1. A clear description of what we're testing\n",
    "2. References to our prompt `python.py` function\n",
    "3. A Provider configuration (in this case Amazon's Nova Micro and Anthropic's Haiku 3.5)\n",
    "4. Test cases. In this case the test cases are the CSV abovefile with variables (`email_content`) and expected outputs (`__expected`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaf906a-b607-4042-a75a-c119d86379e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile promptfooconfig.yaml\n",
    "description: \"GlobalMart Email Classification Evaluation\"\n",
    "\n",
    "prompts:\n",
    "  - prompts.py:classify_email\n",
    "\n",
    "providers:\n",
    "  - id: bedrock:us.amazon.nova-lite-v1:0\n",
    "    label: \"Nova Lite\"\n",
    "    config:\n",
    "      region: us-west-2 # change to us-east-1 depending on your deployment region\n",
    "\n",
    "  - id: bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0\n",
    "    label: \"Haiku 3.5\"\n",
    "    config:\n",
    "      region: us-west-2 # change to us-east-1 depending on your deployment region\n",
    "\n",
    "tests:\n",
    "  - file://dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca912ed-575e-433d-993f-1c8e6e5a78cd",
   "metadata": {},
   "source": [
    "## 3. Running Your First Evaluation\n",
    "Now that we have set up all the components of our email classification system, it's time to run our first evaluation. We'll use the PromptFoo command line interface with some specific settings for our lab environment.\n",
    "Enter the following command to run the evaluation:\n",
    "\n",
    "`promptfoo eval --no-progress-bar --no-cache`\n",
    "\n",
    "Let's understand what these command flags do and why we're using them:\n",
    "\n",
    "**--no-progress-bar:** We're disabling the progress bar because we're running this in a Jupyter notebook environment. While progress bars are helpful when running evaluations in a terminal, they can create visual clutter in our notebook output.\n",
    "\n",
    "**--no-cache:** This flag tells PromptFoo to generate fresh results for every evaluation rather than using any cached responses. While caching is valuable in production environments to save time and reduce API costs, for learning purposes we want to see new results each time we run our evaluation. This helps us:\n",
    "\n",
    "- Observe how the model's responses might vary\n",
    "- Ensure we're not looking at stored results from previous runs\n",
    "- Get a true sense of the model's performance\n",
    "\n",
    "**NOTE:** In a real-world scenario, you might omit these flags to take advantage of PromptFoo's progress tracking and caching features. However, for this learning exercise, getting fresh, clear results helps us better understand how our classification system performs.\n",
    "\n",
    "After running this command, PromptFoo will:\n",
    "1. Process each email in our test dataset\n",
    "2. Send them to our configured Bedrock models\n",
    "3. Apply our Python-based assertions to evaluate the responses\n",
    "4. Generate a detailed report of the results\n",
    "\n",
    "Let's run the evaluation and examine the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d342fd-2d3a-4b65-ba17-ce9acac1bee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!promptfoo eval --no-progress-bar --no-cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651cde02-eeb0-474c-8f7d-ff3409f49592",
   "metadata": {},
   "source": [
    "**IMPORTANT**: In order to share the results of your evaluation and to see a more indept analysis of the resutls you'll need to create an **free** account with [PromptFoo](https://www.promptfoo.app/welcome). You'll only need to run this command once for thie workshop.\n",
    "\n",
    "The commmand is prepared below you just need to insert your API key in the command below. Be sure to remove the `<` and `>` when you paste in your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c14486-e180-464a-9408-443f6611bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!promptfoo auth login --host https://api.promptfoo.app --api-key <insert your api key here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdfbf3-9f71-4e55-8f5f-f97d2bacc258",
   "metadata": {},
   "source": [
    "### Sharing Your Evaluation Results\n",
    "Once you've run your evaluation, and have completed the `promptfoo auth` authentication command above, you can share the results with teammates or save them for later reference. PromptFoo provides a convenient way to do this using the share command:\n",
    "\n",
    "`promptfoo share`\n",
    "\n",
    "When you run this command, PromptFoo will:\n",
    "\n",
    "- Generate a unique, publicly accessible URL for your evaluation results\n",
    "- Display the URL in your terminal like this:\n",
    "```plaintext\n",
    "View results: https://app.promptfoo.dev/eval/f:91b9ea8a-174c-4129-9c52-774c34c96ea4\n",
    "```\n",
    "When you visit this URL, you'll see the same detailed evaluation results that you would see in the local web viewer, but now they're accessible to anyone with the link. This is especially useful for:\n",
    "\n",
    "- Collaborating with team members on improving the classification system\n",
    "- Documenting your evaluation results for future reference\n",
    "- Comparing results across different iterations of your prompts\n",
    "- Creating snapshots of your system's performance at different stages\n",
    "\n",
    "Keep in mind that these shared results will remain accessible via the URL, so you can always refer back to them even after making changes to your evaluation setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177ab1d-e186-469a-bd8e-dc6c2ccc59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!promptfoo share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eba521-60b4-4908-b8a2-01359d9b35a8",
   "metadata": {},
   "source": [
    "This will open a web interface where you can explore the results in depth.\n",
    "\n",
    "## 4. Identifying strengths and weaknesses\n",
    "\n",
    "Examine the results in the web viewer. Pay attention to:\n",
    "1. Overall accuracy: What percentage of emails were correctly classified?\n",
    "2. Performance by category: Are some categories more accurately classified than others?\n",
    "3. Misclassifications: Look at emails that were incorrectly categorized. Can you identify any patterns?\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lab, we've set up and run our first evaluation of the GlobalMart Email Classification System using Promptfoo and Amazon Bedrock. We've learned how to create prompts, define test cases, and analyze the results of our evaluation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
