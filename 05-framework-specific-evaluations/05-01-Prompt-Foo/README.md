# 05-01 PromptFoo for LLM Evaluations

## Overview

This module demonstrates how to use PromptFoo for rigorous LLM evaluation and optimization. PromptFoo enables custom evaluations tailored to your specific use cases, going beyond general benchmarks to measure task-specific performance.

## What is Promptfoo

Promptfoo is an open-source, LLM vendor agnostic, CLI and library for evaluating and red-teaming LLM apps.
With promptfoo, you can:

- Build reliable prompts, models, and RAGs with benchmarks specific to your use-case
- Secure your apps with automated red teaming and pentesting
- Speed up evaluations with caching, concurrency, and live reloading
- Score outputs automatically by defining metrics
- Use as a CLI, library, or in CI/CD


## Key Evaluation Components

Every well-designed LLM evaluation consists of:

1. **Example Input**: The prompt or question given to the model
2. **Golden Answer**: The ideal response, often created by subject matter experts
3. **Model Output**: The actual response generated by the LLM
4. **Score**: A quantitative or qualitative measure of performance

## Evaluation Approaches

### Code-based Grading
Programmatic methods for objective criteria:
- Exact string matching
- Keyword presence checking
- Regular expression pattern matching

### Human-based Grading
For nuanced understanding and subjective judgment:
- Expert review by domain specialists
- User experience panels

### Automated Metrics
Quantitative measures without human intervention:
- **Perplexity**: Model confidence in predictions
- **BLEU**: Machine translation evaluation
- **ROUGE**: Summarization quality assessment
- **F1 Score**: Classification accuracy balance

### Hybrid Approaches
Combining multiple methods for comprehensive evaluation.


## Getting Started
Navigate to `05-Promptfoo-basic.ipynb` notebook which demonstrates the use of Promptfoo to evaluate a basic email classification use case.
Before executing this notebook, please ensure access to Claude 3.5 Haiku and Amazon Nova Lite in `us-west-2` region.

For a detailed hands-on experience working on Promptfoo, please navigate to the following [AWS workshop](https://catalog.workshops.aws/promptfoo/en-US).
