{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Search Agent Evaluation Framework\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook evaluates the city search agent running on Amazon Bedrock AgentCore Runtime.\n",
    "\n",
    "### Pre-Requisite\n",
    "Execute 05-04-01-Agentic-Metrics-AgentCore.ipynb before this notebook\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "- **Multi-dimensional Quality Assessment**: Helpfulness, accuracy, clarity, professionalism, completeness\n",
    "- **Tool Usage Analysis**: web_search tool usage patterns\n",
    "- **Performance Metrics**: Response times and success rates\n",
    "- **LLM-as-Judge**: Claude Sonnet for objective evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (1.40.30)\n",
      "Requirement already satisfied: requests in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.30 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from boto3) (1.40.30)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from boto3) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from botocore<1.41.0,>=1.40.30->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.30->boto3) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets retrieve the citysearch agent arn that was stored in the notebook and setup the evaluator model to be used as the Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configured for agent citysearch, with endpoint arn:aws:bedrock-agentcore:us-east-1:146666888814:runtime/citysearch-583XaH96wT\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "%store -r citysearch_agent_arn\n",
    "AGENT_NAME = \"citysearch\" \n",
    "EVALUATOR_MODEL = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "AGENT_ENDPOINT = citysearch_agent_arn\n",
    "print(f\"‚úÖ Configured for agent {AGENT_NAME}, with endpoint {AGENT_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import requests\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# AWS clients\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "print(\"‚úÖ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Lets define classes for Test Cases and Evaluation responses\n",
    "\n",
    "Three specific tests for a city search agent:\n",
    "\n",
    "    Basic greeting - Tests politeness, expects no tools\n",
    "\n",
    "    Population query - Tests factual lookup, expects web_search tool\n",
    "\n",
    "    Area query - Tests measurement data, expects web_search tool\n",
    "\n",
    "Each test defines what tools should be used and what criteria make a good response, enabling automated evaluation of agent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 3 test cases\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    id: str\n",
    "    query: str\n",
    "    category: str\n",
    "    expected_tools: List[str]\n",
    "    expected_criteria: Dict[str, Any]\n",
    "    description: str\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    test_case_id: str\n",
    "    query: str\n",
    "    response: str\n",
    "    metrics: Dict[str, float]\n",
    "    response_time: float\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "    tool_calls: List[str] = None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        # Convert to dict manually to avoid serialization issues\n",
    "        return {\n",
    "            \"test_case_id\": self.test_case_id,\n",
    "            \"query\": self.query,\n",
    "            \"response\": str(self.response),  # Ensure string conversion\n",
    "            \"metrics\": dict(self.metrics),\n",
    "            \"response_time\": self.response_time,\n",
    "            \"success\": self.success,\n",
    "            \"error_message\": self.error_message,\n",
    "            \"tool_calls\": list(self.tool_calls) if self.tool_calls else []\n",
    "        }\n",
    "\n",
    "# Test cases for city search agent\n",
    "TEST_CASES = [\n",
    "    TestCase(\n",
    "        id=\"basic_greeting\",\n",
    "        query=\"Hi, I need help with finding information about cities\",\n",
    "        category=\"basic_inquiry\",\n",
    "        expected_tools=[],\n",
    "        expected_criteria={\"should_be_polite\": True, \"should_ask_for_details\": True},\n",
    "        description=\"Basic greeting and help request\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"city_population_search\",\n",
    "        query=\"What is the population of Seattle?\",\n",
    "        category=\"population_inquiry\",\n",
    "        expected_tools=[\"web_search\"],\n",
    "        expected_criteria={\"should_provide_population\": True, \"should_be_accurate\": True},\n",
    "        description=\"City population information request\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"city_area_search\",\n",
    "        query=\"How large is Los Angeles in square miles?\",\n",
    "        category=\"area_inquiry\",\n",
    "        expected_tools=[\"web_search\"],\n",
    "        expected_criteria={\"should_provide_area\": True, \"should_be_clear\": True},\n",
    "        description=\"City area information request\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(TEST_CASES)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Define the methods for to invoke agent which includes evaluating LLM responses as well as tool use\n",
    "For tool use detection, X-Ray observability is primarily used. The implementation looks for gen_ai.tool.name annotations in trace segments of both main and sub segments. As a fallback, content analysis of the output is used to determine tool use. \n",
    "The implementation below provides the details of the actual agent invocation with tools used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent invocation functions defined\n"
     ]
    }
   ],
   "source": [
    "# AgentCore client using bedrock-agentcore service\n",
    "agentcore_client = boto3.client('bedrock-agentcore', region_name='us-east-1')\n",
    "\n",
    "async def invoke_agent(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Invoke the agent using AgentCore Runtime\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        payload = json.dumps({\"prompt\": query})\n",
    "        session_id = f\"eval-session-{uuid.uuid4()}\"\n",
    "        \n",
    "        response = agentcore_client.invoke_agent_runtime(\n",
    "            agentRuntimeArn=AGENT_ENDPOINT,\n",
    "            runtimeSessionId=session_id,\n",
    "            payload=payload,\n",
    "            qualifier=\"DEFAULT\"\n",
    "        )\n",
    "        \n",
    "        print(\"AgentCore Response keys:\", list(response.keys()))\n",
    "        \n",
    "        # Extract response text from StreamingBody\n",
    "        response_text = \"\"\n",
    "        if isinstance(response, dict) and 'response' in response:\n",
    "            streaming_body = response['response']\n",
    "            if hasattr(streaming_body, 'read'):\n",
    "                response_text = streaming_body.read().decode('utf-8')\n",
    "                print(f\"Extracted response text: {response_text[:200]}...\")\n",
    "            else:\n",
    "                response_text = str(streaming_body)\n",
    "        else:\n",
    "            response_text = str(response)\n",
    "            \n",
    "        # Extract tool calls from response metadata or content\n",
    "        tool_calls = extract_tool_calls_from_agentcore_observability(response, response_text, session_id)\n",
    "        \n",
    "        return {\n",
    "            \"response\": response_text,\n",
    "            \"success\": True,\n",
    "            \"tool_calls\": tool_calls,\n",
    "            \"response_time\": time.time() - start_time,\n",
    "            \"session_id\": session_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error invoking agent: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return {\n",
    "            \"response\": error_msg,\n",
    "            \"success\": False,\n",
    "            \"tool_calls\": [],\n",
    "            \"response_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "def extract_tool_calls_from_agentcore_observability(response_obj, response_text, session_id) -> List[str]:\n",
    "    \"\"\"Extract tool calls using AgentCore observability gen_ai.tool.name and tool.status.\"\"\"\n",
    "    tools = []\n",
    "    \n",
    "    # Query X-Ray for gen_ai.tool.name spans\n",
    "    if session_id:\n",
    "        try:\n",
    "            xray_client = boto3.client('xray')\n",
    "            print(\"Session id\", session_id)\n",
    "            # Get traces with gen_ai annotations\n",
    "            response = xray_client.get_trace_summaries(\n",
    "                TimeRangeType='Service',\n",
    "                StartTime=time.time() - 300,\n",
    "                EndTime=time.time(),\n",
    "                FilterExpression=f'annotation.session_id = \"{session_id}\"'\n",
    "            )\n",
    "            \n",
    "            for trace_summary in response.get('TraceSummaries', []):\n",
    "                trace_response = xray_client.batch_get_traces(TraceIds=[trace_summary['Id']])\n",
    "                \n",
    "                for trace in trace_response.get('Traces', []):\n",
    "                    for segment in trace.get('Segments', []):\n",
    "                        segment_doc = json.loads(segment['Document'])\n",
    "                        \n",
    "                        # Check for gen_ai.tool.name in annotations\n",
    "                        annotations = segment_doc.get('annotations', {})\n",
    "                        if 'gen_ai.tool.name' in annotations:\n",
    "                            tool_name = annotations['gen_ai.tool.name']\n",
    "                            tool_status = annotations.get('tool.status', 'success')\n",
    "                            \n",
    "                            # Only include successful tool calls\n",
    "                            if tool_status in ['success', 'completed']:\n",
    "                                tools.append(tool_name)\n",
    "                        \n",
    "                        # Check subsegments\n",
    "                        for subsegment in segment_doc.get('subsegments', []):\n",
    "                            sub_annotations = subsegment.get('annotations', {})\n",
    "                            if 'gen_ai.tool.name' in sub_annotations:\n",
    "                                tool_name = sub_annotations['gen_ai.tool.name']\n",
    "                                tool_status = sub_annotations.get('tool.status', 'success')\n",
    "                                \n",
    "                                if tool_status in ['success', 'completed']:\n",
    "                                    tools.append(tool_name)\n",
    "                                    \n",
    "        except Exception as e:\n",
    "            print(f\"X-Ray observability extraction failed: {e}\")\n",
    "    \n",
    "    # Enhanced fallback to content analysis if no observability data\n",
    "    if not tools and response_text:\n",
    "        response_lower = str(response_text).lower()\n",
    "        \n",
    "        # Look for web search indicators\n",
    "        web_search_indicators = [\n",
    "            \"gathered from reliable sources\", \"based on\", \"web search\", \"search results\",\n",
    "            \"according to\", \"information shows\", \"data indicates\", \"results show\",\n",
    "            \"found that\", \"research shows\", \"sources indicate\", \"data suggests\",\n",
    "            \"population of\", \"area of\", \"square miles\", \"residents\", \"million people\",\n",
    "            \"thousand people\", \"sq mi\", \"km¬≤\", \"census data\", \"demographic\",\n",
    "            \"approximately\", \"estimated\", \"as of\", \"current population\", \"latest data\"\n",
    "        ]\n",
    "        \n",
    "        if any(phrase in response_lower for phrase in web_search_indicators):\n",
    "            tools.append(\"web_search\")\n",
    "            print(f\"Tool detected via content analysis: web_search\")\n",
    "    \n",
    "    return list(set(tools))\n",
    "\n",
    "print(\"‚úÖ Agent invocation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Next lets configure the prompts to test against the evaluator model with the response generated and response expected. The evaluator model provides evaluation metrics for both the LLM as well as tool use responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "async def evaluate_response_quality(query: str, response: str, criteria: Dict[str, Any]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate response quality using Claude as judge\"\"\"\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"\n",
    "    You are an expert evaluator for city search AI agents. Evaluate the following response on a scale of 1-5 for each metric.\n",
    "\n",
    "    Customer Query: {query}\n",
    "    Agent Response: {response}\n",
    "\n",
    "    Evaluate on these metrics (1=Poor, 2=Below Average, 3=Average, 4=Good, 5=Excellent):\n",
    "\n",
    "    1. HELPFULNESS: Does the response address the user's needs and provide useful information?\n",
    "    2. ACCURACY: Is the information provided factually correct and reliable?\n",
    "    3. CLARITY: Is the response clear, well-structured, and easy to understand?\n",
    "    4. PROFESSIONALISM: Does the response maintain appropriate tone and professionalism?\n",
    "    5. COMPLETENESS: Does the response fully address all aspects of the query?\n",
    "\n",
    "    Expected criteria: {json.dumps(criteria, indent=2)}\n",
    "\n",
    "    Respond with ONLY a JSON object in this format:\n",
    "    {{\n",
    "        \"helpfulness\": <score>,\n",
    "        \"accuracy\": <score>,\n",
    "        \"clarity\": <score>,\n",
    "        \"professionalism\": <score>,\n",
    "        \"completeness\": <score>,\n",
    "        \"reasoning\": \"Brief explanation of scores\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response_obj = bedrock.invoke_model(\n",
    "            modelId=EVALUATOR_MODEL,\n",
    "            body=json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1000,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "                ]\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response_obj['body'].read())\n",
    "        content = result['content'][0]['text']\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        start_idx = content.find('{')\n",
    "        end_idx = content.rfind('}') + 1\n",
    "        json_str = content[start_idx:end_idx]\n",
    "        \n",
    "        scores = json.loads(json_str)\n",
    "        return {k: v for k, v in scores.items() if k != \"reasoning\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in quality evaluation: {e}\")\n",
    "        return {\n",
    "            \"helpfulness\": 0.0,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"clarity\": 0.0,\n",
    "            \"professionalism\": 0.0,\n",
    "            \"completeness\": 0.0\n",
    "        }\n",
    "\n",
    "def evaluate_tool_usage(expected_tools: List[str], actual_tools: List[str]) -> float:\n",
    "    \"\"\"Evaluate tool usage effectiveness\"\"\"\n",
    "    if not expected_tools:\n",
    "        return 5.0 if not actual_tools else 3.0\n",
    "    \n",
    "    if not actual_tools:\n",
    "        print(f\"Expected tools {expected_tools}, but no tools were called\")\n",
    "        return 0.0\n",
    "    \n",
    "    expected_set = set(expected_tools)\n",
    "    actual_set = set(actual_tools)\n",
    "    \n",
    "    precision = len(expected_set.intersection(actual_set)) / len(actual_set) if actual_set else 0\n",
    "    recall = len(expected_set.intersection(actual_set)) / len(expected_set) if expected_set else 0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1 * 5  # Scale to 0-5\n",
    "\n",
    "print(\"‚úÖ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Lets test a single Testcase end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test case evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "async def evaluate_test_case(test_case: TestCase) -> EvaluationResult:\n",
    "    \"\"\"Evaluate a single test case\"\"\"\n",
    "    print(f\"üîç Evaluating: {test_case.id} - {test_case.description}\")\n",
    "    \n",
    "    # Invoke agent\n",
    "    agent_result = await invoke_agent(test_case.query)\n",
    "    \n",
    "    if not agent_result[\"success\"]:\n",
    "        return EvaluationResult(\n",
    "            test_case_id=test_case.id,\n",
    "            query=test_case.query,\n",
    "            response=\"\",\n",
    "            metrics={},\n",
    "            response_time=agent_result[\"response_time\"],\n",
    "            success=False,\n",
    "            error_message=agent_result.get(\"response\", \"Unknown error\")\n",
    "        )\n",
    "    \n",
    "    # Evaluate response quality\n",
    "    quality_scores = await evaluate_response_quality(\n",
    "        test_case.query,\n",
    "        agent_result[\"response\"],\n",
    "        test_case.expected_criteria\n",
    "    )\n",
    "    \n",
    "    # Evaluate tool usage\n",
    "    tool_score = evaluate_tool_usage(\n",
    "        test_case.expected_tools,\n",
    "        agent_result[\"tool_calls\"]\n",
    "    )\n",
    "    \n",
    "    # Combine all metrics\n",
    "    metrics = {\n",
    "        **quality_scores,\n",
    "        \"tool_usage\": tool_score,\n",
    "        \"response_time\": agent_result[\"response_time\"]\n",
    "    }\n",
    "    \n",
    "    return EvaluationResult(\n",
    "        test_case_id=test_case.id,\n",
    "        query=test_case.query,\n",
    "        response=agent_result[\"response\"],\n",
    "        metrics=metrics,\n",
    "        response_time=agent_result[\"response_time\"],\n",
    "        success=True,\n",
    "        tool_calls=agent_result[\"tool_calls\"]\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Test case evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating: city_population_search - City population information request\n",
      "AgentCore Response keys: ['ResponseMetadata', 'runtimeSessionId', 'traceId', 'baggage', 'contentType', 'statusCode', 'response']\n",
      "Extracted response text: \"As an AI, I don't have real-time data access, but as of the most recent estimates from the U.S. Census Bureau, Seattle, Washington, had a population of around 740,000 as of 2020. For the most up-to-d...\n",
      "Session id eval-session-68787f58-a081-40ac-84ce-fb7a6a68e359\n",
      "Tool detected via content analysis: web_search\n",
      "\n",
      "üìä Demo Result for 'city_population_search':\n",
      "Query: What is the population of Seattle?\n",
      "Response: \"As an AI, I don't have real-time data access, but as of the most recent estimates from the U.S. Census Bureau, Seattle, Washington, had a population of around 740,000 as of 2020. For the most up-to-d...\n",
      "Response Time: 3.157s\n",
      "Tool Calls: ['web_search']\n",
      "Success: True\n",
      "Metrics: {'helpfulness': 4, 'accuracy': 4, 'clarity': 5, 'professionalism': 5, 'completeness': 4, 'tool_usage': 5.0, 'response_time': 3.156960964202881}\n"
     ]
    }
   ],
   "source": [
    "# Test a single case first\n",
    "demo_test = TEST_CASES[1]  # Population search\n",
    "demo_result = await evaluate_test_case(demo_test)\n",
    "\n",
    "print(f\"\\nüìä Demo Result for '{demo_test.id}':\")\n",
    "print(f\"Query: {demo_result.query}\")\n",
    "response_str = str(demo_result.response)\n",
    "print(f\"Response: {response_str[:200]}...\" if len(response_str) > 200 else f\"Response: {response_str}\")\n",
    "print(f\"Response Time: {demo_result.response_time:.3f}s\")\n",
    "print(f\"Tool Calls: {demo_result.tool_calls}\")\n",
    "print(f\"Success: {demo_result.success}\")\n",
    "print(f\"Metrics: {demo_result.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5) Now that we see the evaluation working end to end for a single testcase, lets run through all the testcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "async def run_full_evaluation(test_cases: List[TestCase]) -> Dict[str, Any]:\n",
    "    \"\"\"Run evaluation on all test cases\"\"\"\n",
    "    print(f\"üöÄ Starting evaluation of {len(test_cases)} test cases...\")\n",
    "    \n",
    "    results = []\n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n[{i}/{len(test_cases)}] Processing: {test_case.id}\")\n",
    "        result = await evaluate_test_case(test_case)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Brief pause between tests\n",
    "        await asyncio.sleep(1)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary = calculate_summary(results)\n",
    "    \n",
    "    return {\n",
    "        \"agent_name\": AGENT_NAME,\n",
    "        \"total_test_cases\": len(test_cases),\n",
    "        \"results\": [result.to_dict() for result in results],\n",
    "        \"summary\": summary,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def calculate_summary(results: List[EvaluationResult]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate summary statistics\"\"\"\n",
    "    successful_results = [r for r in results if r.success]\n",
    "    \n",
    "    if not successful_results:\n",
    "        return {\"error\": \"No successful test cases\"}\n",
    "    \n",
    "    # Average scores\n",
    "    metrics = [\"helpfulness\", \"accuracy\", \"clarity\", \"professionalism\", \"completeness\", \"tool_usage\"]\n",
    "    avg_scores = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        scores = [r.metrics.get(metric, 0) for r in successful_results if metric in r.metrics]\n",
    "        avg_scores[metric] = sum(scores) / len(scores) if scores else 0\n",
    "    \n",
    "    # Response time statistics\n",
    "    response_times = sorted([r.response_time for r in successful_results])\n",
    "    n = len(response_times)\n",
    "    \n",
    "    percentiles = {\n",
    "        \"p50\": response_times[n//2] if n > 0 else 0,\n",
    "        \"p90\": response_times[int(n*0.9)] if n > 0 else 0,\n",
    "        \"p95\": response_times[int(n*0.95)] if n > 0 else 0,\n",
    "        \"p99\": response_times[int(n*0.99)] if n > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"success_rate\": len(successful_results) / len(results),\n",
    "        \"average_scores\": avg_scores,\n",
    "        \"overall_score\": sum(avg_scores.values()) / len(avg_scores) if avg_scores else 0,\n",
    "        \"response_time_percentiles\": percentiles,\n",
    "        \"total_successful\": len(successful_results),\n",
    "        \"total_failed\": len(results) - len(successful_results)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Full evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting evaluation of 3 test cases...\n",
      "\n",
      "[1/3] Processing: basic_greeting\n",
      "üîç Evaluating: basic_greeting - Basic greeting and help request\n",
      "AgentCore Response keys: ['ResponseMetadata', 'runtimeSessionId', 'traceId', 'baggage', 'contentType', 'statusCode', 'response']\n",
      "Extracted response text: \"Of course, I'd be happy to help you find information about cities! Here are a few general categories and tips on how to gather information about any city you're interested in:\\n\\n### General Informat...\n",
      "Session id eval-session-26d8f0e7-056f-48be-b886-235848608847\n",
      "Tool detected via content analysis: web_search\n",
      "\n",
      "[2/3] Processing: city_population_search\n",
      "üîç Evaluating: city_population_search - City population information request\n",
      "AgentCore Response keys: ['ResponseMetadata', 'runtimeSessionId', 'traceId', 'baggage', 'contentType', 'statusCode', 'response']\n",
      "Extracted response text: \"The population of Seattle, according to the 2020 United States census, is 737,015.\\n\\nHere is the information provided in XML tags as requested:\\n\\n```xml\\n<pop>737015</pop>\\n```\"...\n",
      "Session id eval-session-2ac2d1dc-b661-496c-84a9-7af767927ee7\n",
      "Tool detected via content analysis: web_search\n",
      "\n",
      "[3/3] Processing: city_area_search\n",
      "üîç Evaluating: city_area_search - City area information request\n",
      "AgentCore Response keys: ['ResponseMetadata', 'runtimeSessionId', 'traceId', 'baggage', 'contentType', 'statusCode', 'response']\n",
      "Extracted response text: \"<thinking> From the web search results, I found that Los Angeles spans a widely diverse geographic area, primarily a desert basin, surrounded by the San Gabriel Mountain range and divided by the Sant...\n",
      "Session id eval-session-f9400df5-6198-4f19-a491-fa1e8fcafa4f\n",
      "Tool detected via content analysis: web_search\n",
      "\n",
      "============================================================\n",
      "üìä EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "ü§ñ Agent: citysearch\n",
      "üìù Total Test Cases: 3\n",
      "‚úÖ Success Rate: 100.0%\n",
      "üéØ Overall Score: 4.11/5.0\n",
      "\n",
      "üìà QUALITY METRICS (1-5 scale):\n",
      "  üü° Helpfulness: 3.67\n",
      "  üü¢ Accuracy: 4.33\n",
      "  üü¢ Clarity: 4.67\n",
      "  üü¢ Professionalism: 4.33\n",
      "  üü° Completeness: 3.33\n",
      "  üü¢ Tool_Usage: 4.33\n",
      "\n",
      "‚è±Ô∏è  RESPONSE TIME PERCENTILES:\n",
      "  P50: 3.826s\n",
      "  P90: 4.193s\n",
      "  P95: 4.193s\n",
      "  P99: 4.193s\n"
     ]
    }
   ],
   "source": [
    "# Run full evaluation\n",
    "evaluation_results = await run_full_evaluation(TEST_CASES)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display results\n",
    "summary = evaluation_results.get(\"summary\", {})\n",
    "print(f\"\\nü§ñ Agent: {evaluation_results['agent_name']}\")\n",
    "print(f\"üìù Total Test Cases: {evaluation_results['total_test_cases']}\")\n",
    "print(f\"‚úÖ Success Rate: {summary.get('success_rate', 0):.1%}\")\n",
    "print(f\"üéØ Overall Score: {summary.get('overall_score', 0):.2f}/5.0\")\n",
    "\n",
    "print(\"\\nüìà QUALITY METRICS (1-5 scale):\")\n",
    "avg_scores = summary.get(\"average_scores\", {})\n",
    "for metric, score in avg_scores.items():\n",
    "    if metric != \"response_time\":\n",
    "        emoji = \"üü¢\" if score >= 4.0 else \"üü°\" if score >= 3.0 else \"üî¥\"\n",
    "        print(f\"  {emoji} {metric.title()}: {score:.2f}\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  RESPONSE TIME PERCENTILES:\")\n",
    "percentiles = summary.get(\"response_time_percentiles\", {})\n",
    "for p, time_val in percentiles.items():\n",
    "    print(f\"  {p.upper()}: {time_val:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Results saved to: evaluation_results_citysearch_20250916_185225.json\n"
     ]
    }
   ],
   "source": [
    "# Save results to file\n",
    "output_file = f\"evaluation_results_{AGENT_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Results saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
