{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Search Agent Evaluation Framework\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook evaluates the city search agent running on Amazon Bedrock AgentCore Runtime.\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "- **Multi-dimensional Quality Assessment**: Helpfulness, accuracy, clarity, professionalism, completeness\n",
    "- **Tool Usage Analysis**: web_search tool usage patterns\n",
    "- **Performance Metrics**: Response times and success rates\n",
    "- **LLM-as-Judge**: Claude Sonnet for objective evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (1.40.30)\n",
      "Requirement already satisfied: requests in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.30 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from boto3) (1.40.30)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from boto3) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from botocore<1.41.0,>=1.40.30->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.30->boto3) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Configured for agent citysearch, with endpoint arn:aws:bedrock-agentcore:us-east-1:XXXXXXXXXXXX:runtime/citysearch-583XaH96wT\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "%store -r citysearch_agent_arn\n",
    "AGENT_NAME = \"citysearch\" \n",
    "EVALUATOR_MODEL = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "AGENT_ENDPOINT = citysearch_agent_arn\n",
    "print(f\"\u2705 Configured for agent {AGENT_NAME}, with endpoint {AGENT_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import requests\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# AWS clients\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "print(\"\u2705 Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTestCase\u001b[39;00m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m      4\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    id: str\n",
    "    query: str\n",
    "    category: str\n",
    "    expected_tools: List[str]\n",
    "    expected_criteria: Dict[str, Any]\n",
    "    description: str\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    test_case_id: str\n",
    "    query: str\n",
    "    response: str\n",
    "    metrics: Dict[str, float]\n",
    "    response_time: float\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "    tool_calls: List[str] = None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        # Convert to dict manually to avoid serialization issues\n",
    "        return {\n",
    "            \"test_case_id\": self.test_case_id,\n",
    "            \"query\": self.query,\n",
    "            \"response\": str(self.response),  # Ensure string conversion\n",
    "            \"metrics\": dict(self.metrics),\n",
    "            \"response_time\": self.response_time,\n",
    "            \"success\": self.success,\n",
    "            \"error_message\": self.error_message,\n",
    "            \"tool_calls\": list(self.tool_calls) if self.tool_calls else []\n",
    "        }\n",
    "\n",
    "# Test cases for city search agent\n",
    "TEST_CASES = [\n",
    "    TestCase(\n",
    "        id=\"basic_greeting\",\n",
    "        query=\"Hi, I need help with finding information about cities\",\n",
    "        category=\"basic_inquiry\",\n",
    "        expected_tools=[],\n",
    "        expected_criteria={\"should_be_polite\": True, \"should_ask_for_details\": True},\n",
    "        description=\"Basic greeting and help request\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"city_population_search\",\n",
    "        query=\"What is the population of Seattle?\",\n",
    "        category=\"population_inquiry\",\n",
    "        expected_tools=[\"web_search\"],\n",
    "        expected_criteria={\"should_provide_population\": True, \"should_be_accurate\": True},\n",
    "        description=\"City population information request\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"city_area_search\",\n",
    "        query=\"How large is Los Angeles in square miles?\",\n",
    "        category=\"area_inquiry\",\n",
    "        expected_tools=[\"web_search\"],\n",
    "        expected_criteria={\"should_provide_area\": True, \"should_be_clear\": True},\n",
    "        description=\"City area information request\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\u2705 Loaded {len(TEST_CASES)} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgentCore client using bedrock-agentcore service\n",
    "agentcore_client = boto3.client('bedrock-agentcore', region_name='us-east-1')\n",
    "\n",
    "async def invoke_agent(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Invoke the agent using AgentCore Runtime\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        payload = json.dumps({\"prompt\": query})\n",
    "        session_id = f\"eval-session-{uuid.uuid4()}\"\n",
    "        \n",
    "        response = agentcore_client.invoke_agent_runtime(\n",
    "            agentRuntimeArn=AGENT_ENDPOINT,\n",
    "            runtimeSessionId=session_id,\n",
    "            payload=payload,\n",
    "            qualifier=\"DEFAULT\"\n",
    "        )\n",
    "        \n",
    "        print(\"AgentCore Response keys:\", list(response.keys()))\n",
    "        \n",
    "        # Extract response text from StreamingBody\n",
    "        response_text = \"\"\n",
    "        if isinstance(response, dict) and 'response' in response:\n",
    "            streaming_body = response['response']\n",
    "            if hasattr(streaming_body, 'read'):\n",
    "                response_text = streaming_body.read().decode('utf-8')\n",
    "                print(f\"Extracted response text: {response_text[:200]}...\")\n",
    "            else:\n",
    "                response_text = str(streaming_body)\n",
    "        else:\n",
    "            response_text = str(response)\n",
    "            \n",
    "        # Extract tool calls from response metadata or content\n",
    "        tool_calls = extract_tool_calls_from_agentcore_observability(response, response_text, session_id)\n",
    "        \n",
    "        return {\n",
    "            \"response\": response_text,\n",
    "            \"success\": True,\n",
    "            \"tool_calls\": tool_calls,\n",
    "            \"response_time\": time.time() - start_time,\n",
    "            \"session_id\": session_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error invoking agent: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return {\n",
    "            \"response\": error_msg,\n",
    "            \"success\": False,\n",
    "            \"tool_calls\": [],\n",
    "            \"response_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "def extract_tool_calls_from_agentcore_observability(response_obj, response_text, session_id) -> List[str]:\n",
    "    \"\"\"Extract tool calls using AgentCore observability gen_ai.tool.name and tool.status.\"\"\"\n",
    "    tools = []\n",
    "    \n",
    "    # Query X-Ray for gen_ai.tool.name spans\n",
    "    if session_id:\n",
    "        try:\n",
    "            xray_client = boto3.client('xray')\n",
    "            _original_print(\"Session id\", session_id)\n",
    "            # Get traces with gen_ai annotations\n",
    "            response = xray_client.get_trace_summaries(\n",
    "                TimeRangeType='Service',\n",
    "                StartTime=time.time() - 300,\n",
    "                EndTime=time.time(),\n",
    "                FilterExpression=f'annotation.session_id = \"{session_id}\"'\n",
    "            )\n",
    "            \n",
    "            for trace_summary in response.get('TraceSummaries', []):\n",
    "                trace_response = xray_client.batch_get_traces(TraceIds=[trace_summary['Id']])\n",
    "                \n",
    "                for trace in trace_response.get('Traces', []):\n",
    "                    for segment in trace.get('Segments', []):\n",
    "                        segment_doc = json.loads(segment['Document'])\n",
    "                        \n",
    "                        # Check for gen_ai.tool.name in annotations\n",
    "                        annotations = segment_doc.get('annotations', {})\n",
    "                        if 'gen_ai.tool.name' in annotations:\n",
    "                            tool_name = annotations['gen_ai.tool.name']\n",
    "                            tool_status = annotations.get('tool.status', 'success')\n",
    "                            \n",
    "                            # Only include successful tool calls\n",
    "                            if tool_status in ['success', 'completed']:\n",
    "                                tools.append(tool_name)\n",
    "                        \n",
    "                        # Check subsegments\n",
    "                        for subsegment in segment_doc.get('subsegments', []):\n",
    "                            sub_annotations = subsegment.get('annotations', {})\n",
    "                            if 'gen_ai.tool.name' in sub_annotations:\n",
    "                                tool_name = sub_annotations['gen_ai.tool.name']\n",
    "                                tool_status = sub_annotations.get('tool.status', 'success')\n",
    "                                \n",
    "                                if tool_status in ['success', 'completed']:\n",
    "                                    tools.append(tool_name)\n",
    "                                    \n",
    "        except Exception as e:\n",
    "            print(f\"X-Ray observability extraction failed: {e}\")\n",
    "    \n",
    "    # Enhanced fallback to content analysis if no observability data\n",
    "    if not tools and response_text:\n",
    "        response_lower = str(response_text).lower()\n",
    "        \n",
    "        # Look for web search indicators\n",
    "        web_search_indicators = [\n",
    "            \"gathered from reliable sources\", \"based on\", \"web search\", \"search results\",\n",
    "            \"according to\", \"information shows\", \"data indicates\", \"results show\",\n",
    "            \"found that\", \"research shows\", \"sources indicate\", \"data suggests\",\n",
    "            \"population of\", \"area of\", \"square miles\", \"residents\", \"million people\",\n",
    "            \"thousand people\", \"sq mi\", \"km\u00b2\", \"census data\", \"demographic\",\n",
    "            \"approximately\", \"estimated\", \"as of\", \"current population\", \"latest data\"\n",
    "        ]\n",
    "        \n",
    "        if any(phrase in response_lower for phrase in web_search_indicators):\n",
    "            tools.append(\"web_search\")\n",
    "            print(f\"Tool detected via content analysis: web_search\")\n",
    "    \n",
    "    return list(set(tools))\n",
    "\n",
    "print(\"\u2705 Agent invocation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_response_quality(query: str, response: str, criteria: Dict[str, Any]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate response quality using Claude as judge\"\"\"\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"\n",
    "    You are an expert evaluator for city search AI agents. Evaluate the following response on a scale of 1-5 for each metric.\n",
    "\n",
    "    Customer Query: {query}\n",
    "    Agent Response: {response}\n",
    "\n",
    "    Evaluate on these metrics (1=Poor, 2=Below Average, 3=Average, 4=Good, 5=Excellent):\n",
    "\n",
    "    1. HELPFULNESS: Does the response address the user's needs and provide useful information?\n",
    "    2. ACCURACY: Is the information provided factually correct and reliable?\n",
    "    3. CLARITY: Is the response clear, well-structured, and easy to understand?\n",
    "    4. PROFESSIONALISM: Does the response maintain appropriate tone and professionalism?\n",
    "    5. COMPLETENESS: Does the response fully address all aspects of the query?\n",
    "\n",
    "    Expected criteria: {json.dumps(criteria, indent=2)}\n",
    "\n",
    "    Respond with ONLY a JSON object in this format:\n",
    "    {{\n",
    "        \"helpfulness\": <score>,\n",
    "        \"accuracy\": <score>,\n",
    "        \"clarity\": <score>,\n",
    "        \"professionalism\": <score>,\n",
    "        \"completeness\": <score>,\n",
    "        \"reasoning\": \"Brief explanation of scores\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response_obj = bedrock.invoke_model(\n",
    "            modelId=EVALUATOR_MODEL,\n",
    "            body=json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1000,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "                ]\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response_obj['body'].read())\n",
    "        content = result['content'][0]['text']\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        start_idx = content.find('{')\n",
    "        end_idx = content.rfind('}') + 1\n",
    "        json_str = content[start_idx:end_idx]\n",
    "        \n",
    "        scores = json.loads(json_str)\n",
    "        return {k: v for k, v in scores.items() if k != \"reasoning\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in quality evaluation: {e}\")\n",
    "        return {\n",
    "            \"helpfulness\": 0.0,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"clarity\": 0.0,\n",
    "            \"professionalism\": 0.0,\n",
    "            \"completeness\": 0.0\n",
    "        }\n",
    "\n",
    "def evaluate_tool_usage(expected_tools: List[str], actual_tools: List[str]) -> float:\n",
    "    \"\"\"Evaluate tool usage effectiveness\"\"\"\n",
    "    if not expected_tools:\n",
    "        return 5.0 if not actual_tools else 3.0\n",
    "    \n",
    "    if not actual_tools:\n",
    "        print(f\"Expected tools {expected_tools}, but no tools were called\")\n",
    "        return 0.0\n",
    "    \n",
    "    expected_set = set(expected_tools)\n",
    "    actual_set = set(actual_tools)\n",
    "    \n",
    "    precision = len(expected_set.intersection(actual_set)) / len(actual_set) if actual_set else 0\n",
    "    recall = len(expected_set.intersection(actual_set)) / len(expected_set) if expected_set else 0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1 * 5  # Scale to 0-5\n",
    "\n",
    "print(\"\u2705 Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_test_case(test_case: TestCase) -> EvaluationResult:\n",
    "    \"\"\"Evaluate a single test case\"\"\"\n",
    "    print(f\"\ud83d\udd0d Evaluating: {test_case.id} - {test_case.description}\")\n",
    "    \n",
    "    # Invoke agent\n",
    "    agent_result = await invoke_agent(test_case.query)\n",
    "    \n",
    "    if not agent_result[\"success\"]:\n",
    "        return EvaluationResult(\n",
    "            test_case_id=test_case.id,\n",
    "            query=test_case.query,\n",
    "            response=\"\",\n",
    "            metrics={},\n",
    "            response_time=agent_result[\"response_time\"],\n",
    "            success=False,\n",
    "            error_message=agent_result.get(\"response\", \"Unknown error\")\n",
    "        )\n",
    "    \n",
    "    # Evaluate response quality\n",
    "    quality_scores = await evaluate_response_quality(\n",
    "        test_case.query,\n",
    "        agent_result[\"response\"],\n",
    "        test_case.expected_criteria\n",
    "    )\n",
    "    \n",
    "    # Evaluate tool usage\n",
    "    tool_score = evaluate_tool_usage(\n",
    "        test_case.expected_tools,\n",
    "        agent_result[\"tool_calls\"]\n",
    "    )\n",
    "    \n",
    "    # Combine all metrics\n",
    "    metrics = {\n",
    "        **quality_scores,\n",
    "        \"tool_usage\": tool_score,\n",
    "        \"response_time\": agent_result[\"response_time\"]\n",
    "    }\n",
    "    \n",
    "    return EvaluationResult(\n",
    "        test_case_id=test_case.id,\n",
    "        query=test_case.query,\n",
    "        response=agent_result[\"response\"],\n",
    "        metrics=metrics,\n",
    "        response_time=agent_result[\"response_time\"],\n",
    "        success=True,\n",
    "        tool_calls=agent_result[\"tool_calls\"]\n",
    "    )\n",
    "\n",
    "print(\"\u2705 Test case evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a single case first\n",
    "demo_test = TEST_CASES[1]  # Population search\n",
    "demo_result = await evaluate_test_case(demo_test)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Demo Result for '{demo_test.id}':\")\n",
    "print(f\"Query: {demo_result.query}\")\n",
    "response_str = str(demo_result.response)\n",
    "print(f\"Response: {response_str[:200]}...\" if len(response_str) > 200 else f\"Response: {response_str}\")\n",
    "print(f\"Response Time: {demo_result.response_time:.3f}s\")\n",
    "print(f\"Tool Calls: {demo_result.tool_calls}\")\n",
    "print(f\"Success: {demo_result.success}\")\n",
    "print(f\"Metrics: {demo_result.metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_full_evaluation(test_cases: List[TestCase]) -> Dict[str, Any]:\n",
    "    \"\"\"Run evaluation on all test cases\"\"\"\n",
    "    print(f\"\ud83d\ude80 Starting evaluation of {len(test_cases)} test cases...\")\n",
    "    \n",
    "    results = []\n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n[{i}/{len(test_cases)}] Processing: {test_case.id}\")\n",
    "        result = await evaluate_test_case(test_case)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Brief pause between tests\n",
    "        await asyncio.sleep(1)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary = calculate_summary(results)\n",
    "    \n",
    "    return {\n",
    "        \"agent_name\": AGENT_NAME,\n",
    "        \"total_test_cases\": len(test_cases),\n",
    "        \"results\": [result.to_dict() for result in results],\n",
    "        \"summary\": summary,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def calculate_summary(results: List[EvaluationResult]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate summary statistics\"\"\"\n",
    "    successful_results = [r for r in results if r.success]\n",
    "    \n",
    "    if not successful_results:\n",
    "        return {\"error\": \"No successful test cases\"}\n",
    "    \n",
    "    # Average scores\n",
    "    metrics = [\"helpfulness\", \"accuracy\", \"clarity\", \"professionalism\", \"completeness\", \"tool_usage\"]\n",
    "    avg_scores = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        scores = [r.metrics.get(metric, 0) for r in successful_results if metric in r.metrics]\n",
    "        avg_scores[metric] = sum(scores) / len(scores) if scores else 0\n",
    "    \n",
    "    # Response time statistics\n",
    "    response_times = sorted([r.response_time for r in successful_results])\n",
    "    n = len(response_times)\n",
    "    \n",
    "    percentiles = {\n",
    "        \"p50\": response_times[n//2] if n > 0 else 0,\n",
    "        \"p90\": response_times[int(n*0.9)] if n > 0 else 0,\n",
    "        \"p95\": response_times[int(n*0.95)] if n > 0 else 0,\n",
    "        \"p99\": response_times[int(n*0.99)] if n > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"success_rate\": len(successful_results) / len(results),\n",
    "        \"average_scores\": avg_scores,\n",
    "        \"overall_score\": sum(avg_scores.values()) / len(avg_scores) if avg_scores else 0,\n",
    "        \"response_time_percentiles\": percentiles,\n",
    "        \"total_successful\": len(successful_results),\n",
    "        \"total_failed\": len(results) - len(successful_results)\n",
    "    }\n",
    "\n",
    "print(\"\u2705 Full evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation\n",
    "evaluation_results = await run_full_evaluation(TEST_CASES)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udcca EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display results\n",
    "summary = evaluation_results.get(\"summary\", {})\n",
    "print(f\"\\n\ud83e\udd16 Agent: {evaluation_results['agent_name']}\")\n",
    "print(f\"\ud83d\udcdd Total Test Cases: {evaluation_results['total_test_cases']}\")\n",
    "print(f\"\u2705 Success Rate: {summary.get('success_rate', 0):.1%}\")\n",
    "print(f\"\ud83c\udfaf Overall Score: {summary.get('overall_score', 0):.2f}/5.0\")\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 QUALITY METRICS (1-5 scale):\")\n",
    "avg_scores = summary.get(\"average_scores\", {})\n",
    "for metric, score in avg_scores.items():\n",
    "    if metric != \"response_time\":\n",
    "        emoji = \"\ud83d\udfe2\" if score >= 4.0 else \"\ud83d\udfe1\" if score >= 3.0 else \"\ud83d\udd34\"\n",
    "        print(f\"  {emoji} {metric.title()}: {score:.2f}\")\n",
    "\n",
    "print(\"\\n\u23f1\ufe0f  RESPONSE TIME PERCENTILES:\")\n",
    "percentiles = summary.get(\"response_time_percentiles\", {})\n",
    "for p, time_val in percentiles.items():\n",
    "    print(f\"  {p.upper()}: {time_val:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "output_file = f\"evaluation_results_{AGENT_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(f\"\ud83d\udcbe Results saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}