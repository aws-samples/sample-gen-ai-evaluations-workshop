{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Search Agent Evaluation Framework\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook evaluates the city search agent running on Amazon Bedrock AgentCore Runtime.\n",
    "\n",
    "### Pre-Requisite\n",
    "Execute 05-04-01-Agentic-Metrics-AgentCore.ipynb before this notebook\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "- **Multi-dimensional Quality Assessment**: Helpfulness, accuracy, clarity, professionalism, completeness\n",
    "- **Tool Usage Analysis**: web_search tool usage patterns\n",
    "- **Performance Metrics**: Response times and success rates\n",
    "- **LLM-as-Judge**: Claude Sonnet for objective evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (1.40.30)\n",
      "Requirement already satisfied: requests in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (2.32.5)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.30 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from boto3) (1.40.30)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from boto3) (0.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from botocore<1.41.0,>=1.40.30->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/skoppar/workspace/sample-gen-ai-evaluations-workshop/.venv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.30->boto3) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets retrieve the citysearch agent arn that was stored in the notebook and setup the evaluator model to be used as the Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Configured for agent citysearch, with endpoint arn:aws:bedrock-agentcore:us-east-1:XXXXXXXXXXXX:runtime/citysearch-583XaH96wT\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "%store -r citysearch_agent_arn\n",
    "AGENT_NAME = \"citysearch\" \n",
    "EVALUATOR_MODEL = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "AGENT_ENDPOINT = citysearch_agent_arn\n",
    "AGENT_QUALIFIER=\"DEFAULT\"\n",
    "print(f\"\u2705 Configured for agent {AGENT_NAME}, with endpoint {AGENT_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Dependencies loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import requests\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# AWS clients\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "print(\"\u2705 Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Lets define classes for Test Cases and Evaluation responses\n",
    "\n",
    "Three specific tests for a city search agent:\n",
    "\n",
    "    Basic greeting - Tests politeness, expects no tools\n",
    "\n",
    "    Population query - Tests factual lookup, expects web_search tool\n",
    "\n",
    "    Area query - Tests measurement data, expects web_search tool\n",
    "\n",
    "Each test defines what tools should be used and what criteria make a good response, enabling automated evaluation of agent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Loaded 3 test cases\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    id: str\n",
    "    query: str\n",
    "    category: str\n",
    "    expected_tools: List[str]\n",
    "    expected_criteria: Dict[str, Any]\n",
    "    description: str\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    test_case_id: str\n",
    "    query: str\n",
    "    response: str\n",
    "    metrics: Dict[str, float]\n",
    "    response_time: float\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "    tool_calls: List[str] = None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        # Convert to dict manually to avoid serialization issues\n",
    "        return {\n",
    "            \"test_case_id\": self.test_case_id,\n",
    "            \"query\": self.query,\n",
    "            \"response\": str(self.response),  # Ensure string conversion\n",
    "            \"metrics\": dict(self.metrics),\n",
    "            \"response_time\": self.response_time,\n",
    "            \"success\": self.success,\n",
    "            \"error_message\": self.error_message,\n",
    "            \"tool_calls\": list(self.tool_calls) if self.tool_calls else []\n",
    "        }\n",
    "\n",
    "# Test cases for city search agent\n",
    "TEST_CASES = [\n",
    "    TestCase(\n",
    "        id=\"basic_greeting\",\n",
    "        query=\"Hi, I need help with finding information about cities\",\n",
    "        category=\"basic_inquiry\",\n",
    "        expected_tools=[],\n",
    "        expected_criteria={\"should_be_polite\": True, \"should_ask_for_details\": True},\n",
    "        description=\"Basic greeting and help request\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"city_population_search\",\n",
    "        query=\"What is the population of Seattle?\",\n",
    "        category=\"population_inquiry\",\n",
    "        expected_tools=[\"web_search\"],\n",
    "        expected_criteria={\"should_provide_population\": True, \"should_be_accurate\": True},\n",
    "        description=\"City population information request\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"city_area_search\",\n",
    "        query=\"How large is Los Angeles in square miles?\",\n",
    "        category=\"area_inquiry\",\n",
    "        expected_tools=[\"web_search\"],\n",
    "        expected_criteria={\"should_provide_area\": True, \"should_be_clear\": True},\n",
    "        description=\"City area information request\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"\u2705 Loaded {len(TEST_CASES)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Define the methods for to invoke agent which includes evaluating LLM responses as well as tool use\n",
    "For tool use detection, X-Ray observability is primarily used. The implementation looks for gen_ai.tool.name annotations in trace segments of both main and sub segments. As a fallback, content analysis of the output is used to determine tool use. \n",
    "The implementation below provides the details of the actual agent invocation with tools used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Agent invocation functions defined\n"
     ]
    }
   ],
   "source": [
    "# AgentCore client using bedrock-agentcore service\n",
    "agentcore_client = boto3.client('bedrock-agentcore', region_name='us-east-1')\n",
    "\n",
    "async def invoke_agent(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Invoke the agent using AgentCore Runtime\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        payload = json.dumps({\"prompt\": query})\n",
    "        session_id = f\"eval-session-{uuid.uuid4()}\"\n",
    "        \n",
    "        response = agentcore_client.invoke_agent_runtime(\n",
    "            agentRuntimeArn=AGENT_ENDPOINT,\n",
    "            runtimeSessionId=session_id,\n",
    "            payload=payload,\n",
    "            qualifier=AGENT_QUALIFIER\n",
    "        )\n",
    "        \n",
    "        print(\"AgentCore Response keys:\", list(response.keys()))\n",
    "        \n",
    "        # Extract response text from StreamingBody\n",
    "        response_text = \"\"\n",
    "        if isinstance(response, dict) and 'response' in response:\n",
    "            streaming_body = response['response']\n",
    "            if hasattr(streaming_body, 'read'):\n",
    "                response_text = streaming_body.read().decode('utf-8')\n",
    "                print(f\"Extracted response text: {response_text[:200]}...\")\n",
    "            else:\n",
    "                response_text = str(streaming_body)\n",
    "        else:\n",
    "            response_text = str(response)\n",
    "            \n",
    "        # Extract tool calls from response metadata or content\n",
    "        log_group_name = extract_agent_log_name(AGENT_ENDPOINT)\n",
    "        tool_calls = extract_tool_calls_from_agentcore_observability(session_id, log_group_name, AGENT_QUALIFIER) \n",
    "        \n",
    "        return {\n",
    "            \"response\": response_text,\n",
    "            \"success\": True,\n",
    "            \"tool_calls\": tool_calls,\n",
    "            \"response_time\": time.time() - start_time,\n",
    "            \"session_id\": session_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error invoking agent: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return {\n",
    "            \"response\": error_msg,\n",
    "            \"success\": False,\n",
    "            \"tool_calls\": [],\n",
    "            \"response_time\": time.time() - start_time\n",
    "        }\n",
    "\n",
    "def extract_agent_log_name(arn):\n",
    "    return arn.split('/')[-1]\n",
    "\n",
    "\n",
    "### Below methods utilize Cloudwatch \n",
    "def extract_tool_calls_from_agentcore_observability(session_id, log_group_name, agent_qualifier, log_group_prefix='/aws/bedrock-agentcore/runtimes'):\n",
    "    logs_client = boto3.client('logs')\n",
    "    log_group_name = f\"{log_group_prefix}/{log_group_name}-{agent_qualifier}\"\n",
    "    print(\"Log group name\", log_group_name)\n",
    "    response = logs_client.filter_log_events(\n",
    "        logGroupName=log_group_name,\n",
    "        filterPattern=session_id\n",
    "    )\n",
    "    logs_list = [event['message'] for event in response['events']]\n",
    "    return extract_logs_for_session(logs_list)\n",
    "\n",
    "def extract_tooluse_from_log(log_message):\n",
    "    tools = []\n",
    "    try:\n",
    "        log_data = json.loads(log_message)\n",
    "        \n",
    "        # Navigate to output messages\n",
    "        messages = log_data.get('body', {}).get('output', {}).get('messages', [])\n",
    "        \n",
    "        for message in messages:\n",
    "            content = message.get('content', {}).get('content', '')\n",
    "            \n",
    "            # Parse the content JSON string\n",
    "            if content:\n",
    "                content_data = json.loads(content)\n",
    "                # Extract toolUse from each item\n",
    "                for item in content_data:\n",
    "                    if 'toolUse' in item:\n",
    "                        tool_name = item['toolUse']['name']\n",
    "                        tools.append(tool_name)\n",
    "                        \n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Error parsing log: {e}\")\n",
    "    \n",
    "    return tools\n",
    "\n",
    "def extract_logs_for_session(logs_list):\n",
    "    all_tools = []\n",
    "    for log in logs_list:\n",
    "        tools = extract_tooluse_from_log(log)\n",
    "        all_tools.extend(tools)\n",
    "    return list(set(all_tools))\n",
    "\n",
    "print(\"\u2705 Agent invocation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Next lets configure the prompts to test against the evaluator model with the response generated and response expected. The evaluator model provides evaluation metrics for both the LLM as well as tool use responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "async def evaluate_response_quality(query: str, response: str, criteria: Dict[str, Any]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate response quality using Claude as judge\"\"\"\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"\n",
    "    You are an expert evaluator for city search AI agents. Evaluate the following response on a scale of 1-5 for each metric.\n",
    "\n",
    "    Customer Query: {query}\n",
    "    Agent Response: {response}\n",
    "\n",
    "    Evaluate on these metrics (1=Poor, 2=Below Average, 3=Average, 4=Good, 5=Excellent):\n",
    "\n",
    "    1. HELPFULNESS: Does the response address the user's needs and provide useful information?\n",
    "    2. ACCURACY: Is the information provided factually correct and reliable?\n",
    "    3. CLARITY: Is the response clear, well-structured, and easy to understand?\n",
    "    4. PROFESSIONALISM: Does the response maintain appropriate tone and professionalism?\n",
    "    5. COMPLETENESS: Does the response fully address all aspects of the query?\n",
    "\n",
    "    Expected criteria: {json.dumps(criteria, indent=2)}\n",
    "\n",
    "    Respond with ONLY a JSON object in this format:\n",
    "    {{\n",
    "        \"helpfulness\": <score>,\n",
    "        \"accuracy\": <score>,\n",
    "        \"clarity\": <score>,\n",
    "        \"professionalism\": <score>,\n",
    "        \"completeness\": <score>,\n",
    "        \"reasoning\": \"Brief explanation of scores\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response_obj = bedrock.invoke_model(\n",
    "            modelId=EVALUATOR_MODEL,\n",
    "            body=json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1000,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "                ]\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response_obj['body'].read())\n",
    "        content = result['content'][0]['text']\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        start_idx = content.find('{')\n",
    "        end_idx = content.rfind('}') + 1\n",
    "        json_str = content[start_idx:end_idx]\n",
    "        \n",
    "        scores = json.loads(json_str)\n",
    "        return {k: v for k, v in scores.items() if k != \"reasoning\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in quality evaluation: {e}\")\n",
    "        return {\n",
    "            \"helpfulness\": 0.0,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"clarity\": 0.0,\n",
    "            \"professionalism\": 0.0,\n",
    "            \"completeness\": 0.0\n",
    "        }\n",
    "\n",
    "def evaluate_tool_usage(expected_tools: List[str], actual_tools: List[str]) -> float:\n",
    "    \"\"\"Evaluate tool usage effectiveness\"\"\"\n",
    "    if not expected_tools:\n",
    "        return 5.0 if not actual_tools else 3.0\n",
    "    \n",
    "    if not actual_tools:\n",
    "        print(f\"Expected tools {expected_tools}, but no tools were called\")\n",
    "        return 0.0\n",
    "    \n",
    "    expected_set = set(expected_tools)\n",
    "    actual_set = set(actual_tools)\n",
    "    \n",
    "    precision = len(expected_set.intersection(actual_set)) / len(actual_set) if actual_set else 0\n",
    "    recall = len(expected_set.intersection(actual_set)) / len(expected_set) if expected_set else 0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1 * 5  # Scale to 0-5\n",
    "\n",
    "print(\"\u2705 Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Lets test a single Testcase end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Test case evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "async def evaluate_test_case(test_case: TestCase) -> EvaluationResult:\n",
    "    \"\"\"Evaluate a single test case\"\"\"\n",
    "    print(f\"\ud83d\udd0d Evaluating: {test_case.id} - {test_case.description}\")\n",
    "    \n",
    "    # Invoke agent\n",
    "    agent_result = await invoke_agent(test_case.query)\n",
    "    \n",
    "    if not agent_result[\"success\"]:\n",
    "        return EvaluationResult(\n",
    "            test_case_id=test_case.id,\n",
    "            query=test_case.query,\n",
    "            response=\"\",\n",
    "            metrics={},\n",
    "            response_time=agent_result[\"response_time\"],\n",
    "            success=False,\n",
    "            error_message=agent_result.get(\"response\", \"Unknown error\")\n",
    "        )\n",
    "    \n",
    "    # Evaluate response quality\n",
    "    quality_scores = await evaluate_response_quality(\n",
    "        test_case.query,\n",
    "        agent_result[\"response\"],\n",
    "        test_case.expected_criteria\n",
    "    )\n",
    "    \n",
    "    # Evaluate tool usage\n",
    "    tool_score = evaluate_tool_usage(\n",
    "        test_case.expected_tools,\n",
    "        agent_result[\"tool_calls\"]\n",
    "    )\n",
    "    \n",
    "    # Combine all metrics\n",
    "    metrics = {\n",
    "        **quality_scores,\n",
    "        \"tool_usage\": tool_score,\n",
    "        \"response_time\": agent_result[\"response_time\"]\n",
    "    }\n",
    "    \n",
    "    return EvaluationResult(\n",
    "        test_case_id=test_case.id,\n",
    "        query=test_case.query,\n",
    "        response=agent_result[\"response\"],\n",
    "        metrics=metrics,\n",
    "        response_time=agent_result[\"response_time\"],\n",
    "        success=True,\n",
    "        tool_calls=agent_result[\"tool_calls\"]\n",
    "    )\n",
    "\n",
    "print(\"\u2705 Test case evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Evaluating: city_population_search - City population information request\n",
      "AgentCore Response keys: ['ResponseMetadata', 'runtimeSessionId', 'traceId', 'baggage', 'contentType', 'statusCode', 'response']\n",
      "Extracted response text: \"The population of Seattle, according to the most recent data from the US Census Bureau's population estimates, is approximately 755,078 as of 2023. \\n\\nHere is the information in XML format:\\n\\n```xm...\n",
      "Log group name /aws/bedrock-agentcore/runtimes/citysearch-583XaH96wT-DEFAULT\n",
      "\n",
      "\ud83d\udcca Demo Result for 'city_population_search':\n",
      "Query: What is the population of Seattle?\n",
      "Response: \"The population of Seattle, according to the most recent data from the US Census Bureau's population estimates, is approximately 755,078 as of 2023. \\n\\nHere is the information in XML format:\\n\\n```xm...\n",
      "Response Time: 5.349s\n",
      "Tool Calls: ['web_search']\n",
      "Success: True\n",
      "Metrics: {'helpfulness': 5, 'accuracy': 4, 'clarity': 5, 'professionalism': 5, 'completeness': 5, 'tool_usage': 5.0, 'response_time': 5.348819971084595}\n"
     ]
    }
   ],
   "source": [
    "# Test a single case first\n",
    "demo_test = TEST_CASES[1]  # Population search\n",
    "demo_result = await evaluate_test_case(demo_test)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Demo Result for '{demo_test.id}':\")\n",
    "print(f\"Query: {demo_result.query}\")\n",
    "response_str = str(demo_result.response)\n",
    "print(f\"Response: {response_str[:200]}...\" if len(response_str) > 200 else f\"Response: {response_str}\")\n",
    "print(f\"Response Time: {demo_result.response_time:.3f}s\")\n",
    "print(f\"Tool Calls: {demo_result.tool_calls}\")\n",
    "print(f\"Success: {demo_result.success}\")\n",
    "print(f\"Metrics: {demo_result.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5) Now that we see the evaluation working end to end for a single testcase, lets run through all the testcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Full evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "async def run_full_evaluation(test_cases: List[TestCase]) -> Dict[str, Any]:\n",
    "    \"\"\"Run evaluation on all test cases\"\"\"\n",
    "    print(f\"\ud83d\ude80 Starting evaluation of {len(test_cases)} test cases...\")\n",
    "    \n",
    "    results = []\n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n[{i}/{len(test_cases)}] Processing: {test_case.id}\")\n",
    "        result = await evaluate_test_case(test_case)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Brief pause between tests\n",
    "        await asyncio.sleep(1)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary = calculate_summary(results)\n",
    "    \n",
    "    return {\n",
    "        \"agent_name\": AGENT_NAME,\n",
    "        \"total_test_cases\": len(test_cases),\n",
    "        \"results\": [result.to_dict() for result in results],\n",
    "        \"summary\": summary,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def calculate_summary(results: List[EvaluationResult]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate summary statistics\"\"\"\n",
    "    successful_results = [r for r in results if r.success]\n",
    "    \n",
    "    if not successful_results:\n",
    "        return {\"error\": \"No successful test cases\"}\n",
    "    \n",
    "    # Average scores\n",
    "    metrics = [\"helpfulness\", \"accuracy\", \"clarity\", \"professionalism\", \"completeness\", \"tool_usage\"]\n",
    "    avg_scores = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        scores = [r.metrics.get(metric, 0) for r in successful_results if metric in r.metrics]\n",
    "        avg_scores[metric] = sum(scores) / len(scores) if scores else 0\n",
    "    \n",
    "    # Response time statistics\n",
    "    response_times = sorted([r.response_time for r in successful_results])\n",
    "    n = len(response_times)\n",
    "    \n",
    "    percentiles = {\n",
    "        \"p50\": response_times[n//2] if n > 0 else 0,\n",
    "        \"p90\": response_times[int(n*0.9)] if n > 0 else 0,\n",
    "        \"p95\": response_times[int(n*0.95)] if n > 0 else 0,\n",
    "        \"p99\": response_times[int(n*0.99)] if n > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"success_rate\": len(successful_results) / len(results),\n",
    "        \"average_scores\": avg_scores,\n",
    "        \"overall_score\": sum(avg_scores.values()) / len(avg_scores) if avg_scores else 0,\n",
    "        \"response_time_percentiles\": percentiles,\n",
    "        \"total_successful\": len(successful_results),\n",
    "        \"total_failed\": len(results) - len(successful_results)\n",
    "    }\n",
    "\n",
    "print(\"\u2705 Full evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Starting evaluation of 3 test cases...\n",
      "\n",
      "[1/3] Processing: basic_greeting\n",
      "\ud83d\udd0d Evaluating: basic_greeting - Basic greeting and help request\n",
      "AgentCore Response keys: ['ResponseMetadata', 'runtimeSessionId', 'traceId', 'baggage', 'contentType', 'statusCode', 'response']\n",
      "Extracted response text: \"Of course, I'd be happy to help you find information about cities! Whether you're looking for details about a specific city's demographics, history, culture, geography, or any other aspect, I can gui...\n",
      "Log group name /aws/bedrock-agentcore/runtimes/citysearch-583XaH96wT-DEFAULT\n",
      "\n",
      "[2/3] Processing: city_population_search\n",
      "\ud83d\udd0d Evaluating: city_population_search - City population information request\n",
      "AgentCore Response keys: ['ResponseMetadata', 'runtimeSessionId', 'traceId', 'baggage', 'contentType', 'statusCode', 'response']\n",
      "Extracted response text: \"<thinking>\\nAccording to the latest data from the 'web_search' tool, Seattle's population has surpassed 800,000 people, as indicated by the state's Office of Financial Management. As of April 2024, t...\n",
      "Log group name /aws/bedrock-agentcore/runtimes/citysearch-583XaH96wT-DEFAULT\n",
      "\n",
      "[3/3] Processing: city_area_search\n",
      "\ud83d\udd0d Evaluating: city_area_search - City area information request\n",
      "AgentCore Response keys: ['ResponseMetadata', 'runtimeSessionId', 'traceId', 'baggage', 'contentType', 'statusCode', 'response']\n",
      "Extracted response text: \"<thinking> I have gathered the necessary information from the web_search tool. The city of Los Angeles spans approximately 469 square miles. I will now provide this information directly to the User.<...\n",
      "Log group name /aws/bedrock-agentcore/runtimes/citysearch-583XaH96wT-DEFAULT\n",
      "\n",
      "============================================================\n",
      "\ud83d\udcca EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "\ud83e\udd16 Agent: citysearch\n",
      "\ud83d\udcdd Total Test Cases: 3\n",
      "\u2705 Success Rate: 100.0%\n",
      "\ud83c\udfaf Overall Score: 4.22/5.0\n",
      "\n",
      "\ud83d\udcc8 QUALITY METRICS (1-5 scale):\n",
      "  \ud83d\udfe2 Helpfulness: 4.33\n",
      "  \ud83d\udfe1 Accuracy: 3.67\n",
      "  \ud83d\udfe2 Clarity: 4.33\n",
      "  \ud83d\udfe2 Professionalism: 4.33\n",
      "  \ud83d\udfe2 Completeness: 4.33\n",
      "  \ud83d\udfe2 Tool_Usage: 4.33\n",
      "\n",
      "\u23f1\ufe0f  RESPONSE TIME PERCENTILES:\n",
      "  P50: 17.418s\n",
      "  P90: 18.628s\n",
      "  P95: 18.628s\n",
      "  P99: 18.628s\n"
     ]
    }
   ],
   "source": [
    "# Run full evaluation\n",
    "evaluation_results = await run_full_evaluation(TEST_CASES)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udcca EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display results\n",
    "summary = evaluation_results.get(\"summary\", {})\n",
    "print(f\"\\n\ud83e\udd16 Agent: {evaluation_results['agent_name']}\")\n",
    "print(f\"\ud83d\udcdd Total Test Cases: {evaluation_results['total_test_cases']}\")\n",
    "print(f\"\u2705 Success Rate: {summary.get('success_rate', 0):.1%}\")\n",
    "print(f\"\ud83c\udfaf Overall Score: {summary.get('overall_score', 0):.2f}/5.0\")\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 QUALITY METRICS (1-5 scale):\")\n",
    "avg_scores = summary.get(\"average_scores\", {})\n",
    "for metric, score in avg_scores.items():\n",
    "    if metric != \"response_time\":\n",
    "        emoji = \"\ud83d\udfe2\" if score >= 4.0 else \"\ud83d\udfe1\" if score >= 3.0 else \"\ud83d\udd34\"\n",
    "        print(f\"  {emoji} {metric.title()}: {score:.2f}\")\n",
    "\n",
    "print(\"\\n\u23f1\ufe0f  RESPONSE TIME PERCENTILES:\")\n",
    "percentiles = summary.get(\"response_time_percentiles\", {})\n",
    "for p, time_val in percentiles.items():\n",
    "    print(f\"  {p.upper()}: {time_val:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcbe Results saved to: evaluation_results_citysearch_20250918_210208.json\n"
     ]
    }
   ],
   "source": [
    "# Save results to file\n",
    "output_file = f\"evaluation_results_{AGENT_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(f\"\ud83d\udcbe Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, \n",
    "1. We built defined testcases to test the agent for expected and actual responses. \n",
    "2. Utilized LLM as Judge to determine evals for LLM responses\n",
    "3. Utilized Cloudwatch log group name filtered for the current session id to determine tool invocations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}