{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strands Agent Evaluation Framework\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates comprehensive evaluation techniques for Strands agents using built-in observability features and custom evaluation metrics.\n",
    "\n",
    "### Strands Evaluation Dimensions:\n",
    "- **Agent Performance**: Measuring accuracy using ground truth datasets\n",
    "- **Tool Execution**: Analyzing tool selection and execution success rates\n",
    "- **Resource Efficiency**: Token usage, latency, and cycle duration analysis\n",
    "- **Agent Reliability**: Consistency across multiple test scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install strands-agents strands-agents-tools\n",
    "!pip install ddgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent, tool \n",
    "from strands.models import BedrockModel \n",
    "from googlesearch import search \n",
    "from bs4 import BeautifulSoup \n",
    "import requests \n",
    "import pandas as pd, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ground Truth Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "#this contains the city, state, population, and land area in square miles in 2024.\n",
    "gold_standard_city_pop = pd.read_csv('city_pop.csv')\n",
    "# Clean the dataset once when loading, wikipedia has commas in the numbers.\n",
    "gold_standard_city_pop['population'] = gold_standard_city_pop['population'].astype(str).str.replace(',', '').astype(float)\n",
    "gold_standard_city_pop['land_area_mi2'] = gold_standard_city_pop['land_area_mi2'].astype(str).str.replace(',', '').astype(float)\n",
    "\n",
    "# Show the first 3 rows, as a reference\n",
    "print(gold_standard_city_pop.head(3))  # First 3 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_city_guess(city, state, chatbot_response, dataset):\n",
    "    \"\"\"\n",
    "    Evaluate population and area guesses against the gold standard dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - city: str, city name\n",
    "    - state: str, state abbreviation (e.g., 'NY', 'CA')\n",
    "    - chatbot_response: Strands AgentResult object to be evaluated\n",
    "    - dataset: pandas DataFrame, the gold standard dataset\n",
    "    \n",
    "    Returns:\n",
    "    - dict with percent errors for population and area, and total tokens, execution time, and tool calls.\n",
    "    \n",
    "    Raises:\n",
    "    - ValueError if city/state combination not found\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean the city name for matching\n",
    "    city_clean = city.strip()\n",
    "\n",
    "    \n",
    "    #use regex to grab the final answer as numbers\n",
    "    final_msg = chatbot_response.message['content'][0]['text']\n",
    "    try:\n",
    "        guessed_pop, guessed_area = int(re.search(r'<pop>(.*?)</pop>', final_msg).group(1)), float(re.search(r'<area>(.*?)</area>', final_msg).group(1))\n",
    "    except:\n",
    "        raise ValueError(f\"XML tags not found in reply\")\n",
    "\n",
    "    \n",
    "    #extract agent loop metrics\n",
    "    total_tokens = chatbot_response.metrics.accumulated_usage['totalTokens']\n",
    "    total_time = sum(chatbot_response.metrics.cycle_durations)\n",
    "    \n",
    "    tool_calls = 0\n",
    "    for t in chatbot_response.metrics.tool_metrics.keys():\n",
    "        tool_calls+= chatbot_response.metrics.tool_metrics[t].call_count\n",
    "\n",
    "    \n",
    "    # Find the city in the dataset\n",
    "    # Use case-insensitive matching and handle potential annotations\n",
    "    mask = (dataset['city'].str.replace(r'\\[.*\\]', '', regex=True).str.strip().str.lower() == city_clean.lower()) & \\\n",
    "           (dataset['state'].str.upper() == state.upper())\n",
    "    \n",
    "    matching_rows = dataset[mask]\n",
    "    \n",
    "    if len(matching_rows) == 0:\n",
    "        raise ValueError(f\"City '{city}' in state '{state}' not found in dataset\")\n",
    "    \n",
    "    if len(matching_rows) > 1:\n",
    "        print(f\"Warning: Multiple matches found for {city}, {state}. Using first match.\")\n",
    "    \n",
    "    # Get the actual values\n",
    "    actual_pop = matching_rows.iloc[0]['population']\n",
    "    actual_area = matching_rows.iloc[0]['land_area_mi2']\n",
    "    \n",
    "    # Calculate percent error: |actual - guess| / actual * 100\n",
    "    pop_error = abs(actual_pop - guessed_pop) / actual_pop * 100\n",
    "    area_error = abs(actual_area - guessed_area) / actual_area * 100\n",
    "    \n",
    "    return {\n",
    "        'city': matching_rows.iloc[0]['city'],\n",
    "        'state': matching_rows.iloc[0]['state'],\n",
    "        'actual_population': actual_pop,\n",
    "        'guessed_population': guessed_pop,\n",
    "        'population_error_percent': round(pop_error, 2),\n",
    "        'actual_area': actual_area,\n",
    "        'guessed_area': guessed_area,\n",
    "        'area_error_percent': round(area_error, 2),\n",
    "        'total_tokens': total_tokens,\n",
    "        'total_time': total_time,\n",
    "        'tool_calls': tool_calls\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strands Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def web_search(topic: str) -> str:\n",
    "    \"\"\"Search Duck Duck go Service for a given topic.\"\"\"\n",
    "    try:\n",
    "        from ddgs import DDGS\n",
    "        results = DDGS().text(topic, max_results=5)\n",
    "        \n",
    "        if not results:\n",
    "            return \"No search results found\"\n",
    "        \n",
    "        result_string = \"\"\n",
    "        for i, result in enumerate(results):\n",
    "            result_string += f\"Result {i+1}: {result.get('title', 'No title')}\\nURL: {result.get('href', 'No URL')}\\nSnippet: {result.get('body', 'No description')}\\n\\n\"\n",
    "        \n",
    "        return result_string\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\"\n",
    "    \n",
    "@tool      \n",
    "def get_page(url: str) -> str:\n",
    "    \"\"\"this function takes a URL and returns the raw text from that page.\n",
    "    it can be used to get more info based on a Google search result listing.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    bs = BeautifulSoup(response.text,'html.parser')\n",
    "    return bs.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. AWS Bedrock Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "#A custom config for Bedrock to only allow short connections - for our demo we expect all calls to be fast.\n",
    "#here we turn off retries, and we time out after 20 seconds.\n",
    "quick_config = Config(\n",
    "    connect_timeout=5,\n",
    "    read_timeout=20,\n",
    "    retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "longer_config = Config(\n",
    "    connect_timeout=10,\n",
    "    read_timeout=60,\n",
    "    retries={\"max_attempts\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Single Agent Baseline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the chatbot.  We'll use Nova Micro to optimize for latency, cost, and capacity\n",
    "chatbot_model_name = \"us.amazon.nova-micro-v1:0\"\n",
    "#add custom timeout for the model, to keep the tool from hanging or retrying too much.\n",
    "chatbot_model = BedrockModel(\n",
    "    model_id=chatbot_model_name,\n",
    "    boto_client_config=quick_config    \n",
    ")\n",
    "chatbot = Agent(tools=[web_search,get_page], model=chatbot_model)\n",
    "#Call the chat bot with a simple request.\n",
    "prompt = \"\"\"How many people live in New York, and what's the area of the city in square miles?\n",
    "After you respond, also include your answer in 'pop' and 'area' XML tags, for programatic processing.\n",
    "The values in the XML tags should only be numbers, no words or commas.\"\"\"\n",
    "chatbot_response = chatbot(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate_city_guess(\"El monte\", \"CA\", chatbot_response, gold_standard_city_pop)\n",
    "print(f\"Population error: {result['population_error_percent']}%\")\n",
    "print(f\"Area error: {result['area_error_percent']}%\")\n",
    "print(f\"Total Tokens: {result['total_tokens']} tokens\")\n",
    "print(f\"Total Time: {result['total_time']:.2f} seconds\")\n",
    "print(f\"Tool Calls: {result['tool_calls']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Single Model Evaluation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def eval_model(model_name: str) -> str:\n",
    "    \"\"\"Start an evaluator for a particular model.\n",
    "    model_name is the model endpoint to be evaluated.\n",
    "    Retruns a string containing information about this model.\n",
    "    \"\"\"\n",
    "    #add custom timeout for the model, to keep the tool from hanging or retrying too much.\n",
    "    chatbot_model = BedrockModel(\n",
    "        model_id=model_name,\n",
    "        boto_client_config=quick_config    \n",
    "    )\n",
    "    \n",
    "    chatbot = Agent(tools=[web_search,get_page], model=chatbot_model, callback_handler=None)# callback_handler=None to suppress sub agent print outs\n",
    "    #Call the chat bot with a simple request.\n",
    "    prompt = \"\"\"How many people live in Phoenix, AZ, and what's the area of the city in square miles?\n",
    "    After you respond, also include your answer in 'pop' and 'area' XML tags, for programatic processing.\n",
    "    The values in the XML tags should only be numbers, no words or commas.\"\"\"\n",
    "    chatbot_response = chatbot(prompt)\n",
    "    result = evaluate_city_guess(\"Phoenix\", \"AZ\", chatbot_response, gold_standard_city_pop)\n",
    "    result_string = \"\"\n",
    "    result_string = result_string + f\"Population error: {result['population_error_percent']}%\" + '\\n'\n",
    "    result_string = result_string + f\"Area error: {result['area_error_percent']}%\" + '\\n'\n",
    "    result_string = result_string + f\"Total Tokens: {result['total_tokens']} tokens\" + '\\n'\n",
    "    result_string = result_string + f\"Total Time: {result['total_time']:.2f} seconds\" + '\\n'\n",
    "    result_string = result_string + f\"Tool Calls: {result['tool_calls']}\"\n",
    "    print (result_string)\n",
    "    return result_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multi-Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_prompt = \"\"\"\n",
    "Use the eval_model tool to evaluate these models:\n",
    "Nova Micro: \"us.amazon.nova-micro-v1:0\",\n",
    "Nova Lite: \"us.amazon.nova-lite-v1:0\",\n",
    "Nova Pro: \"us.amazon.nova-pro-v1:0\",\n",
    "Claude 3 Haiku: \"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "Claude 3 Sonnet: \"us.anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "Provide a table comparason on the results, and include columns for all evaluation data points, including number of tool calls, and the number of times the model failed to evaluate and had to be retried.\n",
    "Do not include the endpoint names in the table, only the model names, to save space.\n",
    "If a model fails to evaluate, you should retry it up to 3 times.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Agent(tools=[eval_model], model=chatbot_model)\n",
    "evaluator_response = evaluator(evaluator_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Multi-City Evaluation Framework\n",
    "\n",
    "### Expanding Evaluation to Multiple Data Points\n",
    "\n",
    "Next, we'll expand our evaluator to be able to check based on more than one data point. We add the calculator too to assist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Evaluate mathematical expressions safely. Use for calculations like population density.\"\"\"\n",
    "    try:\n",
    "        allowed_chars = set('0123456789+-*/()., ')\n",
    "        if not all(c in allowed_chars for c in expression):\n",
    "            return \"Error: Invalid characters\"\n",
    "        return str(eval(expression))\n",
    "    except:\n",
    "        return \"Error: Invalid calculation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-City Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import random\n",
    "\n",
    "def evaluate_multiple_cities(model_name, num_cities=3):\n",
    "    \"\"\"Multi-city evaluation using original evaluate_city_guess function\"\"\"\n",
    "    MAJOR_CITIES = [\n",
    "        (\"New York\", \"NY\"), (\"Los Angeles\", \"CA\"), (\"Chicago\", \"IL\"),\n",
    "        (\"Houston\", \"TX\"), (\"Phoenix\", \"AZ\"), (\"Philadelphia\", \"PA\")\n",
    "    ]\n",
    "    \n",
    "    test_cities = random.sample(MAJOR_CITIES, num_cities)\n",
    "    results = []\n",
    "    \n",
    "    for city, state in test_cities:\n",
    "        try:\n",
    "            chatbot_model = BedrockModel(model_id=model_name, boto_client_config=quick_config)\n",
    "            chatbot = Agent(tools=[web_search, get_page, calculate], model=chatbot_model, callback_handler=None)\n",
    "            \n",
    "            prompt = f\"\"\"How many people live in {city}, {state}, and what's the area of the city in square miles?\n",
    "After you respond, also include your answer in 'pop' and 'area' XML tags, for programatic processing.\n",
    "The values in the XML tags should only be numbers, no words or commas.\"\"\"\n",
    "            \n",
    "            response = chatbot(prompt)\n",
    "            result = evaluate_city_guess(city, state, response, gold_standard_city_pop)\n",
    "            results.append(result)\n",
    "            print(f\"âœ“ {city}, {state}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed {city}, {state}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if results:\n",
    "        return {\n",
    "            'cities_tested': len(results),\n",
    "            'avg_population_error': round(statistics.mean([r['population_error_percent'] for r in results]), 2),\n",
    "            'avg_area_error': round(statistics.mean([r['area_error_percent'] for r in results]), 2),\n",
    "            'total_tokens': sum([r['total_tokens'] for r in results]),\n",
    "            'avg_time_per_city': round(statistics.mean([r['total_time'] for r in results]), 2),\n",
    "            'total_tool_calls': sum([r['tool_calls'] for r in results]),\n",
    "            'individual_results': results\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-City Evaluation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def eval_model_multi(model_name: str, num_cities: int = 3) -> str:\n",
    "    \"\"\"Multi-city version of eval_model using existing evaluation logic\"\"\"\n",
    "    results = evaluate_multiple_cities(model_name, num_cities)\n",
    "    \n",
    "    if results:\n",
    "        result_string = f\"Cities tested: {results['cities_tested']}\\n\"\n",
    "        result_string += f\"Avg population error: {results['avg_population_error']}%\\n\"\n",
    "        result_string += f\"Avg area error: {results['avg_area_error']}%\\n\"\n",
    "        result_string += f\"Total tokens: {results['total_tokens']}\\n\"\n",
    "        result_string += f\"Avg time per city: {results['avg_time_per_city']:.2f} seconds\\n\"\n",
    "        result_string += f\"Total tool calls: {results['total_tool_calls']}\"\n",
    "        print(result_string)\n",
    "        return result_string\n",
    "    else:\n",
    "        return \"Evaluation failed - no cities successfully processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Multi-City Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it\n",
    "results = evaluate_multiple_cities(\"us.amazon.nova-micro-v1:0\", 3)\n",
    "if results:\n",
    "    print(f\"Cities tested: {results['cities_tested']}\")\n",
    "    print(f\"Avg population error: {results['avg_population_error']}%\")\n",
    "    print(f\"Avg area error: {results['avg_area_error']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_evaluator = Agent(tools=[eval_model_multi], model=chatbot_model)\n",
    "\n",
    "multi_prompt = \"\"\"\n",
    "Use eval_model_multi to test these models on 3 cities each:\n",
    "- \"us.amazon.nova-lite-v1:0\"\n",
    "- \"us.anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "Create a comparison table with all metrics.\n",
    "\"\"\"\n",
    "\n",
    "multi_response = multi_evaluator(multi_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Tool Call Evaluation Framework\n",
    "\n",
    "### Tool Selection Accuracy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset = [\n",
    "  { \"id\": 1, \"input\": \"What is 234 + 876?\", \"expected_tool\": \"calculator\", \"expected_output\": \"1110\" },\n",
    "  { \"id\": 2, \"input\": \"Multiply 45 by 19.\", \"expected_tool\": \"calculator\", \"expected_output\": \"855\" },\n",
    "  { \"id\": 3, \"input\": \"What is (15 * 4) + 9?\", \"expected_tool\": \"calculator\", \"expected_output\": \"69\" },\n",
    "  { \"id\": 4, \"input\": \"Read the contents of notes.txt\", \"expected_tool\": \"file_read\", \"expected_output\": \"File contents of notes.txt\" },\n",
    "  { \"id\": 5, \"input\": \"Open and show me what's inside data.csv\", \"expected_tool\": \"file_read\", \"expected_output\": \"CSV content from data.csv\" },\n",
    "  { \"id\": 6, \"input\": \"Display everything in todo.md\", \"expected_tool\": \"file_read\", \"expected_output\": \"Markdown content of todo.md\" },\n",
    "  { \"id\": 7, \"input\": \"Write 'Hello World' into hello.txt\", \"expected_tool\": \"file_write\", \"expected_output\": \"File hello.txt created with 'Hello World'\" },\n",
    "  { \"id\": 8, \"input\": \"Save the text 'AgentCore Rocks!' into core.txt\", \"expected_tool\": \"file_write\", \"expected_output\": \"File core.txt created with text\" },\n",
    "  { \"id\": 9, \"input\": \"Create a file log.txt that contains 'run successful'\", \"expected_tool\": \"file_write\", \"expected_output\": \"File log.txt written\" },\n",
    "  { \"id\": 10, \"input\": \"Run Python code: print(2+3)\", \"expected_tool\": \"code_interpreter\", \"expected_output\": \"5\" },\n",
    "  { \"id\": 11, \"input\": \"Execute Python code: for i in range(3): print(i)\", \"expected_tool\": \"code_interpreter\", \"expected_output\": \"0\\n1\\n2\" },\n",
    "  { \"id\": 12, \"input\": \"Run a Python snippet to calculate factorial of 5\", \"expected_tool\": \"code_interpreter\", \"expected_output\": \"120\" },\n",
    "  { \"id\": 13, \"input\": \"What is the capital of France?\", \"expected_tool\": \"none\", \"expected_output\": \"Paris\" },\n",
    "  { \"id\": 14, \"input\": \"Who is the CEO of Amazon?\", \"expected_tool\": \"none\", \"expected_output\": \"Andy Jassy\" },\n",
    "  { \"id\": 15, \"input\": \"Divide 500 by 25.\", \"expected_tool\": \"calculator\", \"expected_output\": \"20\" },\n",
    "  { \"id\": 16, \"input\": \"Square root of 144?\", \"expected_tool\": \"calculator\", \"expected_output\": \"12\" },\n",
    "  { \"id\": 17, \"input\": \"Show me what's inside config.yaml\", \"expected_tool\": \"file_read\", \"expected_output\": \"YAML file content\" },\n",
    "  { \"id\": 18, \"input\": \"Write 'Done for today' in status.txt\", \"expected_tool\": \"file_write\", \"expected_output\": \"status.txt written\" },\n",
    "  { \"id\": 19, \"input\": \"Execute Python: sum([10,20,30])\", \"expected_tool\": \"code_interpreter\", \"expected_output\": \"60\" },\n",
    "  { \"id\": 20, \"input\": \"What is 99 * 99?\", \"expected_tool\": \"calculator\", \"expected_output\": \"9801\" }\n",
    "]\n",
    "\n",
    "with open('dataset.json', 'w') as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands_tools import calculator, file_read, current_time, file_write, code_interpreter\n",
    "# Create agent with multiple tools\n",
    "agent = Agent(\n",
    "    model=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    tools=[calculator, file_read, current_time, file_write, code_interpreter],\n",
    "    record_direct_tool_call = True\n",
    ")\n",
    "\n",
    "# Define tool-specific test cases\n",
    "\n",
    "# Track tool usage\n",
    "tool_usage_results = []\n",
    "for case in dataset:\n",
    "    response = agent(case[\"input\"])\n",
    "\n",
    "    # Extract used tools from the response metrics\n",
    "    used_tools = []\n",
    "    if hasattr(response, 'metrics') and hasattr(response.metrics, 'tool_metrics'):\n",
    "        for tool_name, tool_metric in response.metrics.tool_metrics.items():\n",
    "            if tool_metric.call_count > 0:\n",
    "                used_tools.append(tool_name)\n",
    "\n",
    "    tool_usage_results.append({\n",
    "        \"query\": case[\"input\"],\n",
    "        \"expected_tool\": case[\"expected_tool\"],\n",
    "        \"used_tools\": used_tools,\n",
    "        \"correct_tool_used\": case[\"expected_tool\"] in used_tools\n",
    "    })\n",
    "\n",
    "# Analyze tool usage accuracy\n",
    "correct_usage_count = sum(1 for result in tool_usage_results if result[\"correct_tool_used\"])\n",
    "accuracy = correct_usage_count / len(tool_usage_results)\n",
    "print('\\n Results:\\n')\n",
    "print(f\"Tool selection accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. LLM As a Judge Evaluation\n",
    "\n",
    "### Using Stronger Models to Evaluate Agent Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "import json\n",
    "\n",
    "# Create the agent to evaluate\n",
    "# Create the agent to evaluate\n",
    "agent = Agent(model=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\")\n",
    "\n",
    "# Create an evaluator agent with a stronger model\n",
    "evaluator = Agent(\n",
    "    model=\"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "    system_prompt=\"\"\"\n",
    "    You are an expert AI evaluator. Your job is to assess the quality of AI responses based on:\n",
    "    1. Accuracy - factual correctness of the response\n",
    "    2. Relevance - how well the response addresses the query\n",
    "    3. Completeness - whether all aspects of the query are addressed\n",
    "    4. Tool usage - appropriate use of available tools\n",
    "\n",
    "    Score each criterion from 1-5, where 1 is poor and 5 is excellent.\n",
    "    Provide an overall score and brief explanation for your assessment.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Load test cases\n",
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    test_cases = json.load(f)\n",
    "\n",
    "# Run evaluations\n",
    "evaluation_results = []\n",
    "for case in test_cases:\n",
    "    # Get agent response\n",
    "    print(case)\n",
    "    agent_response = agent(case['input'])\n",
    "\n",
    "    # Create evaluation prompt\n",
    "    eval_prompt = f\"\"\"\n",
    "    Query: {case['input']}\n",
    "\n",
    "    Response to evaluate:\n",
    "    {agent_response}\n",
    "\n",
    "    Expected response (if available):\n",
    "    {case.get('expected_output', 'Not provided')}\n",
    "\n",
    "    Please evaluate the response based on accuracy, relevance, completeness, and tool usage.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get evaluation\n",
    "    evaluation = evaluator(eval_prompt)\n",
    "\n",
    "    # Store results\n",
    "    evaluation_results.append({\n",
    "        \"test_id\": case.get(\"id\", \"\"),\n",
    "        \"query\": case[\"input\"],\n",
    "        \"agent_response\": str(agent_response),\n",
    "        \"evaluation\": evaluation.message['content']\n",
    "    })\n",
    "\n",
    "# Save evaluation results\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Advanced Strands Metrics Analysis\n",
    "\n",
    "### Detailed Performance Metrics Using Strands Observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent(\"What is the square root of 144?\")\n",
    "\n",
    "def display_metrics(result):\n",
    "    summary = result.metrics.get_summary()\n",
    "    \n",
    "    print(\" Agent Performance Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Core metrics\n",
    "    print(f\" Execution: {summary['total_cycles']} cycles in {summary['total_duration']:.2f}s\")\n",
    "    print(f\" Average cycle time: {summary['average_cycle_time']:.2f}s\")\n",
    "    \n",
    "    # Tool usage\n",
    "    print(f\"\\n Tool Performance:\")\n",
    "    for tool, data in summary['tool_usage'].items():\n",
    "        stats = data['execution_stats']\n",
    "        print(f\"   {tool}: {stats['call_count']} calls | {stats['success_rate']:.0%} success | {stats['average_time']*1000:.1f}ms avg\")\n",
    "    \n",
    "    # Token usage\n",
    "    usage = summary['accumulated_usage']\n",
    "    print(f\"\\n Token Usage:\")\n",
    "    print(f\"   Input: {usage['inputTokens']:,} | Output: {usage['outputTokens']:,} | Total: {usage['totalTokens']:,}\")\n",
    "    \n",
    "    # Latency\n",
    "    print(f\" Total latency: {summary['accumulated_metrics']['latencyMs']:,}ms\")\n",
    "    \n",
    "    # Cycle breakdown\n",
    "    print(f\"\\n Cycle Details:\")\n",
    "    for i, trace in enumerate(summary['traces'], 10):\n",
    "        if trace['duration']:\n",
    "            print(f\"   Cycle {i}: {trace['duration']:.2f}s\")\n",
    "display_metrics(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusions and Key Findings\n",
    "\n",
    "### Strands Agent Evaluation Framework Summary\n",
    "\n",
    "This comprehensive evaluation framework demonstrates multiple approaches for assessing Strands agent performance:\n",
    "\n",
    "**ðŸŽ¯ Accuracy Assessment:**\n",
    "- Ground truth validation using structured outputs (XML tags)\n",
    "- Population and area estimation error calculations\n",
    "- Multi-city consistency testing\n",
    "\n",
    "**âš¡ Performance Monitoring:**\n",
    "- Strands `AgentResult.metrics` for comprehensive analysis\n",
    "- Token usage tracking for cost optimization\n",
    "- Execution time and cycle duration measurement\n",
    "- Tool call frequency and success rate analysis\n",
    "\n",
    "**ðŸ”§ Tool Effectiveness:**\n",
    "- Tool selection accuracy across different task types\n",
    "- Multi-tool coordination assessment\n",
    "- Tool execution success rate monitoring\n",
    "\n",
    "**ðŸ“Š Advanced Evaluation Techniques:**\n",
    "- LLM-as-a-Judge for qualitative assessment\n",
    "- Batch evaluation for consistency analysis\n",
    "- Comparative model performance analysis\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "**Model Performance Patterns:**\n",
    "- Larger models (Claude, Nova Pro) show better accuracy but higher token costs\n",
    "- Smaller models (Nova Micro) are faster but less reliable with complex instructions\n",
    "- Tool selection accuracy varies significantly between model families\n",
    "\n",
    "**Strands Framework Benefits:**\n",
    "- Built-in observability provides comprehensive performance metrics\n",
    "- Agent cycle tracking enables detailed execution analysis\n",
    "- Tool metrics facilitate optimization of agent capabilities\n",
    "- Structured evaluation supports production monitoring\n",
    "\n",
    "**Evaluation Methodology Learnings:**\n",
    "- Structured output requirements (XML tags) are crucial for automated evaluation\n",
    "- Multi-city testing reveals consistency issues not apparent in single-case tests\n",
    "- LLM-as-a-Judge provides valuable qualitative insights\n",
    "- Tool call efficiency is as important as accuracy for production deployments\n",
    "\n",
    "### Production Recommendations:\n",
    "\n",
    "1. **Implement structured outputs** for automated evaluation pipelines\n",
    "2. **Use Strands metrics** for continuous performance monitoring\n",
    "3. **Establish accuracy baselines** using ground truth datasets\n",
    "4. **Monitor tool success rates** for reliability assessment\n",
    "5. **Track token efficiency** for cost optimization\n",
    "6. **Deploy LLM-as-a-Judge** for qualitative response evaluation\n",
    "\n",
    "### Future Enhancements:\n",
    "\n",
    "**Expanded Test Coverage:**\n",
    "- Additional domains beyond city demographics\n",
    "- More complex multi-step reasoning tasks\n",
    "- Real-time data accuracy validation\n",
    "\n",
    "**Advanced Metrics:**\n",
    "- Semantic similarity scoring for text outputs\n",
    "- Confidence calibration analysis\n",
    "- Error pattern classification\n",
    "\n",
    "**Automation Improvements:**\n",
    "- Continuous evaluation pipelines\n",
    "- A/B testing frameworks\n",
    "- Performance regression detection\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“š Reference**: This evaluation framework follows Strands documentation best practices for agent observability and performance measurement. For more details, see: https://strandsagents.com/latest/documentation/docs/user-guide/observability-evaluation/evaluation/\n",
    "\n",
    "**ðŸ”— Repository**: Save this notebook and datasets for reproducible evaluations and comparative analysis across different model versions and configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
