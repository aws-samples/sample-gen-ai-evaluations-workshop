{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "workshop-intro",
   "metadata": {},
   "source": [
    "# Understanding Basic LLM Metrics\n",
    "\n",
    "## Overview\n",
    "In this workshop, you'll learn to measure and analyze key operational metrics for Large Language Models (LLMs) within Amazon Bedrock. Understanding these metrics is crucial for:\n",
    "- Optimizing application performance\n",
    "- Managing costs effectively\n",
    "- Understanding latency vs accuracy\n",
    "\n",
    "## Key Metrics We'll Cover\n",
    "1. **Cost Metrics** - Token usage and pricing\n",
    "2. **Latency Metrics** - End-to-end response time\n",
    "3. **TTFT vs TTLT** - Time to First Token vs Time to Last Token\n",
    "4. **Throttling** - Rate limiting and error handling\n",
    "5. **Throughput** - Tokens per second and requests per minute\n",
    "\n",
    "## Use Case\n",
    "For this workshop our use case will be email summarization where we process incoming emails to extract key information, action items, and important details. This will demonstrates how LLM metrics impact real-world performance in a scenario requiring both accuracy and efficiency. We will compare how different models balance speed, cost, and quality when generating concise email summaries at scale.\n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with Bedrock access\n",
    "- Python 3.10+\n",
    "- boto3 library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "cloudwatch = boto3.client('cloudwatch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-metrics-section",
   "metadata": {},
   "source": [
    "## 1. Cost Metrics\n",
    "\n",
    "Understanding token usage is essential for cost optimization. Different models have different pricing structures based on input and output tokens. A link to bedrock model pricing can be found [here](https://aws.amazon.com/bedrock/pricing/#:~:text=Model%20pricing%20details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cost-calculator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom metric published: {'ResponseMetadata': {'RequestId': '70e519bb-9878-4aa8-9598-75152dc20d53', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '70e519bb-9878-4aa8-9598-75152dc20d53', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 04 Aug 2025 14:01:40 GMT'}, 'RetryAttempts': 0}}\n",
      "Cost Analysis Example:\n",
      "{\n",
      "  \"input_tokens\": 20000,\n",
      "  \"output_tokens\": 1500,\n",
      "  \"input_cost_usd\": \"$0.06000000\",\n",
      "  \"output_cost_usd\": \"$0.02250000\",\n",
      "  \"total_cost_usd\": \"$0.08250000\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Pricing (per 1K tokens)\n",
    "MODEL_PRICING = {\n",
    "    \"us.amazon.nova-lite-v1:0\": {\"input\": 0.00006, \"output\": 0.000015},\n",
    "    \"us.amazon.nova-pro-v1:0\": {\"input\": 0.0008, \"output\": 0.0002},\n",
    "    \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\": {\"input\": 0.003, \"output\": 0.015}\n",
    "}\n",
    "\n",
    "def calculate_cost(model_id: str, input_tokens: int, output_tokens: int) -> Dict:\n",
    "    \"\"\"Calculate the cost of a model invocation based on token usage.\"\"\"\n",
    "    if model_id not in MODEL_PRICING:\n",
    "        return {\"error\": f\"Pricing not available for {model_id}\"}\n",
    "    \n",
    "    pricing = MODEL_PRICING[model_id]\n",
    "    input_cost = (input_tokens / 1000) * pricing[\"input\"]\n",
    "    output_cost = (output_tokens / 1000) * pricing[\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    response = cloudwatch.put_metric_data(\n",
    "    Namespace='llm_custom_operational_metrics',  # A logical container for your metrics\n",
    "    MetricData=[\n",
    "        {\n",
    "            'MetricName': 'TotalCost',  # The name of your custom metric\n",
    "            'Value': total_cost,                  # The value of the metric\n",
    "            'Dimensions': [              # Optional: Add dimensions for more granular analysis\n",
    "                {\n",
    "                    'Name': 'Model',\n",
    "                    'Value': model_id\n",
    "                }\n",
    "            ]\n",
    "        }])\n",
    "\n",
    "    print(\"Custom metric published\")\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"input_cost_usd\": f\"${input_cost:.8f}\", \n",
    "        \"output_cost_usd\": f\"${output_cost:.8f}\",\n",
    "        \"total_cost_usd\": f\"${total_cost:.8f}\"\n",
    "    }\n",
    "\n",
    "# Example cost calculation for Anthropic Claude 3.7 Sonnet\n",
    "example_cost = calculate_cost(\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\", 20000, 1500)\n",
    "print(\"Cost Analysis Example:\")\n",
    "print(json.dumps(example_cost, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latency-metrics-section",
   "metadata": {},
   "source": [
    "## 2. Latency Metrics\n",
    "\n",
    "Latency measures how long it takes to get a complete response. This is critical for user experience and each model differs in latency.\n",
    "\n",
    "The Bedrock converse API provides built in metrics in the invocation response, we will use this to fetch basic metrics such as latency and input/output token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "latency-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency(model_id: str, prompt: str, max_tokens: int = 100) -> Dict:\n",
    "    \"\"\"Measure end-to-end latency for a model invocation.\"\"\"\n",
    "    try:\n",
    "        \n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "            inferenceConfig={\"maxTokens\": max_tokens, \"temperature\": 0.1}\n",
    "        )\n",
    "        \n",
    "        latency_ms = response[\"metrics\"][\"latencyMs\"]\n",
    "        \n",
    "        cost_info = calculate_cost(\n",
    "            model_id, \n",
    "            response[\"usage\"][\"inputTokens\"], \n",
    "            response[\"usage\"][\"outputTokens\"]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"model_id\": model_id,\n",
    "            \"server_latency_ms\": latency_ms,\n",
    "            \"input_tokens\": response[\"usage\"][\"inputTokens\"],\n",
    "            \"output_tokens\": response[\"usage\"][\"outputTokens\"],\n",
    "            \"tokens_per_second\": round(response[\"usage\"][\"outputTokens\"] / (latency_ms / 1000), 1),\n",
    "            **cost_info,\n",
    "            \"error\": False\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": True, \"error_message\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3061a-83cb-4a17-a95e-47c297cb3637",
   "metadata": {},
   "source": [
    "## Testing latency and cost between Anthropic Claude 3.7 and Amazon Nova Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c71439bd-f22e-4166-80bf-246d8f16deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Latency Measurement...\n",
      "Custom metric published: {'ResponseMetadata': {'RequestId': '1296ded1-e1a3-4a9a-a49d-7f3c8e51ddea', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1296ded1-e1a3-4a9a-a49d-7f3c8e51ddea', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 04 Aug 2025 14:01:50 GMT'}, 'RetryAttempts': 0}}\n",
      "{\n",
      "  \"model_id\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
      "  \"server_latency_ms\": 3960,\n",
      "  \"input_tokens\": 15,\n",
      "  \"output_tokens\": 150,\n",
      "  \"tokens_per_second\": 37.9,\n",
      "  \"input_cost_usd\": \"$0.00004500\",\n",
      "  \"output_cost_usd\": \"$0.00225000\",\n",
      "  \"total_cost_usd\": \"$0.00229500\",\n",
      "  \"error\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test latency measurement with claude 3.7 \n",
    "print(\"Testing Latency Measurement...\")\n",
    "latency_result = measure_latency(\n",
    "    \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\", \n",
    "    \"Explain quantum computing in simple terms.\", \n",
    "    max_tokens=150\n",
    ")\n",
    "print(json.dumps(latency_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4452e4-3d23-42fa-93c9-4325dcaeac6c",
   "metadata": {},
   "source": [
    "## Nova Pro measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3cb0f48f-11ed-4803-b169-6d1efe868b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Latency Measurement...\n",
      "Custom metric published: {'ResponseMetadata': {'RequestId': '055b1ae4-7f0e-456e-a8fb-57b887167c99', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '055b1ae4-7f0e-456e-a8fb-57b887167c99', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 04 Aug 2025 14:01:54 GMT'}, 'RetryAttempts': 0}}\n",
      "{\n",
      "  \"model_id\": \"us.amazon.nova-pro-v1:0\",\n",
      "  \"server_latency_ms\": 1196,\n",
      "  \"input_tokens\": 7,\n",
      "  \"output_tokens\": 150,\n",
      "  \"tokens_per_second\": 125.4,\n",
      "  \"input_cost_usd\": \"$0.00000560\",\n",
      "  \"output_cost_usd\": \"$0.00003000\",\n",
      "  \"total_cost_usd\": \"$0.00003560\",\n",
      "  \"error\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test latency measurement with nova pro\n",
    "print(\"Testing Latency Measurement...\")\n",
    "latency_result = measure_latency(\n",
    "    \"us.amazon.nova-pro-v1:0\", \n",
    "    \"Explain quantum computing in simple terms.\", \n",
    "    max_tokens=150\n",
    ")\n",
    "print(json.dumps(latency_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20a89a-3280-49cb-9801-14c0dc17148c",
   "metadata": {},
   "source": [
    "## Analysis of results\n",
    "Notice that both models differ in latency, input token count and cost. This signifys the importance of choosing the right model for your use case as model choice can directly impact many aspects for your Gen AI application. \n",
    "Now lets dive deeper into some other metrics TTFS and TTLT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ttft-ttls-section",
   "metadata": {},
   "source": [
    "# 3. TTFT vs TTLT (Time to First Token vs Time to Last Token)\n",
    "\n",
    "## Time to First Token (TTFT)\n",
    "Measures how quickly a model begins generating its response after receiving a prompt. This metric is crucial for user experience as it affects perceived responsiveness.\n",
    "\n",
    "- **Lower TTFT**: Creates the impression of a more responsive system\n",
    "- **Impact factors**: Model size, architecture, hardware\n",
    "\n",
    "## Time to Last Token (TTLT)\n",
    "Measures the total time from prompt submission to complete response delivery. This metric is vital for throughput and overall system performance.\n",
    "\n",
    "- **Lower TTLT**: Enables processing more requests per unit time\n",
    "- **Faster responses**: Allow for more interactions within a given timeframe, which is beneficial for tasks requiring iterative engagement, like chain-of-thought prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "streaming-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "import json\n",
    "from typing import Dict\n",
    "from decimal import Decimal\n",
    "\n",
    "def put_custom_operational_cw_metrics(model_id: str, ttfs_ms, ttlt_ms,total_cost_usd):\n",
    "    \"\"\"Publish custom metrics to namespace- \"\"\"\n",
    "    \n",
    "    response = cloudwatch.put_metric_data(\n",
    "    Namespace='llm_custom_operational_metrics',  # A logical container for your metrics\n",
    "    MetricData=[\n",
    "        {\n",
    "            'MetricName': 'TimeToFirstToken',  # The name of your custom metric\n",
    "            'Value': ttfs_ms,                  # The value of the metric\n",
    "            'Unit': 'Milliseconds',             # The unit of measurement (e.g., Count, Seconds, Bytes)\n",
    "            'Dimensions': [              # Optional: Add dimensions for more granular analysis\n",
    "                {\n",
    "                    'Name': 'Model',\n",
    "                    'Value': model_id\n",
    "                }\n",
    "            ],\n",
    "            # 'Timestamp': datetime.utcnow(), # Optional: Specify a timestamp, defaults to current time\n",
    "            # 'StorageResolution': 1 # Optional: Set to 1 for high-resolution metrics (1-second granularity)\n",
    "        },\n",
    "        {\n",
    "            'MetricName': 'TimeToLastToken',\n",
    "            'Value': ttlt_ms,\n",
    "            'Unit': 'Milliseconds',\n",
    "            'Dimensions': [\n",
    "                {\n",
    "                    'Name': 'Model',\n",
    "                    'Value': model_id\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'MetricName': 'TotalCost',\n",
    "            'Value': Decimal(total_cost_usd.replace(\"$\", \"\")),\n",
    "            'Dimensions': [\n",
    "                {\n",
    "                    'Name': 'Model',\n",
    "                    'Value': model_id\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Custom metric published.\")\n",
    "\n",
    "\n",
    "def measure_streaming_metrics(model_id: str, prompt: str, max_tokens: int = 200) -> Dict:\n",
    "    \"\"\"Measure TTFS (TTFT) and TTLT (TTLS) using streaming responses with precise timing.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response_stream = bedrock_client.converse_stream(\n",
    "            modelId=model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "            inferenceConfig={\"maxTokens\": max_tokens, \"temperature\": 0.1}\n",
    "        )\n",
    "        \n",
    "        first_token_time = None\n",
    "        last_token_time = None  \n",
    "        token_timestamps = []\n",
    "        input_tokens = 0\n",
    "        output_tokens = 0\n",
    "        response_text = \"\"  # Capture the actual response\n",
    "        \n",
    "        for event in response_stream[\"stream\"]:\n",
    "            current_time = time.time()\n",
    "            \n",
    "            if 'contentBlockDelta' in event:\n",
    "                if first_token_time is None:\n",
    "                    first_token_time = current_time\n",
    "                \n",
    "                # Update last token time for each token received\n",
    "                last_token_time = current_time\n",
    "                token_timestamps.append(current_time)\n",
    "                \n",
    "                # Capture the response text\n",
    "                if 'delta' in event['contentBlockDelta'] and 'text' in event['contentBlockDelta']['delta']:\n",
    "                    response_text += event['contentBlockDelta']['delta']['text']\n",
    "                \n",
    "            elif 'metadata' in event:\n",
    "                usage = event['metadata'].get('usage', {})\n",
    "                input_tokens = usage.get('inputTokens', 0)\n",
    "                output_tokens = usage.get('outputTokens', 0)\n",
    "        \n",
    "        end_time = last_token_time if last_token_time else time.time()\n",
    "        \n",
    "        ttfs_ms = round((first_token_time - start_time) * 1000, 2) if first_token_time else None\n",
    "        ttlt_ms = round((end_time - start_time) * 1000, 2)\n",
    "        \n",
    "        inter_token_latencies = []\n",
    "        if len(token_timestamps) > 1:\n",
    "            for i in range(1, len(token_timestamps)):\n",
    "                inter_token_latencies.append(\n",
    "                    (token_timestamps[i] - token_timestamps[i-1]) * 1000\n",
    "                )\n",
    "        \n",
    "        cost_info = calculate_cost(model_id, input_tokens, output_tokens)\n",
    "        \n",
    "        put_custom_operational_cw_metrics(model_id,ttfs_ms,ttlt_ms,cost_info[\"total_cost_usd\"])\n",
    "        \n",
    "        return {\n",
    "            \"model_id\": model_id,\n",
    "            \"ttfs_ms\": ttfs_ms,  \n",
    "            \"ttlt_ms\": ttlt_ms,  \n",
    "            \"generation_time_ms\": ttlt_ms - ttfs_ms if ttfs_ms else None,\n",
    "            \"tokens_per_second\": round(output_tokens / (ttlt_ms / 1000), 1) if ttlt_ms > 0 else None,\n",
    "            \"avg_inter_token_latency_ms\": round(statistics.mean(inter_token_latencies), 2) if inter_token_latencies else None,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens_received\": len(token_timestamps),\n",
    "            \"response_text\": response_text,  # Include the actual response\n",
    "            **cost_info,\n",
    "            \"error\": False\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"error\": True, \"error_message\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cfd7064-90c1-4647-82d1-72f9f16bdaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Streaming Metrics (TTFS vs TTLT)...\n",
      "Custom metric published: {'ResponseMetadata': {'RequestId': '2fc5cdb2-1aea-4111-b508-e85e7aa24aba', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '2fc5cdb2-1aea-4111-b508-e85e7aa24aba', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 04 Aug 2025 14:02:08 GMT'}, 'RetryAttempts': 0}}\n",
      "Custom metric published.\n",
      "Model: us.amazon.nova-pro-v1:0\n",
      "TTFS (Time to First Token): 303.89ms\n",
      "TTLT (Time to Last Token): 3937.96ms\n",
      "Tokens/Second: 76.2\n",
      "Avg Inter-token Latency: 25.41ms\n",
      "Cost: $0.00006880\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test streaming metrics for Amazon nova pro\n",
    "print(\"Testing Streaming Metrics (TTFS vs TTLT)...\")\n",
    "streaming_result = measure_streaming_metrics(\n",
    "    \"us.amazon.nova-pro-v1:0\",\n",
    "    \"Write a short story about a robot learning to paint.\",\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "# Pretty print with focus on timing metrics\n",
    "if not streaming_result.get(\"error\"):\n",
    "    print(f\"Model: {streaming_result['model_id']}\")\n",
    "    print(f\"TTFS (Time to First Token): {streaming_result['ttfs_ms']}ms\")\n",
    "    print(f\"TTLT (Time to Last Token): {streaming_result['ttlt_ms']}ms\")\n",
    "    print(f\"Tokens/Second: {streaming_result['tokens_per_second']}\")\n",
    "    print(f\"Avg Inter-token Latency: {streaming_result['avg_inter_token_latency_ms']}ms\")\n",
    "    print(f\"Cost: {streaming_result['total_cost_usd']}\")\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(f\"Error: {streaming_result['error_message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d044104-85f6-4759-86ce-9e9d46ccb6ec",
   "metadata": {},
   "source": [
    "## Visualizing Operational metrics using CloudWatch Dashboard\n",
    "\n",
    "Amazon CloudWatch has automatic dashboards for customers to quickly gain insights into the health and performance of their AWS services. An automatic dashboard for Amazon Bedrock is available with [Amazon Bedrock runtime metrics](https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring.html#runtime-cloudwatch-metrics). To access Bedrock automatic dashboard from the AWS Management Console:\n",
    "\n",
    "Select Dashboards from the CloudWatch console, and select the Automatic Dashboards tab. You’ll see an option for an Amazon Bedrock dashboard in the list of available dashboards. \n",
    "\n",
    "You can create a [custom CloudWatch Dashboard](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/create_dashboard.html) and add the Bedrock Automatic Dashboard to it as shown below: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b18d7b",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/operational-metrics-cloudwatch-dashboard.png\" width=\"100%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8da830-b770-491b-8294-3cfdb35b8ebc",
   "metadata": {},
   "source": [
    "## 4. Email Summarization Use case\n",
    "In this example use case we will be using two models to summarize emails and extract key information, action items, and important details. This will demonstrates how LLM metrics impact real-world performance in a scenario requiring both accuracy and efficiency. We will compare how different models balance speed, cost, and quality when generating concise email summaries at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07025843-8a0f-4250-bcd9-c379808ec0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Email: 'Sampleemail2'\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing NOVA-LITE-V1:0...\n",
      "Custom metric published: {'ResponseMetadata': {'RequestId': 'c76e3152-2f26-4839-b6f7-b9d7eccb7557', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c76e3152-2f26-4839-b6f7-b9d7eccb7557', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 04 Aug 2025 14:02:27 GMT'}, 'RetryAttempts': 0}}\n",
      "Custom metric published.\n",
      "Success!\n",
      "   TTFS: 535.74ms\n",
      "   TTLT: 1523.34ms\n",
      "   Speed: 166.1 tokens/sec\n",
      "   Cost: $0.00002924\n",
      "   Tokens: 253 output\n",
      "Custom metric published.\n",
      "\n",
      "Testing NOVA-PRO-V1:0...\n",
      "Custom metric published: {'ResponseMetadata': {'RequestId': 'b0b87e12-6386-48db-970f-022be714acc6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b0b87e12-6386-48db-970f-022be714acc6', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 04 Aug 2025 14:02:30 GMT'}, 'RetryAttempts': 0}}\n",
      "Custom metric published.\n",
      "Success!\n",
      "   TTFS: 300.67ms\n",
      "   TTLT: 1976.31ms\n",
      "   Speed: 140.2 tokens/sec\n",
      "   Cost: $0.00039460\n",
      "   Tokens: 277 output\n",
      "Custom metric published.\n",
      "\n",
      "Testing CLAUDE-3-7-SONNET-20250219-V1:0...\n",
      "Custom metric published: {'ResponseMetadata': {'RequestId': '64962431-3853-4118-a7e4-526d2c8635ea', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '64962431-3853-4118-a7e4-526d2c8635ea', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 04 Aug 2025 14:02:35 GMT'}, 'RetryAttempts': 0}}\n",
      "Custom metric published.\n",
      "Success!\n",
      "   TTFS: 956.56ms\n",
      "   TTLT: 4474.0ms\n",
      "   Speed: 46.9 tokens/sec\n",
      "   Cost: $0.00453000\n",
      "   Tokens: 210 output\n",
      "Custom metric published.\n",
      "\n",
      "Testing Email: 'Sampleemail1'\n",
      "--------------------------------------------------\n",
      "\n",
      "Testing NOVA-LITE-V1:0...\n",
      "Custom metric published: {'ResponseMetadata': {'RequestId': '18e44182-6422-49eb-9914-bc30f031eca8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '18e44182-6422-49eb-9914-bc30f031eca8', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 04 Aug 2025 14:02:37 GMT', 'connection': 'close'}, 'RetryAttempts': 0}}\n",
      "Custom metric published.\n",
      "Success!\n",
      "   TTFS: 358.11ms\n",
      "   TTLT: 1373.65ms\n",
      "   Speed: 141.2 tokens/sec\n",
      "   Cost: $0.00002583\n",
      "   Tokens: 194 output\n",
      "Custom metric published.\n",
      "\n",
      "Testing NOVA-PRO-V1:0...\n",
      "Custom metric published: {'ResponseMetadata': {'RequestId': 'f54457c5-0ce8-473c-87c2-16dad906e0d2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f54457c5-0ce8-473c-87c2-16dad906e0d2', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 04 Aug 2025 14:02:40 GMT'}, 'RetryAttempts': 0}}\n",
      "Custom metric published.\n",
      "Success!\n",
      "   TTFS: 429.0ms\n",
      "   TTLT: 2058.43ms\n",
      "   Speed: 137.0 tokens/sec\n",
      "   Cost: $0.00036200\n",
      "   Tokens: 282 output\n",
      "Custom metric published.\n",
      "\n",
      "Testing CLAUDE-3-7-SONNET-20250219-V1:0...\n",
      "Custom metric published: {'ResponseMetadata': {'RequestId': '7ad1d002-208c-40c3-8919-4a8036734b4a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '7ad1d002-208c-40c3-8919-4a8036734b4a', 'content-type': 'text/xml', 'content-length': '212', 'date': 'Mon, 04 Aug 2025 14:02:48 GMT'}, 'RetryAttempts': 0}}\n",
      "Custom metric published.\n",
      "Success!\n",
      "   TTFS: 1356.73ms\n",
      "   TTLT: 6678.91ms\n",
      "   Speed: 35.3 tokens/sec\n",
      "   Cost: $0.00480300\n",
      "   Tokens: 236 output\n",
      "Custom metric published.\n"
     ]
    }
   ],
   "source": [
    "# Email Summarization Use Case - Model Performance Comparison\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def load_emails_from_folder(folder_path=\"data/emails\"):\n",
    "    \"\"\"Load emails to summarize.\"\"\"\n",
    "    sample_emails = []\n",
    "    \n",
    "    email_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    \n",
    "    for i, file_path in enumerate(email_files, 1):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read().strip()\n",
    "                \n",
    "                # Extract subject from filename or first line\n",
    "                filename = Path(file_path).stem\n",
    "                \n",
    "                # If the file starts with \"Subject:\", extract it\n",
    "                if content.startswith(\"Subject:\"):\n",
    "                    lines = content.split('\\n')\n",
    "                    subject = lines[0].replace(\"Subject:\", \"\").strip()\n",
    "                    email_content = '\\n'.join(lines[1:]).strip()\n",
    "                else:\n",
    "                    # Use filename as subject if no subject line found\n",
    "                    subject = filename.replace(\"_\", \" \").title()\n",
    "                    email_content = content\n",
    "                \n",
    "                sample_emails.append({\n",
    "                    \"id\": i,\n",
    "                    \"subject\": subject,\n",
    "                    \"content\": email_content\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return sample_emails\n",
    "\n",
    "sample_emails = load_emails_from_folder()\n",
    "\n",
    "summarization_prompt = \"\"\"\n",
    "You are an AI assistant that summarizes business emails for busy executives. \n",
    "\n",
    "Please analyze the following email and provide a concise summary that includes:\n",
    "\n",
    "1. **Key Points**: Main topics and important information\n",
    "2. **Action Items**: Specific tasks or decisions required\n",
    "3. **Deadlines**: Any time-sensitive items\n",
    "4. **People/Teams Involved**: Who needs to take action\n",
    "5. **Impact**: Business impact or urgency level\n",
    "\n",
    "Email to summarize:\n",
    "{email_content}\n",
    "\n",
    "Provide a clear, structured summary in 3-4 sentences followed by bullet points for action items.\n",
    "\"\"\"\n",
    "\n",
    "# Models to compare\n",
    "models_to_test = [\n",
    "    \"us.amazon.nova-lite-v1:0\",\n",
    "    \"us.amazon.nova-pro-v1:0\", \n",
    "    \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "]\n",
    "\n",
    "def run_email_summarization_comparison(email_data, models):\n",
    "    \"\"\"Compare models on email summarization task with performance metrics.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nTesting Email: '{email_data['subject']}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_id in models:\n",
    "        print(f\"\\nTesting {model_id.split('.')[-1].upper()}...\")\n",
    "        \n",
    "        prompt = summarization_prompt.format(\n",
    "            email_content=f\"Subject: {email_data['subject']}\\n\\n{email_data['content']}\"\n",
    "        )\n",
    "        \n",
    "        # Measure performance using our streaming metrics function\n",
    "        result = measure_streaming_metrics(\n",
    "            model_id=model_id,\n",
    "            prompt=prompt,\n",
    "            max_tokens=400  \n",
    "        )\n",
    "        \n",
    "        if not result.get(\"error\"):\n",
    "            print(f\"Success!\")\n",
    "            print(f\"   TTFS: {result['ttfs_ms']}ms\")\n",
    "            print(f\"   TTLT: {result['ttlt_ms']}ms\") \n",
    "            print(f\"   Speed: {result['tokens_per_second']} tokens/sec\")\n",
    "            print(f\"   Cost: {result['total_cost_usd']}\")\n",
    "            print(f\"   Tokens: {result['output_tokens']} output\")\n",
    "            \n",
    "            # Add email context and additional data for advanced metrics\n",
    "            result['email_id'] = email_data['id']\n",
    "            result['email_subject'] = email_data['subject']\n",
    "            result['email_content'] = email_data['content']\n",
    "            result['model_name'] = model_id.split('.')[-1]\n",
    "            result['prompt_used'] = prompt\n",
    "\n",
    "            put_custom_operational_cw_metrics(model_id,result['ttfs_ms'],result['ttlt_ms'],result['total_cost_usd'])\n",
    "            \n",
    "        else:\n",
    "            print(f\"Error: {result['error_message']}\")\n",
    "            \n",
    "        results.append(result)\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return results\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for email in sample_emails:\n",
    "    email_results = run_email_summarization_comparison(email, models_to_test)\n",
    "    all_results.extend(email_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805927f-0731-49c2-a53e-67d8cbb05bb0",
   "metadata": {},
   "source": [
    "## 4.1 Visualizing custom operation metrics\n",
    "`put_custom_operational_cw_metrics` function [publishes the following custom metrics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html): \"Time To First Token\", \"Time To Last Token\" and \"Total Cost\" to custom [CloudWatch namespace](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Namespace) `llm_custom_operational_metrics` for each model. This enables you to visualize these custom metrics in your same observability dashboard as shown below:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c7ec1",
   "metadata": {},
   "source": [
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/Custom-operational-metrics-cloudwatch-dashboard.png\" width=\"100%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b88333",
   "metadata": {},
   "source": [
    "Similarly, you can create any additional custom metric you need for your application logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-responses-for-advanced-metrics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing agent responses for advanced metrics evaluation...\n",
      " Prepared response from nova-lite-v1:0 for email 1\n",
      " Prepared response from nova-pro-v1:0 for email 1\n",
      " Skipping result due to error or missing response text\n",
      " Prepared response from nova-lite-v1:0 for email 2\n",
      " Prepared response from nova-pro-v1:0 for email 2\n",
      " Skipping result due to error or missing response text\n"
     ]
    }
   ],
   "source": [
    "# Save responses for quality metrics evaluation (using data already collected)\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Format the results for quality metrics evaluation\n",
    "enhanced_results = []\n",
    "print(\"Preparing agent responses for advanced metrics evaluation...\")\n",
    "\n",
    "for result in all_results:\n",
    "    if not result.get(\"error\") and result.get(\"response_text\"):\n",
    "        enhanced_result = {\n",
    "            \"email_id\": result['email_id'],\n",
    "            \"email_subject\": result['email_subject'],\n",
    "            \"email_content\": result['email_content'],\n",
    "            \"model_id\": result['model_id'],\n",
    "            \"model_name\": result['model_name'],\n",
    "            \"agent_response\": result['response_text'],  # Use captured response\n",
    "            \"prompt_used\": result['prompt_used'],\n",
    "            \"input_tokens\": result['input_tokens'],\n",
    "            \"output_tokens\": result['output_tokens'],\n",
    "            # Include performance metrics for reference\n",
    "            \"performance_metrics\": {\n",
    "                \"ttfs_ms\": result.get('ttfs_ms'),\n",
    "                \"ttlt_ms\": result.get('ttlt_ms'),\n",
    "                \"tokens_per_second\": result.get('tokens_per_second'),\n",
    "                \"total_cost_usd\": result.get('total_cost_usd')\n",
    "            }\n",
    "        }\n",
    "        enhanced_results.append(enhanced_result)\n",
    "        print(f\" Prepared response from {result['model_name']} for email {result['email_id']}\")\n",
    "    else:\n",
    "        print(f\" Skipping result due to error or missing response text\")\n",
    "\n",
    "# Save the enhanced results\n",
    "with open(\"email_responses.json\", 'w') as f:\n",
    "    json.dump(enhanced_results, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d340c-b400-4804-a90c-e4b5ce63910f",
   "metadata": {},
   "source": [
    "## Analyzing Results\n",
    "As you compare the result you will notice that while given the same task the models had varying speed (TTFT/TTLT) as well as differences in output token count and cost. However in this sample we did not inspect the LLM output to identify which model gave the best responses for our use case. Which we will be doing in the next module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices-section",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Key Takeaways\n",
    "- Monitor token usage closely\n",
    "- Choose appropriate models for your use case\n",
    "- Consider input/output token ratios\n",
    "- Use streaming for better user experience (lower TTFT)\n",
    "- Use the correct invocation type based on use case (Batch VS On Demand) \n",
    "- Monitor and set appropriate timeouts\n",
    "- Implement comprehensive error handling\n",
    "- Set up monitoring and alerting to keep track of cost\n",
    "- Plan for rate limits and scaling\n",
    "- Consider caching strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-section",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've learned to measure and analyze key LLM metrics:\n",
    "\n",
    "**Cost Metrics** - Track token usage and optimize spending  \n",
    "**Latency Metrics** - Measure end-to-end response times  \n",
    "**TTFT vs TTLT** - Understand streaming performance  \n",
    "\n",
    "### Next Steps\n",
    "- Evaluate models based on quality metrics\n",
    "- Create use case specific metrics\n",
    "- Evaluate metrics based on your workload\n",
    "- Utilize Opensource frameworks to evaluate your models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f32cf3-7b57-4916-a848-455534f91204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
