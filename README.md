# Generative AI Evaluations Workshop

## Overview

This workshop teaches systematic approaches to evaluating Generative AI workloads for production use. You'll learn to build evaluation frameworks that go beyond basic metrics to ensure reliable model performance while optimizing cost and performance.

## What You'll Learn

### Core Modules
- 01 Operational Metrics: evaluate how your workload is running in terms of cost and performance.
- 02 Quality Metrics: evaluate and tune the quality of your results.
- 03 Agentic Metrics: evaluate your agents and use agents for evaluation.

### Bonus Modules
- 04 Workload Specific Metrics
  - Intelligent Document Processing
  - Guardrails
  - Multi-modal RAG
  - Automated Reasoning
- 05 Framework and Tool Specific Implementations
  - PromptFoo
  - BrainTrust
  - Bedrock Evaluations
  - AgentCore

## Prerequisites

- AWS account with Amazon Bedrock enabled
- Basic Python and ML familiarity
- No security expertise required

## Getting Started

1. Clone the repository
2. Configure AWS credentials
3. Enable required Bedrock models
4. Install dependencies

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the LICENSE file.

