{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b802ff",
   "metadata": {},
   "source": [
    "# Introduction to Stickler: Single Document Evaluation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook introduces the Stickler library for evaluating structured data extraction from any workflow that produces key-value pairs or structured outputs. While we use document processing as our primary example, Stickler's applications extend far beyond this use case. The library can validate structured data extraction from any source - whether you're scraping product information from websites, validating API responses, processing database records, or analyzing log files. Any scenario where you need to verify that extracted information matches expected patterns and requirements can benefit from Stickler's validation framework\n",
    "\n",
    "## Why Use Stickler?\n",
    "\n",
    "- **Standardized Metrics**: Industry-standard evaluation methods for structured data extraction\n",
    "- **Structured Data Focus**: Purpose-built for evaluating key-value extraction accuracy\n",
    "- **Field-Level Analysis**: Evaluate performance across different field types\n",
    "- **Clear Reporting**: Generate readable evaluation reports\n",
    "- **Open Source**: Community-driven library with transparent implementations\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Stickler Setup** - Install and configure the library\n",
    "2. **Data Structure Definition** - Define comparison rules for your fields\n",
    "3. **Single Document Evaluation** - Run evaluation on one document\n",
    "4. **Result Interpretation** - Understand Stickler's output reports\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.12+\n",
    "- Basic understanding of structured data concepts\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b2f87-99e6-41a4-8caa-97921fd94ba8",
   "metadata": {},
   "source": [
    "## Setup and dependencies\n",
    "Stickler is an open-source library ([Github](https://github.com/awslabs/stickler), [Piwheels](https://www.piwheels.org/project/stickler-eval/))for structured data evaluation. Install it using pip:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbf9d554-b827-4461-8943-7a5e1fd72ee0",
   "metadata": {},
   "source": [
    "!pip install stickler-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304cff6-c517-4405-bb25-df0205b889a6",
   "metadata": {},
   "source": [
    "**Import Required Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda283dd-22de-4cc0-ac5a-76fb18e241f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T20:30:30.860886Z",
     "iopub.status.busy": "2025-10-27T20:30:30.859442Z",
     "iopub.status.idle": "2025-10-27T20:30:30.873532Z",
     "shell.execute_reply": "2025-10-27T20:30:30.871884Z",
     "shell.execute_reply.started": "2025-10-27T20:30:30.860850Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from stickler import StructuredModel, ComparableField\n",
    "from stickler.comparators import ExactComparator, NumericComparator, LevenshteinComparator\n",
    "from stickler.structured_object_evaluator.evaluator import StructuredModelEvaluator\n",
    "from stickler.structured_object_evaluator.utils.pretty_print import print_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b8d99",
   "metadata": {},
   "source": [
    "## 1. Data sample\n",
    "\n",
    "Let's consider a single invoice document with three essential fields to extract: the invoice number, vendor name, and total amount. The following example shows both the ground truth (what we expect) and the prediction (what our system extracted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "728033a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T20:30:33.653332Z",
     "iopub.status.busy": "2025-10-27T20:30:33.653036Z",
     "iopub.status.idle": "2025-10-27T20:30:33.657708Z",
     "shell.execute_reply": "2025-10-27T20:30:33.656899Z",
     "shell.execute_reply.started": "2025-10-27T20:30:33.653309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ground truth data (what we expect)\n",
    "ground_truth_data = {\n",
    "    'invoice_number': 'INV-2023-001',\n",
    "    'total': 1500.00,\n",
    "    'vendor': 'ABC Corporation'\n",
    "}\n",
    "\n",
    "# Predicted data (what our system extracted)\n",
    "prediction_data = {\n",
    "    'invoice_number': 'INV-2023-001',  # Perfect match\n",
    "    'total': 1499.95,                  # Very close - should match\n",
    "    'vendor': 'ABC Corp'               # Abbreviated - should still match\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1bb70a",
   "metadata": {},
   "source": [
    "This example represents common real-world scenarios in document extraction where we encounter exact matches for standardized fields like invoice numbers, text variations for company names, and small numerical differences in monetary values. We will use this data to demonstrate how the evaluation framework processes these different types of matches and calculates the corresponding accuracy metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b356d491",
   "metadata": {},
   "source": [
    "## 2. Define your data structure\n",
    "\n",
    "Before we can evaluate our extracted data, we need to define how each field should be compared and weighted. We create a structured model that specifies the comparison rules for each field in our invoice document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8764ff70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T20:30:35.786203Z",
     "iopub.status.busy": "2025-10-27T20:30:35.785423Z",
     "iopub.status.idle": "2025-10-27T20:30:35.792236Z",
     "shell.execute_reply": "2025-10-27T20:30:35.791273Z",
     "shell.execute_reply.started": "2025-10-27T20:30:35.786174Z"
    }
   },
   "outputs": [],
   "source": [
    "class Invoice(StructuredModel):\n",
    "    \"\"\"Simple invoice model for demonstration.\"\"\"\n",
    "    \n",
    "    invoice_number: str = ComparableField(\n",
    "        comparator=LevenshteinComparator(),\n",
    "        threshold=0.9,\n",
    "        weight=2.0  # This field is important\n",
    "    )\n",
    "    \n",
    "    total: float = ComparableField(\n",
    "        threshold=0.8,  # More forgiving threshold for demo\n",
    "        weight=2.0  # This field is important\n",
    "    )\n",
    "\n",
    "    vendor: str = ComparableField(\n",
    "        comparator=LevenshteinComparator(),\n",
    "        threshold=0.5,  # Much more forgiving for demo purposes\n",
    "        weight=1.0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9dbc0",
   "metadata": {},
   "source": [
    "In this structure, we define three fields with different comparison rules. The invoice number and total amount are given higher weights (2.0) to reflect their importance in the evaluation. We use the Levenshtein comparator for text fields, with a strict threshold (0.9) for invoice numbers but a more lenient one (0.5) for vendor names to accommodate common variations in company names. The total amount uses a threshold of 0.8 to allow for minor numerical differences while still ensuring accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b94e96",
   "metadata": {},
   "source": [
    "## 3. Initialize Data Structure\n",
    "\n",
    "Now we convert our raw data into structured objects that follow our defined Invoice model. We use the from_json method to create Invoice instances from both our prediction and ground truth data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18217efe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T20:30:37.446238Z",
     "iopub.status.busy": "2025-10-27T20:30:37.445946Z",
     "iopub.status.idle": "2025-10-27T20:30:37.453216Z",
     "shell.execute_reply": "2025-10-27T20:30:37.452485Z",
     "shell.execute_reply.started": "2025-10-27T20:30:37.446215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth values:\n",
      "{\n",
      "    \"invoice_number\": \"INV-2023-001\",\n",
      "    \"total\": 1500.0,\n",
      "    \"vendor\": \"ABC Corporation\"\n",
      "}\n",
      "Predicted values:\n",
      "{\n",
      "    \"invoice_number\": \"INV-2023-001\",\n",
      "    \"total\": 1499.95,\n",
      "    \"vendor\": \"ABC Corp\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ground_truth = Invoice.from_json(ground_truth_data)\n",
    "prediction = Invoice.from_json(prediction_data)\n",
    "\n",
    "print(\"Ground truth values:\")\n",
    "print(json.dumps(ground_truth_data,indent=4))\n",
    "\n",
    "print(\"Predicted values:\")\n",
    "print(json.dumps(prediction_data,indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbb001",
   "metadata": {},
   "source": [
    "The printed output shows our data has been successfully structured according to our Invoice model. We can see the exact values for each field, and the empty extra_fields dictionary indicates that all our data fits within our defined structure. These structured objects will allow us to perform our evaluation using the comparison rules we defined in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862b98c",
   "metadata": {},
   "source": [
    "## 4. Structured model evaluation\n",
    "\n",
    "We use the StructuredModelEvaluator to compare our prediction against the ground truth and visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f190dcaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T20:30:39.743702Z",
     "iopub.status.busy": "2025-10-27T20:30:39.742911Z",
     "iopub.status.idle": "2025-10-27T20:30:39.753907Z",
     "shell.execute_reply": "2025-10-27T20:30:39.753156Z",
     "shell.execute_reply.started": "2025-10-27T20:30:39.743657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFUSION MATRIX SUMMARY ===\n",
      "\n",
      "--- Raw Counts ---\n",
      "Metric             Count\n",
      "-------------------------\n",
      "True Positive          2\n",
      "False Positive         1\n",
      "True Negative          0\n",
      "False Negative         0\n",
      "False Discovery        1\n",
      "\n",
      "--- Derived Metrics ---\n",
      "Metric               Value Visual                \n",
      "--------------------------------------------------\n",
      "Precision           66.67% █████████████░░░░░░░  \n",
      "Recall             100.00% ████████████████████  \n",
      "F1 Score            80.00% ████████████████░░░░  \n",
      "Accuracy            66.67% █████████████░░░░░░░  \n",
      "\n",
      "\n",
      "=== FIELD-LEVEL METRICS ===\n",
      "\n",
      "Field                    TP     FP     TN     FN     FD     Prec   Recall       F1      Acc Visual                \n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "invoice_number            1      0      0      0      0  100.00%  100.00%  100.00%  100.00% ████████████████████  \n",
      "\n",
      "total                     0      1      0      0      1    0.00%    0.00%    0.00%    0.00% ░░░░░░░░░░░░░░░░░░░░  \n",
      "\n",
      "vendor                    1      0      0      0      0  100.00%  100.00%  100.00%  100.00% ████████████████████  \n",
      "\n",
      "\n",
      "\n",
      "=== CONFUSION MATRIX VISUALIZATION ===\n",
      "\n",
      "TTTTTTTTTTTTTTTTTTTTFFFFFFFFFFDDDDDDDDDD\n",
      "\n",
      "Legend:\n",
      "  T True Positive (TP): 2 (50.00%)\n",
      "  N True Negative (TN): 0 (0.00%)\n",
      "  F False Positive (FP): 1 (25.00%)\n",
      "  M False Negative (FN): 0 (0.00%)\n",
      "  D False Discovery (FD): 1 (25.00%)\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "evaluator = StructuredModelEvaluator()\n",
    "result = evaluator.evaluate(ground_truth, prediction)\n",
    "print(print_confusion_matrix(result, show_details=True))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54e384",
   "metadata": {},
   "source": [
    "The output provides a comprehensive view of the evaluation results in three sections:\n",
    "\n",
    "1. **Confusion Matrix Summary** shows the overall performance:\n",
    "   - Raw counts (True Positives, False Positives, False Discovery)\n",
    "   - Derived metrics (Precision, Recall, F1-score, Accuracy)\n",
    "   - Visual representation of metrics using progress bars\n",
    "\n",
    "2. **Field-Level Metrics** breaks down performance by field:\n",
    "   - Individual metrics for each field (invoice_number, vendor, total)\n",
    "   - Shows Precision, Recall, F1-score, and Accuracy per field\n",
    "   - Visual indicators of performance levels\n",
    "\n",
    "3. **Confusion Matrix Visualization** provides a visual representation of matches and mismatches:\n",
    "   - Shows distribution of True Positives (T), False Positives (F), and False Discoveries (D)\n",
    "   - Includes percentage breakdown of each category\n",
    "   - Helps quickly identify patterns in matching performance\n",
    "\n",
    "This detailed breakdown helps understand how well our structured data evaluation is performing both overall and at the field level, making it easier to identify areas for potential improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fc13a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The structured data evaluation framework provides a systematic way to assess extraction accuracy through multiple levels of analysis. By calculating precision, recall, and F1-scores at both the overall document level and individual field level, we can thoroughly understand the performance of our extraction system.\n",
    "\n",
    "The detailed evaluation output, including confusion matrices and visual representations, helps identify patterns in matching performance and highlights areas that may need attention. This comprehensive approach to evaluation is essential for real-world applications where understanding the accuracy and reliability of extracted data is crucial.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you've learned the basics of Stickler for single document evaluation, here are recommended next steps:\n",
    "\n",
    "#### 1. **Experiment with Different Configurations**\n",
    "- Adjust threshold values to see how they affect matching performance\n",
    "- Try different comparators (ExactComparator, NumericComparator) for various field types\n",
    "- Modify field weights to reflect your specific use case priorities\n",
    "\n",
    "#### 2. **Scale to Multiple Documents**\n",
    "- Explore the companion notebook: `04-01-Stickler-Bulk-Evaluation.ipynb`\n",
    "- Learn batch evaluation techniques for larger datasets\n",
    "- Understand aggregate performance metrics across document collections\n",
    "\n",
    "#### 3. **Apply to Your Own Data**\n",
    "- Define structured models for your specific document types or data sources\n",
    "- Customize comparison rules based on your field characteristics\n",
    "- Integrate Stickler evaluation into your existing data processing workflows\n",
    "\n",
    "#### 5. **Community and Resources**\n",
    "- Visit the [Stickler GitHub repository](https://github.com/awslabs/stickler) for documentation and examples\n",
    "- Contribute to the open-source project with feedback or improvements\n",
    "- Share your use cases and evaluation results with the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a67ed6-bf55-4848-a21a-8a6055c9e24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
