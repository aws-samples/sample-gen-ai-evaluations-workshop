{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5aa63f-eeea-4e4a-9c62-a46ce5185d80",
   "metadata": {},
   "source": [
    "# Understanding Structured Data Evaluation Metrics\n",
    "\n",
    "## Overview\n",
    "In this workshop, you'll learn to measure and analyze key evaluation metrics for structured data extraction models. Understanding these metrics is crucial for:\n",
    "- Assessing model accuracy at field and document levels\n",
    "- Identifying problematic fields and patterns\n",
    "- Optimizing extraction performance\n",
    "- Quality assurance of automated data extraction\n",
    "\n",
    "## Key Metrics We'll Cover\n",
    "1. Peformance metrics\n",
    "- Precision: Accuracy of extracted fields\n",
    "- Recall: Completeness of extraction\n",
    "- F1-Score: Balanced measure of precision and recall\n",
    "\n",
    "2. Evaluation levels\n",
    "- Overall Document Level: Aggregate metrics across all fields\n",
    "- Field-Level Analysis: Individual performance for each field type\n",
    "\n",
    "## Use case\n",
    "We'll evaluate an invoice processing system that extracts key information from business documents. This demonstrates real-world structured data evaluation in a scenario requiring high accuracy across multiple fields. \n",
    "\n",
    "## Prerequisites\n",
    "- AWS account with Bedrock access\n",
    "- Python 3.10+\n",
    "- boto3 library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c6168-f05c-450c-ab76-83a7f644c115",
   "metadata": {},
   "source": [
    "## Setup and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cc412d-ed85-42b2-93f3-19b56f17efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genaidp_lib.key_information_evaluation.structured_object_evaluator.models.structured_model import StructuredModel\n",
    "from genaidp_lib.key_information_evaluation.structured_object_evaluator.models.comparable_field import ComparableField\n",
    "from genaidp_lib.key_information_evaluation.common.comparators.levenshtein import LevenshteinComparator\n",
    "from genaidp_lib.key_information_evaluation.structured_object_evaluator.evaluator import StructuredModelEvaluator\n",
    "from genaidp_lib.key_information_evaluation.structured_object_evaluator.utils.pretty_print import print_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8dcc2-38d2-4c92-95d3-b5111f9db168",
   "metadata": {},
   "source": [
    "## 1. Data sample\n",
    "\n",
    "Let's consider a single invoice document with three essential fields to extract: the invoice number, vendor name, and total amount. The following example shows both the ground truth (what we expect) and the prediction (what our system extracted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfdb87c-3c26-4a77-a3df-b469ba0a0f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth data (what we expect)\n",
    "ground_truth_data = {\n",
    "    'invoice_number': 'INV-2023-001',\n",
    "    'vendor': 'ABC Corporation',\n",
    "    'total': 1500.00\n",
    "}\n",
    "\n",
    "# Predicted data (what our system extracted)\n",
    "prediction_data = {\n",
    "    'invoice_number': 'INV-2023-001',  # Perfect match\n",
    "    'vendor': 'ABC Corp',              # Abbreviated - should still match\n",
    "    'total': 1499.95                   # Very close - should match\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3db12c-7f13-4e88-a05c-bbcab9e87a7e",
   "metadata": {},
   "source": [
    "This example represents common real-world scenarios in document extraction where we encounter exact matches for standardized fields like invoice numbers, text variations for company names, and small numerical differences in monetary values. We will use this data to demonstrate how the evaluation framework processes these different types of matches and calculates the corresponding accuracy metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edf1c0f-0d56-4800-8a91-40640dcb5a81",
   "metadata": {},
   "source": [
    "## 2. Define your data structure\n",
    "\n",
    "Before we can evaluate our extracted data, we need to define how each field should be compared and weighted. We create a structured model that specifies the comparison rules for each field in our invoice document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b930bfc-636a-4b9f-ad61-1b7ffa63c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invoice(StructuredModel):\n",
    "    \"\"\"Simple invoice model for demonstration.\"\"\"\n",
    "    \n",
    "    invoice_number: str = ComparableField(\n",
    "        comparator=LevenshteinComparator(),\n",
    "        threshold=0.9,\n",
    "        weight=2.0  # This field is important\n",
    "    )\n",
    "    \n",
    "    vendor: str = ComparableField(\n",
    "        comparator=LevenshteinComparator(),\n",
    "        threshold=0.5,  # Much more forgiving for demo purposes\n",
    "        weight=1.0\n",
    "    )\n",
    "    \n",
    "    total: float = ComparableField(\n",
    "        threshold=0.8,  # More forgiving threshold for demo\n",
    "        weight=2.0  # This field is important\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091ce48-34cb-4fbb-89f9-b2ea183bec7f",
   "metadata": {},
   "source": [
    "In this structure, we define three fields with different comparison rules. The invoice number and total amount are given higher weights (2.0) to reflect their importance in the evaluation. We use the Levenshtein comparator for text fields, with a strict threshold (0.9) for invoice numbers but a more lenient one (0.5) for vendor names to accommodate common variations in company names. The total amount uses a threshold of 0.8 to allow for minor numerical differences while still ensuring accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91775d4e-2795-485e-a9cf-bde790a7195e",
   "metadata": {},
   "source": [
    "## 3. Initialize Data Structure\n",
    "\n",
    "Now we convert our raw data into structured objects that follow our defined Invoice model. We use the from_json method to create Invoice instances from both our prediction and ground truth data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddd6245-22ca-46c8-abb2-cda1ac554476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "extra_fields: {}\n",
      "invoice_number: INV-2023-001\n",
      "vendor: ABC Corp\n",
      "total: 1499.95\n",
      "\n",
      "Ground truth values:\n",
      "extra_fields: {}\n",
      "invoice_number: INV-2023-001\n",
      "vendor: ABC Corporation\n",
      "total: 1500.0\n"
     ]
    }
   ],
   "source": [
    "prediction = Invoice.from_json(prediction_data)\n",
    "ground_truth = Invoice.from_json(ground_truth_data)\n",
    "\n",
    "print(\"Predicted values:\")\n",
    "for field_name in prediction.__fields__:\n",
    "    print(f\"{field_name}: {getattr(prediction, field_name)}\")\n",
    "\n",
    "print(\"\\nGround truth values:\")\n",
    "for field_name in ground_truth.__fields__:\n",
    "    print(f\"{field_name}: {getattr(ground_truth, field_name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7571d-991e-41ae-a834-98615639eec6",
   "metadata": {},
   "source": [
    "The printed output shows our data has been successfully structured according to our Invoice model. We can see the exact values for each field, and the empty extra_fields dictionary indicates that all our data fits within our defined structure. These structured objects will allow us to perform our evaluation using the comparison rules we defined in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1afee19-aea9-4247-8fc7-a6f8538a60c9",
   "metadata": {},
   "source": [
    "## 4. Structured model evaluation\n",
    "\n",
    "We use the StructuredModelEvaluator to compare our prediction against the ground truth and visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39565eb-e3da-4e2a-8911-2799d95d53e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFUSION MATRIX SUMMARY ===\n",
      "\n",
      "--- Raw Counts ---\n",
      "Metric             Count\n",
      "-------------------------\n",
      "True Positive          2\n",
      "False Positive         1\n",
      "True Negative          0\n",
      "False Negative         0\n",
      "False Discovery        1\n",
      "\n",
      "--- Derived Metrics ---\n",
      "Metric               Value Visual                \n",
      "--------------------------------------------------\n",
      "Precision           66.67% █████████████░░░░░░░  \n",
      "Recall             100.00% ████████████████████  \n",
      "F1 Score            80.00% ████████████████░░░░  \n",
      "Accuracy            66.67% █████████████░░░░░░░  \n",
      "\n",
      "\n",
      "=== FIELD-LEVEL METRICS ===\n",
      "\n",
      "Field                    TP     FP     TN     FN     FD     Prec   Recall       F1      Acc Visual                \n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "invoice_number            1      0      0      0      0  100.00%  100.00%  100.00%  100.00% ████████████████████  \n",
      "\n",
      "total                     0      1      0      0      1    0.00%    0.00%    0.00%    0.00% ░░░░░░░░░░░░░░░░░░░░  \n",
      "\n",
      "vendor                    1      0      0      0      0  100.00%  100.00%  100.00%  100.00% ████████████████████  \n",
      "\n",
      "\n",
      "\n",
      "=== CONFUSION MATRIX VISUALIZATION ===\n",
      "\n",
      "TTTTTTTTTTTTTTTTTTTTFFFFFFFFFFDDDDDDDDDD\n",
      "\n",
      "Legend:\n",
      "  T True Positive (TP): 2 (50.00%)\n",
      "  N True Negative (TN): 0 (0.00%)\n",
      "  F False Positive (FP): 1 (25.00%)\n",
      "  M False Negative (FN): 0 (0.00%)\n",
      "  D False Discovery (FD): 1 (25.00%)\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "evaluator = StructuredModelEvaluator()\n",
    "result = evaluator.evaluate(ground_truth, prediction)\n",
    "print(print_confusion_matrix(result, show_details=True))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003494d-bcd0-4310-92c9-878f393535d2",
   "metadata": {},
   "source": [
    "The output provides a comprehensive view of the evaluation results in three sections:\n",
    "\n",
    "1. **Confusion Matrix Summary** shows the overall performance:\n",
    "   - Raw counts (True Positives, False Positives, False Discovery)\n",
    "   - Derived metrics (Precision, Recall, F1-score, Accuracy)\n",
    "   - Visual representation of metrics using progress bars\n",
    "\n",
    "2. **Field-Level Metrics** breaks down performance by field:\n",
    "   - Individual metrics for each field (invoice_number, vendor, total)\n",
    "   - Shows Precision, Recall, F1-score, and Accuracy per field\n",
    "   - Visual indicators of performance levels\n",
    "\n",
    "3. **Confusion Matrix Visualization** provides a visual representation of matches and mismatches:\n",
    "   - Shows distribution of True Positives (T), False Positives (F), and False Discoveries (D)\n",
    "   - Includes percentage breakdown of each category\n",
    "   - Helps quickly identify patterns in matching performance\n",
    "\n",
    "This detailed breakdown helps understand how well our structured data evaluation is performing both overall and at the field level, making it easier to identify areas for potential improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524e238-fae4-43c1-94a3-f47c271ef58b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The structured data evaluation framework provides a systematic way to assess extraction accuracy through multiple levels of analysis. By calculating precision, recall, and F1-scores at both the overall document level and individual field level, we can thoroughly understand the performance of our extraction system.\n",
    "\n",
    "The detailed evaluation output, including confusion matrices and visual representations, helps identify patterns in matching performance and highlights areas that may need attention. This comprehensive approach to evaluation is essential for real-world applications where understanding the accuracy and reliability of extracted data is crucial.\n",
    "\n",
    "### Next steps\n",
    "For further exploration, the framework can be adapted to evaluate different types of structured data and various matching requirements, making it a versatile tool for assessing data extraction performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50c548-4b11-442a-8ecf-71ab49651189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
