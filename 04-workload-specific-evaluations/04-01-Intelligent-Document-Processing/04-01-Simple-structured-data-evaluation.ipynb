{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16a530a",
   "metadata": {},
   "source": [
    "# Understanding Structured Data Evaluation Metrics\n",
    "\n",
    "## Overview\n",
    "In this workshop, you'll learn to measure and analyze key evaluation metrics for structured data extraction models. Understanding these metrics is crucial for:\n",
    "- Assessing model accuracy at field and document levels\n",
    "- Identifying problematic fields and patterns\n",
    "- Optimizing extraction performance\n",
    "- Quality assurance of automated data extraction\n",
    "\n",
    "## Key Metrics We'll Cover\n",
    "1. Peformance metrics\n",
    "- Precision: Accuracy of extracted fields\n",
    "- Recall: Completeness of extraction\n",
    "- F1-Score: Balanced measure of precision and recall\n",
    "\n",
    "2. Evaluation levels\n",
    "- Overall Document Level: Aggregate metrics across all fields\n",
    "- Field-Level Analysis: Individual performance for each field type\n",
    "\n",
    "## Use case\n",
    "We'll evaluate an invoice processing system that extracts key information from business documents. This demonstrates real-world structured data evaluation in a scenario requiring high accuracy across multiple fields. \n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d791a1",
   "metadata": {},
   "source": [
    "## 1. Data Sample\n",
    "\n",
    "Let's consider a single invoice document with three essential fields to extract: the invoice number, vendor name, and total amount. The following example shows both the ground truth (what we expect) and the prediction (what our system extracted) over five document samples. The data demonstrates five key situations that happen in document processing:\n",
    "\n",
    "1. **Correct Match (True Positive)**: When our system extracts \"INV-001\" and that's correct\n",
    "2. **Correct Empty (True Negative)**: When our system and ground truth both say a field is empty\n",
    "3. **False Find (False Alarm)**: When our system finds \"INV-003\" but there shouldn't be anything\n",
    "4. **Wrong Value (False Discovery)**: When our system finds \"Global Svcs\" but it should be \"Global Services\"\n",
    "5. **Missed Value (False Negative)**: When our system misses \"INV-005\" that exists in the document\n",
    "\n",
    "Each document in our example shows different combinations of these scenarios, helping us understand where our system performs well and where it needs improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0380f6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"DocumentID\": 1,\n",
      "        \"invoice_number\": \"INV-001\",\n",
      "        \"vendor\": \"Acme Corp\",\n",
      "        \"total\": 1500.0\n",
      "    },\n",
      "    {\n",
      "        \"DocumentID\": 2,\n",
      "        \"invoice_number\": \"INV-002\",\n",
      "        \"vendor\": \"\",\n",
      "        \"total\": 2200.5\n",
      "    },\n",
      "    {\n",
      "        \"DocumentID\": 3,\n",
      "        \"invoice_number\": \"\",\n",
      "        \"vendor\": \"SuperTech\",\n",
      "        \"total\": null\n",
      "    },\n",
      "    {\n",
      "        \"DocumentID\": 4,\n",
      "        \"invoice_number\": \"INV-004\",\n",
      "        \"vendor\": \"Global Services\",\n",
      "        \"total\": 3750.0\n",
      "    },\n",
      "    {\n",
      "        \"DocumentID\": 5,\n",
      "        \"invoice_number\": \"INV-005\",\n",
      "        \"vendor\": \"Mega Industries\",\n",
      "        \"total\": 1200.75\n",
      "    }\n",
      "]\n",
      "[\n",
      "    {\n",
      "        \"DocumentID\": 1,\n",
      "        \"invoice_number\": \"INV-001\",\n",
      "        \"vendor\": \"Acme Corp\",\n",
      "        \"total\": 1500.0\n",
      "    },\n",
      "    {\n",
      "        \"DocumentID\": 2,\n",
      "        \"invoice_number\": \"INV-002\",\n",
      "        \"vendor\": \"\",\n",
      "        \"total\": 2250.0\n",
      "    },\n",
      "    {\n",
      "        \"DocumentID\": 3,\n",
      "        \"invoice_number\": \"INV-003\",\n",
      "        \"vendor\": \"\",\n",
      "        \"total\": null\n",
      "    },\n",
      "    {\n",
      "        \"DocumentID\": 4,\n",
      "        \"invoice_number\": \"\",\n",
      "        \"vendor\": \"Global Svcs\",\n",
      "        \"total\": 3750.0\n",
      "    },\n",
      "    {\n",
      "        \"DocumentID\": 5,\n",
      "        \"invoice_number\": \"\",\n",
      "        \"vendor\": \"\",\n",
      "        \"total\": null\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "ground_truth_data = [\n",
    "    {'DocumentID': 1, 'invoice_number': 'INV-001', 'vendor': 'Acme Corp', 'total': 1500.00},\n",
    "    {'DocumentID': 2, 'invoice_number': 'INV-002', 'vendor': '', 'total': 2200.50},\n",
    "    {'DocumentID': 3, 'invoice_number': '', 'vendor': 'SuperTech', 'total': None},\n",
    "    {'DocumentID': 4, 'invoice_number': 'INV-004', 'vendor': 'Global Services', 'total': 3750.00},\n",
    "    {'DocumentID': 5, 'invoice_number': 'INV-005', 'vendor': 'Mega Industries', 'total': 1200.75}\n",
    "]\n",
    "\n",
    "prediction_data = [\n",
    "    {'DocumentID': 1, 'invoice_number': 'INV-001', 'vendor': 'Acme Corp', 'total': 1500.00},  # All true positives\n",
    "    {'DocumentID': 2, 'invoice_number': 'INV-002', 'vendor': '', 'total': 2250.00},  # invoice_number: True positve, vendor: True negative, total: False discovery\n",
    "    {'DocumentID': 3, 'invoice_number': 'INV-003', 'vendor': '', 'total': None},  # invoice_number: False alarm, vendor: False negative, total: True negative\n",
    "    {'DocumentID': 4, 'invoice_number': '', 'vendor': 'Global Svcs', 'total': 3750.00},  # invoice_number: False negative, vendor: False discovery, total: True positive\n",
    "    {'DocumentID': 5, 'invoice_number': '', 'vendor': '', 'total': None}  # invoice_number: False negative, vendor: False negative, total: False negative\n",
    "]\n",
    "\n",
    "print(json.dumps(ground_truth_data,indent=4))\n",
    "print(json.dumps(prediction_data,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad72619",
   "metadata": {},
   "source": [
    "## 2. Compare and classify\n",
    "\n",
    "The sample data above represents common scenarios in document processing. Before we can calculate our evaluation metrics, we need to classify each prediction into the appropriate category. This classification forms the foundation for calculating higher-level metrics like precision and recall. The function below compares our predictions against ground truth and classifies each field into one of five categories:\n",
    "* True positive (TP) is when ground truth matches the estimate and both have actual values\n",
    "* True negative (TN) is when ground truth matches the estimation and both have empty values\n",
    "* When ground truth does not match estimate, and estimation has actual value.\n",
    "    * False alarm (FA) is given above condition, and ground truth is empty.\n",
    "    * False discovery (FD) is given above condition, and ground truth has actual value.\n",
    "\n",
    "\n",
    "* False negative (FN) is when ground truth does not match estimation, estimation has null value and ground truth has actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6a6cc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field-level count:\n",
      "{\n",
      "    \"invoice_number\": {\n",
      "        \"TP\": 2,\n",
      "        \"TN\": 0,\n",
      "        \"FA\": 1,\n",
      "        \"FD\": 0,\n",
      "        \"FN\": 2\n",
      "    },\n",
      "    \"vendor\": {\n",
      "        \"TP\": 1,\n",
      "        \"TN\": 1,\n",
      "        \"FA\": 0,\n",
      "        \"FD\": 1,\n",
      "        \"FN\": 2\n",
      "    },\n",
      "    \"total\": {\n",
      "        \"TP\": 2,\n",
      "        \"TN\": 1,\n",
      "        \"FA\": 0,\n",
      "        \"FD\": 1,\n",
      "        \"FN\": 1\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def compare_and_classify_field_predictions(ground_truth_data, prediction_data):\n",
    "    \"\"\"\n",
    "    Compare ground truth and prediction data at field level and count metrics.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_data: List of dictionaries containing ground truth values\n",
    "        prediction_data: List of dictionaries containing prediction values\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with counts of TP, TN, FA, FD, FN for each field type\n",
    "    \"\"\"\n",
    "    # Get all unique field names as a union from both datasets, excluding DocumentID\n",
    "    fields = set().union(*(set(doc.keys()) for doc in ground_truth_data + prediction_data)) - {'DocumentID'}\n",
    "    \n",
    "    # Initialize counters for each field\n",
    "    results = {field: {'TP': 0, 'TN': 0, 'FA': 0, 'FD': 0, 'FN': 0} for field in fields}\n",
    "    \n",
    "    # Check if value is empty or None\n",
    "    def is_empty(value):\n",
    "        if value is None:\n",
    "            return True\n",
    "        if isinstance(value, str) and value.strip() == '':\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # Check if two values match\n",
    "    def values_match(val1, val2):\n",
    "        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):\n",
    "            # For numeric values, allow small tolerance\n",
    "            return abs(val1 - val2) < 0.01\n",
    "        return val1 == val2\n",
    "    \n",
    "    # Process each document\n",
    "    for gt_doc, pred_doc in zip(ground_truth_data, prediction_data):\n",
    "        assert gt_doc['DocumentID'] == pred_doc['DocumentID'], \"Document IDs don't match\"\n",
    "        \n",
    "        # Check each field\n",
    "        for field in fields:\n",
    "            # Get values, defaulting to None if the field doesn't exist in a document\n",
    "            gt_value = gt_doc.get(field, None)\n",
    "            pred_value = pred_doc.get(field, None)\n",
    "            \n",
    "            gt_empty = is_empty(gt_value)\n",
    "            pred_empty = is_empty(pred_value)\n",
    "            \n",
    "            if not gt_empty and not pred_empty:\n",
    "                # Both have values\n",
    "                if values_match(gt_value, pred_value):\n",
    "                    # Values match - True Positive\n",
    "                    results[field]['TP'] += 1\n",
    "                else:\n",
    "                    # Values don't match - False Discovery\n",
    "                    results[field]['FD'] += 1\n",
    "            elif gt_empty and pred_empty:\n",
    "                # Both empty - True Negative\n",
    "                results[field]['TN'] += 1\n",
    "            elif gt_empty and not pred_empty:\n",
    "                # Ground truth empty, prediction has value - False Alarm\n",
    "                results[field]['FA'] += 1\n",
    "            elif not gt_empty and pred_empty:\n",
    "                # Ground truth has value, prediction empty - False Negative\n",
    "                results[field]['FN'] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "evaluation_results = compare_and_classify_field_predictions(ground_truth_data, prediction_data)\n",
    "print(\"Field-level count:\")\n",
    "\n",
    "print(json.dumps(evaluation_results,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faee8fe",
   "metadata": {},
   "source": [
    "## 3. Compute evaluation metrics\n",
    "\n",
    "Understanding these metrics helps us assess different aspects of our extraction system:\n",
    "\n",
    "* **Precision** measures how accurate our positive predictions are. A high precision means when our system extracts a value, it's usually correct.\n",
    "  * Calculation: Precision = TP / (TP + FA + FD)\n",
    "  * Example: If precision is 0.90, 90% of the fields we extracted are correct.\n",
    "\n",
    "* **Recall** measures how complete our extractions are. A high recall means we're not missing many values that should be extracted.\n",
    "  * Calculation: Recall = TP / (TP + FN + FD)\n",
    "  * Example: If recall is 0.85, we successfully found 85% of all fields that should have been extracted.\n",
    "\n",
    "* **F1-score** balances precision and recall. It's particularly useful when you need a single metric to compare different models.\n",
    "  * Calculation: F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "  * A high F1-score indicates good performance in both precision and recall.\n",
    "  * A low F1-score suggests the system is struggling with either precision, recall, or both.\n",
    "\n",
    "* **Accuracy** measures the overall correctness of predictions, including both positive and negative cases.\n",
    "  * Calculation: Accuracy = (TP + TN) / (TP + TN + FA + FD + FN)\n",
    "  * Particularly valuable when correct identification of empty fields is as important as correct extractions\n",
    "  * Example: In legal document processing, correctly identifying when a field is empty can be crucial\n",
    "  * In sparse datasets, high accuracy indicates the system correctly handles both present and absent values\n",
    "\n",
    "Key points about these metrics:\n",
    "- Each metric provides different insights into system performance\n",
    "- The importance of each metric depends on your specific use case and requirements\n",
    "- Precision and recall often have a trade-off relationship\n",
    "- F1-score provides a balanced view of precision and recall\n",
    "- Accuracy gives insight into overall system reliability, including empty field handling\n",
    "\n",
    "When evaluating document processing systems:\n",
    "- Consider your specific use case requirements\n",
    "- Look at all metrics to get a complete picture of system performance\n",
    "- Align metric priorities with business requirements and error impact\n",
    "- Pay attention to both extraction accuracy and empty field handling\n",
    "\n",
    "The following code implements these metrics calculations:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb17b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"invoice_number\": {\n",
      "        \"precision\": 0.6666666666666666,\n",
      "        \"recall\": 0.5,\n",
      "        \"f1\": 0.5714285714285715,\n",
      "        \"accuracy\": 0.4,\n",
      "        \"counts\": {\n",
      "            \"TP\": 2,\n",
      "            \"TN\": 0,\n",
      "            \"FA\": 1,\n",
      "            \"FD\": 0,\n",
      "            \"FN\": 2\n",
      "        }\n",
      "    },\n",
      "    \"vendor\": {\n",
      "        \"precision\": 0.5,\n",
      "        \"recall\": 0.25,\n",
      "        \"f1\": 0.3333333333333333,\n",
      "        \"accuracy\": 0.4,\n",
      "        \"counts\": {\n",
      "            \"TP\": 1,\n",
      "            \"TN\": 1,\n",
      "            \"FA\": 0,\n",
      "            \"FD\": 1,\n",
      "            \"FN\": 2\n",
      "        }\n",
      "    },\n",
      "    \"total\": {\n",
      "        \"precision\": 0.6666666666666666,\n",
      "        \"recall\": 0.5,\n",
      "        \"f1\": 0.5714285714285715,\n",
      "        \"accuracy\": 0.6,\n",
      "        \"counts\": {\n",
      "            \"TP\": 2,\n",
      "            \"TN\": 1,\n",
      "            \"FA\": 0,\n",
      "            \"FD\": 1,\n",
      "            \"FN\": 1\n",
      "        }\n",
      "    },\n",
      "    \"overall\": {\n",
      "        \"precision\": 0.625,\n",
      "        \"recall\": 0.4166666666666667,\n",
      "        \"f1\": 0.5,\n",
      "        \"accuracy\": 0.4666666666666667,\n",
      "        \"counts\": {\n",
      "            \"TP\": 5,\n",
      "            \"TN\": 2,\n",
      "            \"FA\": 1,\n",
      "            \"FD\": 2,\n",
      "            \"FN\": 5\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def calculate_metrics(evaluation_results):\n",
    "    \"\"\"\n",
    "    Calculates precision, recall, F1-score, and accuracy from evaluation results\n",
    "    for individual fields and for an overall summary.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_results: Dictionary where keys are field names and values are\n",
    "                            dictionaries containing counts for 'TP', 'TN', 'FA',\n",
    "                            'FD', and 'FN'.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics (precision, recall, f1, accuracy) for each field\n",
    "        and an \"overall\" summary.\n",
    "    \"\"\"\n",
    "\n",
    "    def _safe_divide(numerator, denominator):\n",
    "        \"\"\"Helper to perform division, returning 0.0 on ZeroDivisionError.\"\"\"\n",
    "        try:\n",
    "            return numerator / denominator\n",
    "        except ZeroDivisionError:\n",
    "            return 0.0\n",
    "\n",
    "    def _calculate_metrics_from_counts(counts):\n",
    "        \"\"\"Helper function to calculate metrics for a single set of counts.\"\"\"\n",
    "        TP = counts.get('TP', 0)\n",
    "        TN = counts.get('TN', 0)\n",
    "        FA = counts.get('FA', 0)\n",
    "        FD = counts.get('FD', 0)\n",
    "        FN = counts.get('FN', 0)\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        # Use the safe_divide helper function for each calculation\n",
    "        precision = _safe_divide(TP, (TP + FA + FD))\n",
    "        recall = _safe_divide(TP, (TP + FN + FD))\n",
    "        \n",
    "        f1 = _safe_divide((2 * precision * recall), (precision + recall))\n",
    "        accuracy = _safe_divide((TP + TN), (TP + FA + FD + TN + FN))\n",
    "        \n",
    "        metrics['precision'] = precision\n",
    "        metrics['recall'] = recall\n",
    "        metrics['f1'] = f1\n",
    "        metrics['accuracy'] = accuracy\n",
    "        metrics['counts'] = counts\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    # Calculate metrics for each field using a dictionary comprehension\n",
    "    metrics = {\n",
    "        field: _calculate_metrics_from_counts(counts)\n",
    "        for field, counts in evaluation_results.items()\n",
    "    }\n",
    "    \n",
    "    # Aggregate counts for overall metrics using collections.Counter\n",
    "    overall_counts = collections.Counter()\n",
    "    for counts in evaluation_results.values():\n",
    "        overall_counts.update(counts)\n",
    "    \n",
    "    # Calculate overall metrics using the same helper function\n",
    "    overall_metrics = _calculate_metrics_from_counts(overall_counts)\n",
    "    \n",
    "    # Add overall metrics to the results dictionary\n",
    "    metrics['overall'] = overall_metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Then calculate metrics\n",
    "metrics_results = calculate_metrics(evaluation_results)\n",
    "\n",
    "print(json.dumps(metrics_results,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aabf76f",
   "metadata": {},
   "source": [
    "## 4. Visualize the results\n",
    "\n",
    "Effective visualization of evaluation metrics helps us:\n",
    "1. Quickly identify performance patterns\n",
    "2. Compare performance across different fields\n",
    "3. Communicate results to stakeholders\n",
    "4. Identify areas needing improvement\n",
    "\n",
    "Our visualization provides two key views:\n",
    "\n",
    "### Overall Metrics View\n",
    "Shows system-wide performance with visual progress bars where:\n",
    "- Each bar is scaled to 100% for easy comparison\n",
    "- Key metrics (Precision, Recall, F1-score, Accuracy) are displayed with both numeric values and visual bars\n",
    "\n",
    "### Field-Level Details View\n",
    "Provides a detailed breakdown per field showing:\n",
    "- Raw counts (TP, FA, TN, FN, FD)\n",
    "- Calculated metrics (Precision, Recall, F1-score, Accuracy)\n",
    "- Visual F1-score representation for quick performance assessment\n",
    "\n",
    "The following code generates these visualizations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f8d1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Overall Metrics ---\n",
      "Metric               Value Visual                \n",
      "--------------------------------------------------\n",
      "Precision            62.50% ████████████░░░░░░░░  \n",
      "Recall               41.67% ████████░░░░░░░░░░░░  \n",
      "F1 Score             50.00% ██████████░░░░░░░░░░  \n",
      "Accuracy             46.67% █████████░░░░░░░░░░░  \n",
      "\n",
      "\n",
      "=== FIELD-LEVEL METRICS ===\n",
      "\n",
      "Field                    TP     FA     TN     FN     FD     Prec   Recall       F1      Acc     Visual                \n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "invoice_number            2      1      0      2      0    66.67%  50.00%     57.14%  40.00%    ███████████░░░░░░░░░  \n",
      "vendor                    1      0      1      2      1    50.00%  25.00%     33.33%  40.00%    ██████░░░░░░░░░░░░░░  \n",
      "total                     2      0      1      1      1    66.67%  50.00%     57.14%  60.00%    ███████████░░░░░░░░░  \n"
     ]
    }
   ],
   "source": [
    "def visualize_metrics(metrics_results):\n",
    "    \"\"\"\n",
    "    Create a visual representation of the evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics_results: Dictionary with metrics and counts from calculate_metrics function\n",
    "    \"\"\"\n",
    "    # Get overall counts for the summary\n",
    "    overall = metrics_results['overall']\n",
    "    counts = overall['counts']\n",
    "    \n",
    "    # Create a visual bar with filled and empty blocks based on percentage\n",
    "    def create_visual_bar(percentage, width=20):\n",
    "        filled = int(percentage * width / 100)\n",
    "        return '█' * filled + '░' * (width - filled)\n",
    "    \n",
    "    # Format percentage for display\n",
    "    def format_pct(value):\n",
    "        return f\"{value * 100:.2f}%\"\n",
    "\n",
    "    \n",
    "    print(\"\\n--- Overall Metrics ---\")\n",
    "    print(\"Metric               Value Visual                \")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "    prec = overall['precision'] * 100\n",
    "    recall = overall['recall'] * 100\n",
    "    f1 = overall['f1'] * 100\n",
    "    accuracy = overall['accuracy'] * 100\n",
    "    \n",
    "    print(f\"Precision           {prec:6.2f}% {create_visual_bar(prec)}  \")\n",
    "    print(f\"Recall              {recall:6.2f}% {create_visual_bar(recall)}  \")\n",
    "    print(f\"F1 Score            {f1:6.2f}% {create_visual_bar(f1)}  \")\n",
    "    print(f\"Accuracy            {accuracy:6.2f}% {create_visual_bar(accuracy)}  \")\n",
    "    \n",
    "    # === FIELD-LEVEL METRICS ===\n",
    "    print(\"\\n\\n=== FIELD-LEVEL METRICS ===\\n\")\n",
    "    \n",
    "    # Header\n",
    "    print(\"Field                    TP     FA     TN     FN     FD     Prec   Recall       F1      Acc     Visual                \")\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    # Print metrics for each field except 'overall'\n",
    "    for field, metrics in metrics_results.items():\n",
    "        if field == 'overall':\n",
    "            continue\n",
    "        \n",
    "        field_counts = metrics['counts']\n",
    "        tp = field_counts['TP']\n",
    "        fa = field_counts['FA']\n",
    "        tn = field_counts['TN']\n",
    "        fn = field_counts['FN']\n",
    "        fd = field_counts['FD']\n",
    "        \n",
    "        prec = metrics['precision'] * 100\n",
    "        recall = metrics['recall'] * 100\n",
    "        f1 = metrics['f1'] * 100\n",
    "        accuracy = metrics['accuracy'] * 100\n",
    "        \n",
    "        # Choose which metric to visualize (using F1 score)\n",
    "        visual = create_visual_bar(f1)\n",
    "        \n",
    "        print(f\"{field:20s} {tp:6d} {fa:6d} {tn:6d} {fn:6d} {fd:6d} {prec:8.2f}% {recall:6.2f}% {f1:9.2f}% {accuracy:6.2f}%    {visual}  \")\n",
    "\n",
    "visualize_metrics(metrics_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7caf20",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "\n",
    "In this workshop, we built a foundation for evaluating structured data extraction systems through:\n",
    "1. A classification framework (TP, TN, FA, FD, FN) for comparing extracted versus ground truth values\n",
    "2. Key performance metrics (precision, recall, F1-score, accuracy) derived from these classifications\n",
    "3. Visualization tools to analyze performance at system and field levels\n",
    "\n",
    "Important considerations:\n",
    "- Our example used simple exact matching - real applications need more sophisticated comparison logic:\n",
    "  * Case/punctuation sensitivity\n",
    "  * Numeric tolerances\n",
    "  * Approximate matching for longer texts\n",
    "  * Format standardization\n",
    "\n",
    "- Our example used flat structure - real documents often have complex nested data:\n",
    "  * Lists (e.g., line items in invoices)\n",
    "  * Nested objects (e.g., address details)\n",
    "  * Arrays of objects (e.g., multiple parties in contracts)\n",
    "  * These require additional logic for matching and metric calculations\n",
    "\n",
    "This framework provides a starting point - adapt the matching logic, handling of nested structures, and visualization to meet your specific evaluation needs.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "* Customize the evaluation framework for your specific document types and field structures\n",
    "* Implement appropriate matching techniques for different field types in your data\n",
    "* Integrate this evaluation process into your model development and monitoring workflows\n",
    "* Use the insights gained to iteratively improve your extraction models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9cd48c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
