{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Multimodal RAGAS Implementation - Complete Tutorial\n",
    "\n",
    "This notebook provides a comprehensive, step-by-step implementation of the Enhanced Multimodal RAGAS system. It combines traditional RAGAS (Retrieval-Augmented Generation Assessment) with custom multimodal metrics using ImageBind embeddings.\n",
    "\n",
    "##  Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. **How to set up multimodal embeddings** using ImageBind for text, vision, and audio\n",
    "2. **How to integrate RAGAS with custom multimodal metrics** for comprehensive evaluation\n",
    "3. **How to evaluate retrieval systems across different modalities** (text-only, vision-only, audio-only, multimodal)\n",
    "4. **How to implement comprehensive evaluation workflows** with detailed metrics and visualizations\n",
    "5. **How to interpret evaluation results** and understand what each metric tells us\n",
    "\n",
    "##  System Architecture Overview\n",
    "\n",
    "The system consists of several key components:\n",
    "- **ImageBind Model**: Creates unified embeddings across text, vision, and audio modalities\n",
    "- **FAISS Indices**: Enables efficient similarity search for each modality\n",
    "- **RAGAS Metrics**: Provides standard RAG evaluation (answer relevancy, faithfulness, context precision/recall)\n",
    "- **Custom Multimodal Metrics**: Adds modality-specific evaluation capabilities\n",
    "- **SageMaker Integration**: Uses AWS SageMaker endpoints for LLM-based evaluation\n",
    "\n",
    "##  What We'll Evaluate\n",
    "\n",
    "We'll test four different retrieval strategies:\n",
    "1. **Text-only retrieval**: Using only text embeddings\n",
    "2. **Vision-only retrieval**: Using only image embeddings\n",
    "3. **Audio-only retrieval**: Using only audio embeddings\n",
    "4. **Full multimodal retrieval**: Combining all three modalities\n",
    "\n",
    "Each strategy will be evaluated using both traditional RAGAS metrics and our custom multimodal metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "Let's start by importing all the necessary libraries. Each group serves a specific purpose in our multimodal evaluation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/ImageBind.git\n",
    "%cd ImageBind\n",
    "# Skip the requirements.txt as it has incompatible PyTorch versions\n",
    "%pip install pandas numpy torch torchaudio faiss-cpu scikit-learn boto3 langchain-aws langchain-core ragas datasets tqdm matplotlib seaborn ipython soundfile decord ftfy regex Pillow pytorchvideo==0.1.5\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python libraries for data manipulation and system operations\n",
    "\n",
    "IMAGE_BIND_PATH = ''\n",
    "assert IMAGE_BIND_PATH != \"\", \"Please put your path to the ImageBind folder downloaded by the above cell\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional, Union\n",
    "import asyncio\n",
    "\n",
    "# Data manipulation and numerical computing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning and similarity search\n",
    "import sys\n",
    "import torchvision.transforms.functional as F\n",
    "# Create the missing module\n",
    "sys.modules['torchvision.transforms.functional_tensor'] = F\n",
    "\n",
    "import torch\n",
    "import faiss\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ImageBind for multimodal embeddings\n",
    "# This is the core of our multimodal system - it creates unified embeddings\n",
    "sys.path.append(IMAGE_BIND_PATH)\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "\n",
    "# AWS and SageMaker for LLM integration\n",
    "import boto3\n",
    "from botocore.response import StreamingBody\n",
    "\n",
    "# LangChain for LLM interaction\n",
    "from langchain_aws.chat_models.sagemaker_endpoint import ChatSagemakerEndpoint, ChatModelContentHandler\n",
    "from langchain_core.messages import HumanMessage, AIMessageChunk, SystemMessage\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "# RAGAS for traditional RAG evaluation\n",
    "import ragas\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision, context_recall\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from datasets import Dataset\n",
    "\n",
    "# Visualization and progress tracking\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "# Custom utilities (our helper functions)\n",
    "\n",
    "from utils import *\n",
    "from utils import MultimodalShowcase\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" All libraries imported successfully!\")\n",
    "print(f\" PyTorch version: {torch.__version__}\")\n",
    "print(f\" CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Deploy SageMaker Endpoint\n",
    "\n",
    "Before we proceed with the evaluation, we need to deploy a SageMaker endpoint for LLM-based evaluation. This step will deploy the Qwen2.5-1.5B-Instruct model and automatically capture the endpoint name for use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import additional SageMaker libraries for deployment\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "print(\" Deploying Qwen2.5-1.5B-Instruct Model to SageMaker Endpoint...\")\n",
    "print(\" This may take 5-10 minutes for the endpoint to be ready.\")\n",
    "\n",
    "# Get the Hugging Face LLM image URI\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "    \"huggingface\",\n",
    "    version=\"3.0.1\"\n",
    ")\n",
    "\n",
    "# Get the execution role\n",
    "role = get_execution_role()\n",
    "print(f\" Using IAM role: {role}\")\n",
    "\n",
    "# Configure the model\n",
    "hub = {\n",
    "    'HF_TASK': 'text-generation', \n",
    "    'HF_MODEL_ID': 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "}\n",
    "\n",
    "# Create the HuggingFace model\n",
    "model_for_deployment = HuggingFaceModel(\n",
    "    role=role,\n",
    "    env=hub,\n",
    "    image_uri=llm_image,\n",
    ")\n",
    "\n",
    "# Generate a unique endpoint name\n",
    "endpoint_name = name_from_base(\"qwen25\")\n",
    "print(f\" Generated endpoint name: {endpoint_name}\")\n",
    "# Deployment configuration\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Deploy the model\n",
    "print(f\" Deploying to {instance_type} instance...\")\n",
    "predictor = model_for_deployment.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    routing_config={\n",
    "        \"RoutingStrategy\": sagemaker.enums.RoutingStrategy.LEAST_OUTSTANDING_REQUESTS\n",
    "    }\n",
    ")\n",
    "\n",
    "# Automatically assign the endpoint name to our configuration variable\n",
    "SAGEMAKER_ENDPOINT_NAME = endpoint_name\n",
    "\n",
    "print(f\" SageMaker endpoint deployed successfully!\")\n",
    "print(f\" Endpoint name: {SAGEMAKER_ENDPOINT_NAME}\")\n",
    "print(f\" Endpoint status: {predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration and Setup\n",
    "\n",
    "Now that our SageMaker endpoint is deployed, let's configure the key parameters for our evaluation system. The endpoint name has been automatically captured from the deployment above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Download and Extract the Cinepile Dataset\n",
    "\n",
    "Before we can begin our multimodal RAGAS evaluation, we need to download the Cinepile dataset, which contains the multimodal movie data (text, \n",
    "images, and audio) that our system will use for retrieval and evaluation.\n",
    "\n",
    "The Cinepile dataset is a curated collection of movie-related content that includes:  \n",
    "\n",
    "• **Text descriptions** and dialogue from movies  \n",
    "• **Video frames** extracted from movie clips    \n",
    "• **Audio clips** from movie scenes  \n",
    "• **Total size**: 487MB compressed, ~584MB when extracted  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Cinepile dataset (487MB)\n",
    "!wget \"https://d22xjg1p9prwde.cloudfront.net/cinepile-dataset.tar.gz\"\n",
    "\n",
    "# Extract the dataset\n",
    "!tar -xzf cinepile-dataset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Configuration Parameters\n",
    "# The SAGEMAKER_ENDPOINT_NAME has been automatically set from the deployment above\n",
    "# If you need to use an existing endpoint, uncomment and modify the line below:\n",
    "# SAGEMAKER_ENDPOINT_NAME = \"\"\n",
    "\n",
    "# Data paths - these point to our multimodal dataset\n",
    "CINEPILE_DATA_PATH = \"\"\n",
    "assert CINEPILE_DATA_PATH != \"\", \"Please Provide Your Path To The Cinepile Dataset\"\n",
    "set_cinepile_data_path(CINEPILE_DATA_PATH)\n",
    "\n",
    "# Device configuration - use GPU if available for faster processing\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\" Using device: {device}\")\n",
    "\n",
    "# Initialize global variables that will store our models and data\n",
    "# These will be populated as we progress through the notebook\n",
    "model = None  # ImageBind model\n",
    "data_entries = None  # Our multimodal dataset\n",
    "question_data = None  # Questions for evaluation\n",
    "text_index = None  # FAISS index for text embeddings\n",
    "vision_index = None  # FAISS index for vision embeddings\n",
    "audio_index = None  # FAISS index for audio embeddings\n",
    "multimodal_indices = None  # Combined multimodal indices\n",
    "normalized_embeddings = None  # Normalized embedding vectors\n",
    "\n",
    "print(\" Configuration complete!\")\n",
    "print(f\" SageMaker endpoint: {SAGEMAKER_ENDPOINT_NAME}\")\n",
    "print(f\" Data path: {CINEPILE_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load and Explore the Dataset\n",
    "\n",
    "Our dataset contains multimodal movie data from the Cinepile dataset. Each entry includes:\n",
    "- **Text**: Descriptions and dialogue\n",
    "- **Vision**: Movie frames/images\n",
    "- **Audio**: Sound clips from movies\n",
    "\n",
    "Let's load the data and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the multimodal dataset\n",
    "print(\" Loading cinepile dataset...\")\n",
    "data_entries = load_cinepile_data()\n",
    "\n",
    "print(f\" Loaded {len(data_entries)} multimodal entries\")\n",
    "print(\"\\n Let's examine the first entry to understand our data structure:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display the structure of the first data entry\n",
    "first_entry = data_entries[0]\n",
    "print(f\" Keys in each entry: {list(first_entry.keys())}\")\n",
    "print(\"\\n Sample entry details:\")\n",
    "for key, value in first_entry.items():\n",
    "    if isinstance(value, str):\n",
    "        # Truncate long strings for readability\n",
    "        display_value = value[:100] + \"...\" if len(value) > 100 else value\n",
    "        print(f\"  {key}: {display_value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)} - {value}\")\n",
    "\n",
    "print(\"\\n Understanding the data structure:\")\n",
    "print(\"   • Each entry represents a multimodal movie scene\")\n",
    "print(\"   • Text contains dialogue or descriptions\")\n",
    "print(\"   • Vision contains image file paths\")\n",
    "print(\"   • Audio contains audio file paths\")\n",
    "print(\"   • This rich multimodal data allows us to test different retrieval strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Initialize ImageBind Model\n",
    "\n",
    "ImageBind is Meta's groundbreaking model that creates unified embeddings across multiple modalities. It can understand and relate:\n",
    "- Text descriptions\n",
    "- Images/video frames  \n",
    "- Audio clips\n",
    "- And more!\n",
    "\n",
    "The key insight is that ImageBind learns a shared embedding space where semantically similar content from different modalities are close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ImageBind model\n",
    "print(\" Loading ImageBind model...\")\n",
    "print(\"   This may take a moment as we load the pre-trained weights\")\n",
    "\n",
    "# Load the huge version of ImageBind for best performance\n",
    "model = imagebind_model.imagebind_huge(pretrained=True).eval().to(device)\n",
    "\n",
    "print(\" ImageBind model loaded successfully!\")\n",
    "print(f\" Model device: {next(model.parameters()).device}\")\n",
    "print(\"\\n What makes ImageBind special:\")\n",
    "print(\"   • Creates unified embeddings across text, vision, and audio\")\n",
    "print(\"   • Semantically similar content from different modalities are close in embedding space\")\n",
    "print(\"   • Enables cross-modal retrieval (e.g., find images using text queries)\")\n",
    "print(\"   • Pre-trained on massive multimodal datasets\")\n",
    "\n",
    "# Test that the model is working\n",
    "print(\"\\n Quick model test...\")\n",
    "try:\n",
    "    # Create a simple test embedding\n",
    "    test_text = [\"A simple test\"]\n",
    "    test_inputs = {ModalityType.TEXT: data.load_and_transform_text(test_text, device)}\n",
    "    with torch.no_grad():\n",
    "        test_embeddings = model(test_inputs)\n",
    "    print(f\" Model test successful! Embedding shape: {test_embeddings[ModalityType.TEXT].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\" Model test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Multimodal ImageBind Embeddings\n",
    "\n",
    "Now we'll create embeddings for all our data across three modalities. This is where the magic happens - ImageBind will create vector representations that capture the semantic meaning of our text, images, and audio.\n",
    "\n",
    "###  What are embeddings?\n",
    "Embeddings are dense vector representations that capture semantic meaning. Similar content will have similar embeddings (measured by cosine similarity).\n",
    "\n",
    "###  Processing Strategy\n",
    "We process data in batches to manage memory efficiently, especially when dealing with large datasets and GPU memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for all modalities\n",
    "print(\" Creating ImageBind embeddings for all modalities...\")\n",
    "print(f\" Processing {len(data_entries)} entries in batches\")\n",
    "\n",
    "# Create embeddings with batch processing for memory efficiency\n",
    "embeddings = create_imagebind_embeddings(\n",
    "    model=model, \n",
    "    data_entries=data_entries, \n",
    "    batch_size=6\n",
    ")\n",
    "print(\"\\n Embeddings created successfully!\")\n",
    "print(\"\\n Embedding Statistics:\")\n",
    "print(f\"    Text embeddings: {embeddings['text'].shape}\")\n",
    "print(f\"    Vision embeddings: {embeddings['vision'].shape}\")\n",
    "print(f\"    Audio embeddings: {embeddings['audio'].shape}\")\n",
    "\n",
    "# Show sample embeddings to understand the data\n",
    "print(\"\\n Sample embedding vectors (first 10 dimensions):\")\n",
    "print(f\"    Text sample: {embeddings['text'][0][:10]}\")\n",
    "print(f\"    Vision sample: {embeddings['vision'][0][:10]}\")\n",
    "print(f\"    Audio sample: {embeddings['audio'][0][:10]}\")\n",
    "\n",
    "print(\"\\n Key insights about these embeddings:\")\n",
    "print(\"   • Each embedding is a 1024-dimensional vector\")\n",
    "print(\"   • Vectors are normalized (unit length) for cosine similarity\")\n",
    "print(\"   • Similar content across modalities will have similar embeddings\")\n",
    "print(\"   • These embeddings enable cross-modal retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create FAISS Indices for Efficient Similarity Search\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search. We'll create separate indices for each modality to enable fast retrieval.\n",
    "\n",
    "###  Why FAISS?\n",
    "- **Speed**: Much faster than brute-force similarity search\n",
    "- **Scalability**: Can handle millions of vectors\n",
    "- **Memory efficiency**: Optimized data structures\n",
    "- **GPU support**: Can leverage GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate FAISS indices for each modality\n",
    "print(\" Creating FAISS indices for efficient similarity search...\")\n",
    "\n",
    "indices, normalized_embeddings = create_separate_indices(embeddings)\n",
    "\n",
    "# Extract individual indices for easy access\n",
    "text_index = indices['text']\n",
    "vision_index = indices['vision']\n",
    "audio_index = indices['audio']\n",
    "\n",
    "print(\" FAISS indices created successfully!\")\n",
    "print(\"\\n Index Statistics:\")\n",
    "print(f\"    Text index: {text_index.ntotal} vectors, dimension {text_index.d}\")\n",
    "print(f\"    Vision index: {vision_index.ntotal} vectors, dimension {vision_index.d}\")\n",
    "print(f\"    Audio index: {audio_index.ntotal} vectors, dimension {audio_index.d}\")\n",
    "\n",
    "print(\"\\n Understanding FAISS indices:\")\n",
    "print(\"   • Each index stores normalized embeddings for fast cosine similarity\")\n",
    "print(\"   • IndexFlatIP performs exact inner product search\")\n",
    "print(\"   • Normalized vectors make inner product equivalent to cosine similarity\")\n",
    "print(\"   • These indices enable sub-second retrieval even with large datasets\")\n",
    "\n",
    "# Test the indices with a simple search\n",
    "print(\"\\n Testing index functionality...\")\n",
    "try:\n",
    "    # Search for the most similar items to the first text embedding\n",
    "    test_query = normalized_embeddings['text'][0:1]  # First embedding as query\n",
    "    distances, indices_result = text_index.search(test_query, k=3)\n",
    "    print(f\" Index test successful! Found {len(indices_result[0])} similar items\")\n",
    "    print(f\"   Similarity scores: {distances[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\" Index test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Multimodal Combined Indices\n",
    "\n",
    "Beyond single-modality retrieval, we can combine embeddings from multiple modalities. This enables more sophisticated retrieval strategies that leverage the strengths of different modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multimodal combined indices\n",
    "print(\" Creating multimodal combined indices...\")\n",
    "print(\"   This enables retrieval using multiple modalities simultaneously\")\n",
    "\n",
    "multimodal_indices, multimodal_embeddings = create_multimodal_indices(normalized_embeddings)\n",
    "\n",
    "print(\" Multimodal indices created successfully!\")\n",
    "print(\"\\n Multimodal Index Statistics:\")\n",
    "for combo_name, index in multimodal_indices.items():\n",
    "    print(f\"    {combo_name}: {index.ntotal} vectors, dimension {index.d}\")\n",
    "\n",
    "print(\"\\n Understanding multimodal combinations:\")\n",
    "print(\"   • text_vision: Averages text and vision embeddings\")\n",
    "print(\"   • text_audio: Averages text and audio embeddings\")\n",
    "print(\"   • full_multimodal: Averages all three modalities\")\n",
    "print(\"   • Each combination captures different aspects of semantic similarity\")\n",
    "print(\"   • Enables queries that consider multiple types of information\")\n",
    "\n",
    "# Show the available multimodal combinations\n",
    "print(f\"\\n Available retrieval strategies: {list(multimodal_indices.keys())}\")\n",
    "\n",
    "print(\"\\n When to use each strategy:\")\n",
    "print(\"    Text-only: When you have text queries and want text-based results\")\n",
    "print(\"    Vision-only: When you have image queries or want visually similar results\")\n",
    "print(\"    Audio-only: When you have audio queries or want acoustically similar results\")\n",
    "print(\"    Multimodal: When you want to leverage multiple types of information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Demonstrate Retrieval Capabilities\n",
    "\n",
    "Let's see our multimodal retrieval system in action! We'll demonstrate how different modalities can be used for retrieval and show the actual results.\n",
    "\n",
    "###  Interactive Retrieval Demo\n",
    "We'll use our MultimodalShowcase class to demonstrate retrieval across different modalities with real examples from our movie dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the showcase system for interactive demonstrations\n",
    "print(\" Setting up multimodal retrieval showcase...\")\n",
    "\n",
    "showcase = MultimodalShowcase(\n",
    "    data_entries=data_entries,\n",
    "    question_data=question_data,\n",
    "    text_index=text_index,\n",
    "    vision_index=vision_index,\n",
    "    audio_index=audio_index,\n",
    "    multimodal_indices=multimodal_indices,\n",
    "    normalized_embeddings=normalized_embeddings\n",
    ")\n",
    "\n",
    "# Setup the showcase with question data\n",
    "showcase.setup_showcase()\n",
    "\n",
    "print(\" Showcase system ready!\")\n",
    "print(f\" Loaded {len(showcase.question_data['questions'])} evaluation questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Text-Based Retrieval Demo\n",
    "\n",
    "Let's start with text-based retrieval. This shows how our system finds relevant content using only textual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate text-based retrieval with formatted text display\n",
    "print(\" DEMONSTRATING TEXT-BASED RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select a few questions to demonstrate\n",
    "demo_questions = [0, 1, 2]\n",
    "\n",
    "for question_idx in demo_questions:\n",
    "    question = showcase.question_data['questions'].iloc[question_idx]\n",
    "    movie_name = showcase.question_data['dataframe'].iloc[question_idx]['movie_name']\n",
    "    \n",
    "    print(f\"\\n\" + \"\" * 25)\n",
    "    print(f\"DEMO QUESTION {question_idx}: {movie_name}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\" * 50)\n",
    "    \n",
    "    # Text retrieval - shows FULL formatted text content\n",
    "    print(f\"\\n\" + \"\" * 15 + \" TEXT WITH CHOICES \" + \"\" * 15)\n",
    "    showcase.show_text_retrieval_with_choices(question_idx, top_k=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n Text Retrieval Insights:\")\n",
    "print(\"   • Uses semantic similarity between question text and document text\")\n",
    "print(\"   • Good for finding conceptually related content\")\n",
    "print(\"   • May miss visual or auditory cues that could be relevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vision-Based Retrieval Demo\n",
    "\n",
    "Now let's see how vision-based retrieval works. This is particularly interesting because it can find visually similar scenes even when the text descriptions are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vision-based retrieval with actual image display\n",
    "print(\" DEMONSTRATING VISION-BASED RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the same questions but show vision retrieval\n",
    "demo_questions = [0, 1]\n",
    "\n",
    "for question_idx in demo_questions:\n",
    "    question = showcase.question_data['questions'].iloc[question_idx]\n",
    "    movie_name = showcase.question_data['dataframe'].iloc[question_idx]['movie_name']\n",
    "    \n",
    "    print(f\"\\n\" + \"\" * 25)\n",
    "    print(f\"DEMO QUESTION {question_idx}: {movie_name}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\" * 50)\n",
    "    \n",
    "    # Vision retrieval - displays actual images/frames\n",
    "    print(f\"\\n\" + \"\" * 15 + \" VISION RETRIEVAL \" + \"\" * 15)\n",
    "    showcase.show_vision_retrieval_with_choices(question_idx, top_k=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n Vision Retrieval Insights:\")\n",
    "print(\"   • Finds visually similar scenes and compositions\")\n",
    "print(\"   • Can identify similar lighting, colors, and visual elements\")\n",
    "print(\"   • Particularly useful for visual question answering\")\n",
    "print(\"   • May capture visual context that text descriptions miss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Audio-Based Retrieval Demo\n",
    "\n",
    "Audio-based retrieval can find content based on acoustic similarity, including music, sound effects, and speech patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate audio-based retrieval with audio playback\n",
    "print(\" DEMONSTRATING AUDIO-BASED RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use selected questions for audio demonstration\n",
    "demo_questions = [0, 1]\n",
    "\n",
    "for question_idx in demo_questions:\n",
    "    question = showcase.question_data['questions'].iloc[question_idx]\n",
    "    movie_name = showcase.question_data['dataframe'].iloc[question_idx]['movie_name']\n",
    "    \n",
    "    print(f\"\\n\" + \"\" * 25)\n",
    "    print(f\"DEMO QUESTION {question_idx}: {movie_name}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\" * 50)\n",
    "    \n",
    "    # Audio retrieval - plays actual audio files\n",
    "    print(f\"\\n\" + \"\" * 15 + \" AUDIO RETRIEVAL \" + \"\" * 15)\n",
    "    showcase.show_audio_retrieval_with_choices(question_idx, top_k=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n Audio Retrieval Insights:\")\n",
    "print(\"   • Finds acoustically similar content\")\n",
    "print(\"   • Can identify similar music, sound effects, or speech patterns\")\n",
    "print(\"   • Useful for questions about audio content or emotional tone\")\n",
    "print(\"   • Captures auditory context that other modalities might miss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multimodal Retrieval Demo\n",
    "\n",
    "Finally, let's see the full multimodal retrieval in action, combining text, vision, and audio for comprehensive content understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate full multimodal retrieval with all modalities\n",
    "print(\" DEMONSTRATING FULL MULTIMODAL RETRIEVAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use selected questions for multimodal demonstration\n",
    "demo_questions = [0, 1]\n",
    "\n",
    "for question_idx in demo_questions:\n",
    "    question = showcase.question_data['questions'].iloc[question_idx]\n",
    "    movie_name = showcase.question_data['dataframe'].iloc[question_idx]['movie_name']\n",
    "    \n",
    "    print(f\"\\n\" + \"\" * 25)\n",
    "    print(f\"DEMO QUESTION {question_idx}: {movie_name}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\" * 50)\n",
    "    \n",
    "    # Multimodal retrieval - shows text, images, and plays audio\n",
    "    print(f\"\\n\" + \"\" * 15 + \" MULTIMODAL RETRIEVAL \" + \"\" * 15)\n",
    "    showcase.show_multimodal_retrieval_with_choices(question_idx, top_k=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"\\n Multimodal Retrieval Insights:\")\n",
    "print(\"   • Combines information from text, vision, and audio\")\n",
    "print(\"   • Provides the most comprehensive understanding of content\")\n",
    "print(\"   • Can capture nuances that single modalities might miss\")\n",
    "print(\"   • Best for complex questions requiring multiple types of understanding\")\n",
    "print(\"   • May be computationally more expensive but often more accurate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Comprehensive Multimodal RAGAS Evaluation\n",
    "\n",
    "Now we'll run our comprehensive evaluation system that combines traditional RAGAS metrics with our custom multimodal metrics.\n",
    "\n",
    "###  What We're Evaluating\n",
    "\n",
    "**Traditional RAGAS Metrics:**\n",
    "- **Answer Relevancy**: How relevant is the generated answer to the question?\n",
    "- **Faithfulness**: Is the answer faithful to the retrieved context?\n",
    "- **Context Precision**: How precise is the retrieved context?\n",
    "- **Context Recall**: How much of the relevant context was retrieved?\n",
    "\n",
    "**Custom Multimodal Metrics:**\n",
    "- **Multimodal Faithfulness**: Cross-modal consistency of retrieved content\n",
    "- **Multimodal Relevancy**: Semantic relevance across modalities\n",
    "- **Average Similarity**: Overall embedding similarity scores\n",
    "\n",
    "**Standard Retrieval Metrics:**\n",
    "- **Precision@1**: Is the top result correct?\n",
    "- **MRR@5**: Mean Reciprocal Rank of correct answers in top 5\n",
    "- **NDCG@5**: Normalized Discounted Cumulative Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our comprehensive evaluation system\n",
    "from utils import EnhancedMultimodalEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuration for evaluation\n",
    "NUM_QUESTIONS_TO_TEST = 5  # Adjust this based on your needs\n",
    "\n",
    "print(\" Starting Enhanced Multimodal Evaluation\")\n",
    "print(f\" Testing {NUM_QUESTIONS_TO_TEST} questions across 4 retrieval strategies\")\n",
    "print(\"\\n Evaluation Strategies:\")\n",
    "print(\"   1.  Text-only retrieval\")\n",
    "print(\"   2.  Vision-only retrieval\")\n",
    "print(\"   3.  Audio-only retrieval\")\n",
    "print(\"   4.  Full multimodal retrieval\")\n",
    "\n",
    "# Initialize and setup the evaluator\n",
    "evaluator = EnhancedMultimodalEvaluator(SAGEMAKER_ENDPOINT_NAME, NUM_QUESTIONS_TO_TEST)\n",
    "evaluator.setup_evaluation_components()\n",
    "\n",
    "print(\"\\n Evaluation system initialized!\")\n",
    "print(\" Ready to run comprehensive multimodal evaluation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‍ Running the Evaluation\n",
    "\n",
    "This is where we run our comprehensive evaluation across all strategies and metrics. The evaluation will:\n",
    "\n",
    "1. **Test each retrieval strategy** on our question set\n",
    "2. **Calculate traditional RAGAS metrics** using the SageMaker LLM\n",
    "3. **Compute custom multimodal metrics** using ImageBind embeddings\n",
    "4. **Generate detailed performance comparisons** across all strategies\n",
    "\n",
    "⏱ **Note**: This may take several minutes as we're running LLM evaluations for each question and strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the comprehensive evaluation\n",
    "print(\"‍ RUNNING COMPREHENSIVE MULTIMODAL EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"⏱ This may take several minutes as we evaluate each strategy...\")\n",
    "print(\"\\n Progress will be shown for each question and strategy\")\n",
    "\n",
    "# Execute the evaluation\n",
    "evaluation_results = evaluator.run_comprehensive_evaluation()\n",
    "\n",
    "print(\"\\n Evaluation completed successfully!\")\n",
    "print(f\" Evaluated {len(evaluation_results)} different strategies\")\n",
    "print(f\" Each strategy tested on {NUM_QUESTIONS_TO_TEST} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Results Analysis and Visualization\n",
    "\n",
    "Now let's analyze our results! We'll create comprehensive visualizations and tables to understand how each retrieval strategy performs.\n",
    "\n",
    "###  Performance Comparison Table\n",
    "\n",
    "This table shows the performance of each strategy across all metrics. **Green highlighting** indicates the best performing strategy for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" COMPREHENSIVE EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create styled table with green highlighting for best performance\n",
    "styled_table = evaluator.create_styled_comparison_table()\n",
    "if styled_table is not None:\n",
    "    print(\"\\n Performance Comparison Table (Best values highlighted in green):\")\n",
    "    display(styled_table)\n",
    "else:\n",
    "    print(\" Could not create styled table, showing basic results...\")\n",
    "    \n",
    "# Get the raw data for additional analysis\n",
    "df, best_metrics, modality_effectiveness = evaluator.create_comparison_table()\n",
    "\n",
    "print(\"\\n BEST PERFORMING STRATEGIES BY METRIC:\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in best_metrics.items():\n",
    "    if isinstance(value, tuple):\n",
    "        strategy, score = value\n",
    "        print(f\" {metric}: {strategy} ({score:.3f})\")\n",
    "    else:\n",
    "        # Find the best strategy for this metric\n",
    "        best_strategy = df[metric].idxmax()\n",
    "        best_value = df[metric].max()\n",
    "        print(f\" {metric}: {best_strategy} ({best_value:.3f})\")\n",
    "\n",
    "print(\"\\n OVERALL MODALITY EFFECTIVENESS RANKING:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate average scores for each strategy\n",
    "strategy_averages = []\n",
    "for strategy, metrics in modality_effectiveness.items():\n",
    "    avg_score = sum(metrics.values()) / len(metrics)\n",
    "    strategy_averages.append((strategy, avg_score))\n",
    "\n",
    "# Sort by average score (highest first)\n",
    "strategy_averages.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the ranking\n",
    "for i, (strategy, avg_score) in enumerate(strategy_averages, 1):\n",
    "    print(f\"{i}. {strategy}: {avg_score:.3f} (average across all metrics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Performance Visualization\n",
    "\n",
    "Let's create visualizations to better understand the performance patterns across different strategies and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "print(\" CREATING PERFORMANCE VISUALIZATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create a heatmap visualization\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(df, annot=True, fmt='.3f', cmap='RdYlGn', center=0.5)\n",
    "plt.title('Multimodal Evaluation Results\\n(Green = Better Performance)', fontweight='bold')\n",
    "plt.xlabel('Evaluation Metrics')\n",
    "plt.ylabel('Retrieval Strategies')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\" Visualizations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Understanding the Results - Key Insights\n",
    "\n",
    "Let's dive deep into what our evaluation results tell us about multimodal retrieval performance.\n",
    "\n",
    "###  Metric Interpretation Guide\n",
    "\n",
    "Understanding what each metric means is crucial for interpreting our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide detailed analysis of the results\n",
    "print(\" DETAILED RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n METRIC INTERPRETATION GUIDE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "metric_explanations = {\n",
    "    \"Precision@1\": \"Is the top retrieved result correct? (1.0 = perfect, 0.0 = wrong)\",\n",
    "    \"Recall@1\": \"Did we retrieve the correct answer in the top result?\",\n",
    "    \"MRR@5\": \"Mean Reciprocal Rank - how quickly do we find the right answer?\",\n",
    "    \"NDCG@5\": \"Normalized Discounted Cumulative Gain - quality of ranking\",\n",
    "    \"RAGAS Answer Relevancy\": \"How relevant is the generated answer to the question?\",\n",
    "    \"RAGAS Faithfulness\": \"Is the answer faithful to the retrieved context?\",\n",
    "    \"RAGAS Context Precision\": \"How precise is the retrieved context?\",\n",
    "    \"RAGAS Context Recall\": \"How much relevant context was retrieved?\",\n",
    "    \"Multimodal Faithfulness\": \"Cross-modal consistency of retrieved content\",\n",
    "    \"Multimodal Relevancy\": \"Semantic relevance across modalities\",\n",
    "    \"Multimodal Avg Similarity\": \"Overall embedding similarity scores\"\n",
    "}\n",
    "\n",
    "for metric, explanation in metric_explanations.items():\n",
    "    print(f\" {metric}:\")\n",
    "    print(f\"   {explanation}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n PRACTICAL RECOMMENDATIONS:\")\n",
    "print(\"-\" * 35)\n",
    "recommendations = [\n",
    "    \" Use text-only for purely conceptual questions\",\n",
    "    \" Use vision-only for visual scene analysis\",\n",
    "    \" Use audio-only for questions about sound, music, or emotion\",\n",
    "    \" Use multimodal for complex questions requiring multiple types of understanding\",\n",
    "    \" Monitor both traditional and multimodal metrics for comprehensive evaluation\",\n",
    "    \" Consider computational cost vs. performance trade-offs in production\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save Results and Export Data\n",
    "\n",
    "Let's save our evaluation results for future reference and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results for future reference\n",
    "print(\" SAVING EVALUATION RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Save the detailed results using the evaluator's save function\n",
    "evaluator.save_results()\n",
    "\n",
    "# Save the comparison table as CSV for easy analysis\n",
    "df.to_csv(\"multimodal_evaluation_comparison.csv\")\n",
    "print(\" Comparison table saved as 'multimodal_evaluation_comparison.csv'\")\n",
    "\n",
    "# Calculate the best overall strategy\n",
    "strategy_averages = []\n",
    "for strategy, metrics in modality_effectiveness.items():\n",
    "    avg_score = sum(metrics.values()) / len(metrics)\n",
    "    strategy_averages.append((strategy, avg_score))\n",
    "\n",
    "# Sort by average score (highest first)\n",
    "strategy_averages.sort(key=lambda x: x[1], reverse=True)\n",
    "best_strategy, best_score = strategy_averages[0]\n",
    "\n",
    "# Save a summary report\n",
    "summary_report = {\n",
    "    \"evaluation_summary\": {\n",
    "        \"num_questions_tested\": NUM_QUESTIONS_TO_TEST,\n",
    "        \"strategies_evaluated\": list(df.index),\n",
    "        \"metrics_calculated\": list(df.columns),\n",
    "        \"best_overall_strategy\": best_strategy,\n",
    "        \"best_overall_score\": best_score\n",
    "    },\n",
    "    \"best_metrics\": best_metrics,\n",
    "    \"modality_effectiveness_ranking\": dict(modality_effectiveness)\n",
    "}\n",
    "\n",
    "with open(\"evaluation_summary_report.json\", \"w\") as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(\" Summary report saved as 'evaluation_summary_report.json'\")\n",
    "print(\" Detailed results saved as 'enhanced_multimodal_evaluation_results.json'\")\n",
    "\n",
    "print(\"\\n FILES CREATED:\")\n",
    "print(\"    multimodal_evaluation_comparison.csv - Performance comparison table\")\n",
    "print(\"    evaluation_summary_report.json - High-level summary\")\n",
    "print(\"    enhanced_multimodal_evaluation_results.json - Detailed results\")\n",
    "\n",
    "print(\"\\n EVALUATION COMPLETE!\")\n",
    "print(\"All results have been saved and are ready for further analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've successfully implemented and run a comprehensive multimodal RAGAS evaluation system. Here's what you've accomplished:\n",
    "\n",
    "###  What You've Learned\n",
    "\n",
    "1. **Multimodal Embeddings**: How to create unified embeddings across text, vision, and audio using ImageBind\n",
    "2. **Efficient Retrieval**: How to build FAISS indices for fast similarity search across modalities\n",
    "3. **Comprehensive Evaluation**: How to combine traditional RAGAS metrics with custom multimodal metrics\n",
    "4. **Performance Analysis**: How to interpret and visualize multimodal retrieval performance\n",
    "5. **Strategic Insights**: When to use different retrieval strategies based on your use case\n",
    "\n",
    "###  Next Steps and Extensions\n",
    "\n",
    "Here are some ways you can extend this work:\n",
    "\n",
    "1. **Scale Up**: Test with larger datasets and more questions\n",
    "2. **Fine-tune Weights**: Experiment with different weighting schemes for multimodal combinations\n",
    "3. **Add More Modalities**: Extend to include other modalities like depth, thermal, etc.\n",
    "4. **Custom Metrics**: Develop domain-specific evaluation metrics\n",
    "5. **Production Deployment**: Optimize for real-time retrieval in production systems\n",
    "6. **A/B Testing**: Compare against other multimodal retrieval approaches\n",
    "\n",
    "###  Key Takeaways\n",
    "\n",
    "- **No Single Best Strategy**: Different retrieval strategies excel in different scenarios\n",
    "- **Multimodal is Powerful**: Combining modalities can provide richer understanding\n",
    "- **Evaluation is Complex**: Comprehensive evaluation requires multiple metrics and perspectives\n",
    "- **Context Matters**: The best approach depends on your specific use case and data\n",
    "\n",
    "###  Resources for Further Learning\n",
    "\n",
    "- [ImageBind Paper](https://arxiv.org/abs/2305.05665) - The foundational research\n",
    "- [RAGAS Documentation](https://docs.ragas.io/) - Traditional RAG evaluation\n",
    "- [FAISS Documentation](https://faiss.ai/) - Efficient similarity search\n",
    "- [Multimodal AI Research](https://paperswithcode.com/task/multimodal-learning) - Latest developments\n",
    "\n",
    "Thank you for following this comprehensive tutorial! You now have a solid foundation for building and evaluating multimodal retrieval systems. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
