{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7a6cf5-ab59-40d5-baf2-02b03ce4589e",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The purpose of this notebook is to demonstrate how to evaluate a basic RAG system. Overall there are many factors that can impact its performance. That is why, it is crucial to evaluate each component indvidually as well as to validate the entire system.\n",
    "Typically, a RAG system evaluation covers a wide range of componenents and configurations such as chunk size & strategy, embedding model, retrieval strategy, rerank model, LLM-As-A-Judge prompt, RAG prompt, and the LLM used to generate the final response.\n",
    "\n",
    "Given that this is an abridged version, we focus on how to evaluate the information retrieval task as well as how to validate the entire system end-to-end. For additional evaluations that cover embeddings, reranking, and LLM-As-A-Judge and RAG prompt engineering, please refer to the deep dive workshop covered in the [GenAI System Evaluation repository](https://github.com/aws-samples/genai-system-evaluation/tree/main).\n",
    "\n",
    "\n",
    "**Why do we need RAG evaluations?**\n",
    "\n",
    "Unlike for LLMs or embedding models, there are no leaderboards for an entire RAG system or its components. This can make it difficult to assess with which components and configuration to start with, or what to optimize when working with an existing RAG system. And to make matters worse, there are many factors that can impact the performance of a RAG system and its components. Therefore, it is crucial to have a systematic evaluation approach.\n",
    "\n",
    "Without it, a change in one part of the system, such as the chunk size that determines how a source text is stored in a knowledge base, could have an unintended impact on other parts of the system that could go unnoticed.\n",
    "\n",
    "And even in cases where we have benchmarks or leaderboards like for LLMs or embedding models, it is still important to understand “What” we’re using the model for. For example for embedding models, the most popular public benchmark is the Massive Text Embedding Benchmark or MTEB. HuggingFace maintains a leaderboard to compare general purpose embedding models against each other to see how they stack up against a wide range of tasks. \n",
    "This is a decent starting place, but you have to ask yourself, how well does this dataset compliment the task you really care about. If you are creating a RAG solution for Lawyers, then you are much more interested in how well the embedding model works for comparing legal text vs. how well it works for medical text. This is why it’s important to build out your own evaluation. A model that might not rank high on a general-purpose benchmark, could rank very high on your specific use case. If none of them work very well, then you can make a case for fine tuning an existing model on your data.\n",
    "\n",
    "**What metrics should you care about?**\n",
    "\n",
    "Which metrics you care about, depends on which part of the RAG system you are evaluating. \n",
    "For the information retrieval task you typically look at metrics like recall@k and precision@k.\n",
    "Whereas for the end-to-end evaluation besides human evaluation, you typically use an LLM-As-A-Judge technique with evaluation criteria such as context utilization, completeness, conciseness, context relevancy, and clarity.\n",
    "\n",
    "**How to evaluate**\n",
    "\n",
    "To perform the information retrieval evaluation, you need to set up a retrieval task. Generate vector representations of items (documents or chunks) in a shared semantic space and perform a K-nearest-neighbor search on them using a similarity measure (e.g. cosine-similarity). This gives you the top-k retrieved item for each query.\n",
    "Then you need a set of relevance judgments that indicate which documents are relevant to each query. These are typically created by human annotators or derived from click data in production systems.\n",
    "For each query, you then count the number of relevant items in the top-k retrieved results. Calculate the precision using (number of relevant documents / k). Average the precision values across all queries. And then you apply these same techniques to other metrics like recall, NDCG, or MAP for a more comprehensive information retrieval evaluation.\n",
    "\n",
    "To performe an end-to-end evaluation, you start with defining a evaluation prompt that incorporates your evaluation criteria, and then once you calibrated and aligned the LLM-As-A-Judge prompt with human preferences, you can use the LLM-As-A-Judge technique to create numerical scores for each of your evaluation criteria.\n",
    "\n",
    "## How do you create relevance judgements?\n",
    "This is a pretty manual process. For this example, we curated a dataset by taking large chunks of the Opensearch documentation into an LLM and asked it to come up with a couple example questions about the context. \n",
    "To enable experimentation, we corelate answers to 1 to 3 pages. By doing this, you can tweak the chunking strategy, but the relative file paths will stay constant so you don't have to redo your validation dataset every time you make a change to the chunks. \n",
    "\n",
    "\n",
    "# What will you do in this notebook? \n",
    "* You start with a basic sentence splitting chunking strategy, create embeddings for them, and then store them in a vector store (ChromaDB).\n",
    "* Then you use a pre-created, curated evaluation dataset to run a information retrieval task experiment and review the results based on metrics such as recall@k and precision@k.\n",
    "* Assuming that you've validated chunking strategy, embedding models, rerank models, LLM-As-A-Judge prompt, and the RAG prompt itself, you then move forward with validating the entire system through an end-to-end evaluation with the LLM-As-A-Judge technique.\n",
    "\n",
    "**Lets get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29588ea7-23a0-420d-8d27-6cf5f73f3e88",
   "metadata": {},
   "source": [
    "# Initialize clients and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98973b30-6269-4073-a4ba-29880921f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma and Bedrock clients initialized for region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError, BotoCoreError\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "from typing import cast, Dict, Any, List\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from functools import wraps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Node\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# Initialize Chroma client from our persisted store\n",
    "chroma_client = chromadb.PersistentClient(path=\"../data/chroma\")\n",
    "\n",
    "# Also initialize the bedrock client so we can call models\n",
    "config = Config(\n",
    "   retries = {\n",
    "      'max_attempts': 10,\n",
    "      'mode': 'standard'\n",
    "   }\n",
    ")\n",
    "\n",
    "session = boto3.Session(profile_name='team')\n",
    "# Get the region from the session, default to us-east-1 if not set\n",
    "region = session.region_name or 'us-east-1'\n",
    "\n",
    "bedrock = session.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=region,\n",
    "        config=config\n",
    ")\n",
    "\n",
    "print(\"Chroma and Bedrock clients initialized for region:\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ef785-c919-4eb9-83bb-1862572d619a",
   "metadata": {},
   "source": [
    "# Start running experiments!\n",
    "\n",
    "## Experiment 1\n",
    "In this first experiment we're going to set up a retrieval task using ChromaDB as vector store, Titan Text V2 as embedding model, and use a big chunk size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe690c",
   "metadata": {},
   "source": [
    "## 1.1 Create chunks\n",
    "In this step we first define a custom wrapper class around LlamaIndex to decouple the Chroma collection from LlamaIndex.\n",
    "And then we use this wrapper to split up documents from the OpenSearch documentation in the input dir into ~2046 chunk sizes with the overlap (or smaller if the file isn't that big). We should get around ~10k chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8003fb-15ea-4995-9859-6f0f0ba0b725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Created 10678 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Create a class to use instead of LlamaIndex Nodes. This way we decouple our chroma collections from LlamaIndexes\n",
    "class RAGChunk(BaseModel):\n",
    "    id_: str\n",
    "    text: str\n",
    "    metadata: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "class SentenceSplitterChunkingStrategy:\n",
    "    def __init__(self, input_dir: str, chunk_size: int = 256, chunk_overlap: int = 128):\n",
    "        self.input_dir = input_dir\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.pipeline = self._create_pipeline()\n",
    "\n",
    "        # Helper to get regex pattern for normalizing relative file paths.\n",
    "        self.relative_path_pattern = rf\"{re.escape(input_dir)}(/.*)\"\n",
    "\n",
    "    def _extract_relative_path(self, full_path):\n",
    "        # Get Regex pattern\n",
    "        pattern = self.relative_path_pattern\n",
    "        match = re.search(pattern, full_path)\n",
    "        if match:\n",
    "            return match.group(1).lstrip('/')\n",
    "        return None\n",
    "\n",
    "    def _create_pipeline(self) -> IngestionPipeline:\n",
    "        transformations = [\n",
    "            SentenceSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap),\n",
    "        ]\n",
    "        return IngestionPipeline(transformations=transformations)\n",
    "\n",
    "    def load_documents(self) -> List[Document]:\n",
    "        # If you're using a different type of file besides md, you'll want to change this. \n",
    "        return SimpleDirectoryReader(\n",
    "            input_dir=self.input_dir, \n",
    "            recursive=True,\n",
    "            required_exts=['.md']\n",
    "        ).load_data()\n",
    "\n",
    "    def to_ragchunks(self, nodes: List[Node]) -> List[RAGChunk]:\n",
    "        return [\n",
    "            RAGChunk(\n",
    "                id_=node.node_id,\n",
    "                text=node.text,\n",
    "                metadata={\n",
    "                    **node.metadata,\n",
    "                    'relative_path': self._extract_relative_path(node.metadata['file_path'])\n",
    "                }\n",
    "            )\n",
    "            for node in nodes\n",
    "        ]\n",
    "\n",
    "    def process(self) -> List[RAGChunk]:\n",
    "        documents = self.load_documents()\n",
    "        nodes = self.pipeline.run(documents=documents)\n",
    "        rag_chunks = self.to_ragchunks(nodes)\n",
    "        \n",
    "        print(f\"Processing complete. Created {len(rag_chunks)} chunks.\")\n",
    "        return rag_chunks\n",
    "    \n",
    "\n",
    "chunking_strategy = SentenceSplitterChunkingStrategy(\n",
    "    input_dir=\"../data/opensearch-docs/documentation-website\",\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=128\n",
    ")\n",
    "\n",
    "# Get the nodes from the chunker.\n",
    "chunks: RAGChunk = chunking_strategy.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5324adb-df27-4fcf-9ae9-b127c2e6f73e",
   "metadata": {},
   "source": [
    "### 1.2 Setup retrieval task\n",
    "The next step is to set up a retrieval task. Here we use ChromaDB as the vector database and create a wrapper class for the retrieval task. \n",
    "\n",
    "The embedding function is used during the creation of the collection and is also automatically applied to queries during the actual retrieval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61bf1505-2ae4-411c-a3c0-7ab20a5ab678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RetrievalResult(BaseModel):\n",
    "    id: str\n",
    "    document: str\n",
    "    embedding: List[float]\n",
    "    distance: float\n",
    "    metadata: Dict = {}\n",
    "\n",
    "# Base retrieval class. Can be reused if you decide to implement a different retrieval class.\n",
    "class BaseRetrievalTask(ABC):\n",
    "    @abstractmethod\n",
    "    def retrieve(self, query_text: str, n_results: int) -> List[RetrievalResult]:\n",
    "        \"\"\"\n",
    "        Retrieve documents based on the given query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query string to search for.\n",
    "\n",
    "        Returns:\n",
    "            List[RetrievalResult]: A list of RetrievalResult objects that are relevant to the query.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "# Example of a concrete implementation\n",
    "class ChromaDBRetrievalTask(BaseRetrievalTask):\n",
    "\n",
    "    def __init__(self, chroma_client, collection_name: str, embedding_function, chunks: List[RAGChunk]):\n",
    "        self.client = chroma_client\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_function = embedding_function\n",
    "        self.chunks = chunks\n",
    "\n",
    "        # Create the collection\n",
    "        self.collection = self._create_collection()\n",
    "\n",
    "    def _create_collection(self):\n",
    "        return self.client.get_or_create_collection(\n",
    "            name=self.collection_name,\n",
    "            embedding_function=self.embedding_function\n",
    "        )\n",
    "\n",
    "    def add_chunks_to_collection(self, batch_size: int = 20, num_workers: int = 10):\n",
    "        batches = [self.chunks[i:i + batch_size] for i in range(0, len(self.chunks), batch_size)]\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            futures = [executor.submit(self._add_batch, batch) for batch in batches]\n",
    "            for future in as_completed(futures):\n",
    "                future.result()  # This will raise an exception if one occurred during the execution\n",
    "        print('Finished Ingesting Chunks Into Collection')\n",
    "\n",
    "    def _add_batch(self, batch: List[RAGChunk]):\n",
    "        self.collection.add(\n",
    "            ids=[chunk.id_ for chunk in batch],\n",
    "            documents=[chunk.text for chunk in batch],\n",
    "            metadatas=[chunk.metadata for chunk in batch]\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query_text: str, n_results: int = 5) -> List[RetrievalResult]:\n",
    "        # Query the collection\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results,\n",
    "            include=['embeddings', 'documents', 'metadatas', 'distances']\n",
    "        )\n",
    "\n",
    "        # Transform the results into RetrievalResult objects\n",
    "        retrieval_results = []\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            retrieval_results.append(RetrievalResult(\n",
    "                id=results['ids'][0][i],\n",
    "                document=results['documents'][0][i],\n",
    "                embedding=results['embeddings'][0][i],\n",
    "                distance=results['distances'][0][i],\n",
    "                metadata=results['metadatas'][0][i] if results['metadatas'][0] else {}\n",
    "            ))\n",
    "\n",
    "        return retrieval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b316a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a custom embedding function with retry logic for Bedrock\n",
    "class CustomBedrockEmbeddingFunction(EmbeddingFunction[Documents]):\n",
    "    \"\"\"\n",
    "    A custom ChromaDB embedding function for Amazon Bedrock with robust throttling handling.\n",
    "\n",
    "    This function is designed to replace the standard AmazonBedrockEmbeddingFunction\n",
    "    by implementing a retry mechanism with exponential backoff and jitter using\n",
    "    the 'tenacity' library, ensuring stability during high-volume requests.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id: str = \"amazon.titan-embed-text-v1\",\n",
    "        region_name: str = \"us-east-1\",\n",
    "        client: Any = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the embedding function.\n",
    "\n",
    "        Args:\n",
    "            model_id: The identifier for the Amazon Bedrock embedding model.\n",
    "            region_name: The AWS region to use.\n",
    "            client: An optional pre-configured boto3 bedrock-runtime client.\n",
    "        \"\"\"\n",
    "        self.model_id = model_id\n",
    "        if client:\n",
    "            self.client = client\n",
    "        else:\n",
    "            print(\"Missing Bedrock client - please provide one!\")\n",
    "\n",
    "    @retry(\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=10),\n",
    "        stop=stop_after_attempt(10),\n",
    "        retry=retry_if_exception_type((ClientError)),\n",
    "        before_sleep=lambda retry_state: print(\n",
    "            f\"ThrottlingException encountered. Retrying attempt \"\n",
    "            f\"{retry_state.attempt_number} in {retry_state.seconds_since_start:.2f}s...\"\n",
    "        ),\n",
    "    )\n",
    "    def _get_embeddings_with_retry(self, input_text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Internal method to get an embedding with retries on throttling errors.\n",
    "        \"\"\"\n",
    "        body = json.dumps({\"inputText\": input_text})\n",
    "        \n",
    "        response = self.client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=self.model_id,\n",
    "            accept=\"*/*\",\n",
    "            contentType=\"application/json\",\n",
    "        )\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        \n",
    "        return cast(List[float], response_body.get(\"embedding\"))\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        \"\"\"\n",
    "        Processes a list of documents and returns their embeddings.\n",
    "\n",
    "        Args:\n",
    "            input: A list of text documents to embed.\n",
    "\n",
    "        Returns:\n",
    "            A list of embeddings for the input documents.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for text in input:\n",
    "            embedding = self._get_embeddings_with_retry(text)\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec70972-4873-4f6b-aa86-c317ad987bab",
   "metadata": {},
   "source": [
    "### 1.3 Populate the vector store\n",
    "Next we define the embedding function and populate the vector database with vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "015eb494-855c-45d7-8d51-8294aa758a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define some experiment variables\n",
    "EMBEDDING_MODEL_ID: str = 'amazon.titan-embed-text-v2:0'\n",
    "EXPERIMENT_1_COLLECTION_NAME: str = 'experiment_1_collection'\n",
    "\n",
    "# Initialize the custom embedding function\n",
    "bedrock_ef = CustomBedrockEmbeddingFunction(model_id=EMBEDDING_MODEL_ID, client=bedrock)\n",
    "\n",
    "# Create our retrieval task. All retrieval tasks in this tutorial implement BaseRetrievalTask which has the method retrieve()\n",
    "# If you'd like to extend this to a different retrieval configuration, all you have to do is create a class that that implements\n",
    "# this abstract class and the rest is the same\n",
    "experiment_1_retrieval_task: BaseRetrievalTask = ChromaDBRetrievalTask(\n",
    "    chroma_client = chroma_client, \n",
    "    collection_name = EXPERIMENT_1_COLLECTION_NAME,\n",
    "    embedding_function = bedrock_ef,\n",
    "    chunks = chunks\n",
    ")\n",
    "\n",
    "# This takes a while to run, therefore we already created the collection for the purpose of this workshop and commented out this line\n",
    "# experiment_1_retrieval_task.add_chunks_to_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baa46a88-bc03-48a3-85bc-1977f32ebbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Lets verify it works!\n",
    "print(len(experiment_1_retrieval_task.retrieve('What does * do?', n_results=1)) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31466f24-ac9f-4cfb-9958-6ffb50e4d710",
   "metadata": {},
   "source": [
    "### 1.4 Define validation dataset\n",
    "The pre-created validation dataset contains a set of 24 questions users might ask a RAG system designed to answer questions from the OpenSearch documentation.\n",
    "\n",
    "It has the following structure:\n",
    "\n",
    "query_text: I'm using version 2.1 of open search and trying to use zstd compression. Why isn't it working?\n",
    "\n",
    "relevant_doc_ids: \"[\"\"_im-plugin/index-codecs.md\"\", \"\"_tuning-your-cluster/performance.md\"\"]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "678b12de-085a-4e19-90f8-d7d508939061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset loaded with 24 entries.\n"
     ]
    }
   ],
   "source": [
    "# Load and clean the eval dataset for the information retrieval task\n",
    "def get_clean_eval_dataset():\n",
    "    EVAL_PATH = '../data/eval-datasets/1_embeddings_validation.csv'\n",
    "    eval_df = pd.read_csv(EVAL_PATH)\n",
    "\n",
    "    # Clean up the DataFrame\n",
    "    eval_df = eval_df.rename(columns=lambda x: x.strip())  # Remove any leading/trailing whitespace from column names\n",
    "    eval_df = eval_df.drop(columns=[col for col in eval_df.columns if col.startswith('Unnamed')])  # Remove unnamed columns\n",
    "    eval_df = eval_df.dropna(how='all')  # Remove rows that are all NaN\n",
    "    \n",
    "    # Strip whitespace from string columns\n",
    "    for col in eval_df.select_dtypes(['object']):\n",
    "        eval_df[col] = eval_df[col].str.strip()\n",
    "    \n",
    "    # Ensure 'relevant_doc_ids' is a string column\n",
    "    eval_df['relevant_doc_ids'] = eval_df['relevant_doc_ids'].astype(str)\n",
    "\n",
    "    return eval_df\n",
    "\n",
    "eval_df = get_clean_eval_dataset()\n",
    "\n",
    "print(f\"Validation dataset loaded with {len(eval_df)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae59c14-2f8f-4274-8231-318cb3dc1321",
   "metadata": {},
   "source": [
    "### 1.5 Define information retrieval metrics\n",
    "The IRMetricsCalculator class below calculates a series of metrics that are useful for evaluating the information retrieval task in a RAG system. Remember, we are only evaluating the retrieval at this stage, and not yet the sytems/models ability to create an answer from the information retrieval results.\n",
    "\n",
    "#### Metrics\n",
    "* precision@k - are all the found chunks correct chunks?\n",
    "* recall@k - did it find all the correct chunks?\n",
    "* ndcg@k - how high are all of the relevant items ranked in the returned list?\n",
    "\n",
    "These individual metrics are the basis for creating an aggregate view of our validation dataset to get a sense for how well it's performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9e79d0e-8a07-450f-a3f1-9ded7650b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Helper class for calculating metrics.\n",
    "class IRMetricsCalculator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        return len(set(relevant) & set(retrieved_k)) / k if k > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def recall_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        return len(set(relevant) & set(retrieved_k)) / len(relevant) if len(relevant) > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def dcg_at_k(relevant, retrieved, k):\n",
    "        retrieved_k = retrieved[:k]\n",
    "        dcg = 0\n",
    "        for i, item in enumerate(retrieved_k):\n",
    "            if item in relevant:\n",
    "                dcg += 1 / np.log2(i + 2)\n",
    "        return dcg\n",
    "\n",
    "    @staticmethod\n",
    "    def ndcg_at_k(relevant, retrieved, k):\n",
    "        dcg = IRMetricsCalculator.dcg_at_k(relevant, retrieved, k)\n",
    "        idcg = IRMetricsCalculator.dcg_at_k(relevant, relevant, k)\n",
    "        return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_json_list(json_string):\n",
    "        try:\n",
    "            return json.loads(json_string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {json_string} with error {e}\")\n",
    "            return []\n",
    "\n",
    "    def calculate_metrics(self, k_values=[1, 3, 5, 10]):\n",
    "        for k in k_values:\n",
    "            self.df[f'precision@{k}'] = self.df.apply(lambda row: self.precision_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "            self.df[f'recall@{k}'] = self.df.apply(lambda row: self.recall_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "            self.df[f'ndcg@{k}'] = self.df.apply(lambda row: self.ndcg_at_k(\n",
    "                self.parse_json_list(row['relevant_doc_ids']),\n",
    "                self.parse_json_list(row['retrieved_doc_ids']), k), axis=1)\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40741563-93ec-45fb-b481-5ac2882b146c",
   "metadata": {},
   "source": [
    "### 1.6 Setup task runner\n",
    "In the step below we setup a task runner that iterates through our dataframe, runs a retrieval task on the input and uses the IRCalculator to generate metrics on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c0f0aeb-2588-4e60-90fd-6e9c17984ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalTaskRunner:\n",
    "    def __init__(self, eval_df: pd.DataFrame, retrieval_task: BaseRetrievalTask):\n",
    "        self.eval_df = eval_df\n",
    "        self.retrieval_task = retrieval_task\n",
    "\n",
    "    def _get_unique_file_paths(self, results: List[RetrievalResult]) -> List[str]:\n",
    "        # Since Python 3.7, dicts retain insertion order.\n",
    "        return list(dict.fromkeys(r.metadata['relative_path'] for r in results))\n",
    "        \n",
    "\n",
    "    def run(self) -> pd.DataFrame:\n",
    "        # Make a copy of the dataframe so we don't modify the original.\n",
    "        df = pd.DataFrame(self.eval_df)\n",
    "        \n",
    "        results = []\n",
    "        for index, row in df.iterrows():\n",
    "            query: str = row['query_text']\n",
    "            \n",
    "            # Run retrieval task\n",
    "            retrieval_results: List[RetrievalResult] = self.retrieval_task.retrieve(query)\n",
    "            \n",
    "            # Extract unique page numbers for comparison with validation dataset.\n",
    "            ordered_filepaths: List[str] = self._get_unique_file_paths(retrieval_results)\n",
    "\n",
    "            retrieved_chunks = [ {'relative_path': r.metadata['relative_path'], 'chunk': r.document} for r in retrieval_results ]\n",
    "\n",
    "            # Create new record\n",
    "            result = {\n",
    "                'query_text': query,\n",
    "                'relevant_doc_ids': row['relevant_doc_ids'],\n",
    "                'retrieved_doc_ids': json.dumps(ordered_filepaths),\n",
    "                'retrieved_chunks': json.dumps(retrieved_chunks), # Best way to preserve the chunks\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        new_dataframe = pd.DataFrame(results)\n",
    "        # return new_dataframe\n",
    "\n",
    "        ir_calc: IRMetricsCalculator = IRMetricsCalculator(new_dataframe)\n",
    "        return ir_calc.calculate_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675e733-783c-4e44-89a3-5a5bf6fec2fa",
   "metadata": {},
   "source": [
    "### 1.7 Run first experiment\n",
    "The command below triggers the first experiment and stores the results in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fdf7d38-b4ff-4718-86e1-40ad219cd6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_1_results: pd.DataFrame = RetrievalTaskRunner(eval_df, experiment_1_retrieval_task).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0877e71-494a-4d60-9528-66312a7097dc",
   "metadata": {},
   "source": [
    "### 1.8 Create the information retrieval experiment summary\n",
    "You can review the results for each individual query, but it doesn't quite tell the whole story. That's why we create a summary view showing Mean Average Precision, Mean Reciprocal Rank (MRR), as well as general averages across all the individual metrics we calculated. This tells us know how well the retrieval task is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d17b963-ea87-4527-aac3-e0060268b749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAP (Mean Average Precision)</td>\n",
       "      <td>0.060185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MRR (Mean Reciprocal Rank)</td>\n",
       "      <td>0.097222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mean Precision@1</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mean Recall@1</td>\n",
       "      <td>0.423611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mean NDCG@1</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mean Precision@3</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mean Recall@3</td>\n",
       "      <td>0.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mean NDCG@3</td>\n",
       "      <td>0.584949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mean Precision@5</td>\n",
       "      <td>0.191667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mean Recall@5</td>\n",
       "      <td>0.673611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mean NDCG@5</td>\n",
       "      <td>0.604373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>% Queries with Relevant Doc in Top 1</td>\n",
       "      <td>54.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>% Queries with Relevant Doc in Top 3</td>\n",
       "      <td>70.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>% Queries with Relevant Doc in Top 5</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Metric      Value\n",
       "0           MAP (Mean Average Precision)   0.060185\n",
       "1             MRR (Mean Reciprocal Rank)   0.097222\n",
       "2                       Mean Precision@1   0.541667\n",
       "3                          Mean Recall@1   0.423611\n",
       "4                            Mean NDCG@1   0.541667\n",
       "5                       Mean Precision@3   0.291667\n",
       "6                          Mean Recall@3   0.638889\n",
       "7                            Mean NDCG@3   0.584949\n",
       "8                       Mean Precision@5   0.191667\n",
       "9                          Mean Recall@5   0.673611\n",
       "10                           Mean NDCG@5   0.604373\n",
       "11  % Queries with Relevant Doc in Top 1  54.166667\n",
       "12  % Queries with Relevant Doc in Top 3  70.833333\n",
       "13  % Queries with Relevant Doc in Top 5  75.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper class to summarize the experiment results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class ExperimentSummarizer:\n",
    "    def __init__(self, df):\n",
    "        self.df = pd.DataFrame(df)\n",
    "        self.summary_df = None\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_ap(relevant_docs, retrieved_docs):\n",
    "        relevant_set = set(relevant_docs.split(','))\n",
    "        retrieved_list = retrieved_docs.split(',')\n",
    "        relevant_count = 0\n",
    "        total_precision = 0\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_list, 1):\n",
    "            if doc in relevant_set:\n",
    "                relevant_count += 1\n",
    "                total_precision += relevant_count / i\n",
    "        \n",
    "        return total_precision / len(relevant_set) if relevant_set else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_reciprocal_rank(relevant_docs, retrieved_docs):\n",
    "        relevant_set = set(relevant_docs.split(','))\n",
    "        retrieved_list = retrieved_docs.split(',')\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_list, 1):\n",
    "            if doc in relevant_set:\n",
    "                return 1 / i\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def calculate_map(self):\n",
    "        self.df['AP'] = self.df.apply(lambda row: self.calculate_ap(row['relevant_doc_ids'], row['retrieved_doc_ids']), axis=1)\n",
    "        return self.df['AP'].mean()\n",
    "\n",
    "    def calculate_mrr(self):\n",
    "        self.df['RR'] = self.df.apply(lambda row: self.calculate_reciprocal_rank(row['relevant_doc_ids'], row['retrieved_doc_ids']), axis=1)\n",
    "        return self.df['RR'].mean()\n",
    "\n",
    "    def calculate_mean_metrics(self):\n",
    "        return self.df[[\n",
    "            'precision@1', 'recall@1', 'ndcg@1',\n",
    "            'precision@3', 'recall@3', 'ndcg@3',\n",
    "            'precision@5', 'recall@5', 'ndcg@5'\n",
    "        ]].mean()\n",
    "\n",
    "    def calculate_top_k_percentages(self):\n",
    "        top_1 = (self.df['precision@1'] > 0).mean() * 100\n",
    "        top_3 = (self.df['precision@3'] > 0).mean() * 100\n",
    "        top_5 = (self.df['precision@5'] > 0).mean() * 100\n",
    "        return top_1, top_3, top_5\n",
    "\n",
    "    def analyze(self):\n",
    "        map_score = self.calculate_map()\n",
    "        mrr_score = self.calculate_mrr()\n",
    "        mean_metrics = self.calculate_mean_metrics()\n",
    "        top_1, top_3, top_5 = self.calculate_top_k_percentages()\n",
    "\n",
    "        self.summary_df = pd.DataFrame({\n",
    "            'Metric': [\n",
    "                'MAP (Mean Average Precision)',\n",
    "                'MRR (Mean Reciprocal Rank)',\n",
    "                'Mean Precision@1', 'Mean Recall@1', 'Mean NDCG@1',\n",
    "                'Mean Precision@3', 'Mean Recall@3', 'Mean NDCG@3',\n",
    "                'Mean Precision@5', 'Mean Recall@5', 'Mean NDCG@5',\n",
    "                '% Queries with Relevant Doc in Top 1',\n",
    "                '% Queries with Relevant Doc in Top 3',\n",
    "                '% Queries with Relevant Doc in Top 5'\n",
    "            ],\n",
    "            'Value': [\n",
    "                map_score,\n",
    "                mrr_score,\n",
    "                mean_metrics['precision@1'], mean_metrics['recall@1'], mean_metrics['ndcg@1'],\n",
    "                mean_metrics['precision@3'], mean_metrics['recall@3'], mean_metrics['ndcg@3'],\n",
    "                mean_metrics['precision@5'], mean_metrics['recall@5'], mean_metrics['ndcg@5'],\n",
    "                top_1, top_3, top_5\n",
    "            ]\n",
    "        })\n",
    "        return self.summary_df\n",
    "\n",
    "    def get_summary(self):\n",
    "        if self.summary_df is None:\n",
    "            self.analyze()\n",
    "        return self.summary_df\n",
    "    \n",
    "# Lets use the class above to create aggregate metrics to see how well the system performs.\n",
    "experiment_1_summary = ExperimentSummarizer(experiment_1_results).analyze()\n",
    "experiment_1_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa7f19-d5de-4dc6-8e48-9e87fb6fa7a7",
   "metadata": {},
   "source": [
    "### 1.9 Takeaways from the information retrieval experiment\n",
    "The results could be a lot better. \n",
    "At this step, you typically assess whether the top k results have relevant data in them. This makes recall@5 arguably the most important metric for this step.\n",
    "\n",
    "However, you cannot ignore the other metrics, as they inform you about other critical aspects of the information retrieval task's performance. For example, in general you want to limit the amount of context you pass back to the model to save on input token cost and minimize latency. Therefore knowing metrics such as precision@1 and precision@5 are useful metrics, as they give you an idea of how well the embeddings are working on their own at ranking the results. \n",
    "\n",
    "**Optional: Experiment with different chunk sizes, embedding models, or rerankers to see if it performs better**\n",
    "\n",
    "It is important to remember that this is just the starting point, typically you would perform many more experiments with different chunk sizes, chunking strategies, embedding models, or rerankers to see if the information retrieval task results improve as demonstrated in the [deep dive for basic RAG evaluation](https://github.com/aws-samples/genai-system-evaluation/tree/main).\n",
    "\n",
    "Once the information retrieval task is delivering good results, it is time to evaluate the answer generation as well as the overall end to end performance of your RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a7787",
   "metadata": {},
   "source": [
    "## 2. End-to-end RAG system evaluation\n",
    "\n",
    "\n",
    "**What metrics should I care about?**\n",
    "\n",
    "For an E2E system, we care about metrics such as context utilization, completeness, conciseness, context relevancy, and clarity.\n",
    "\n",
    "\n",
    "**What will you do?**\n",
    "\n",
    "* Curate a dataset of questions and ground truth answers (we've created one already)\n",
    "* Review/Create a RAG prompt and grading rubric\n",
    "* Setup RAG system\n",
    "* Run the RAG process and the subsequent evaluation process for all of the sets in the validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9278df2",
   "metadata": {},
   "source": [
    "### 2.1 Import validation dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55dd98ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the eval dataset for the end-to-end evaluation\n",
    "eval_df = pd.read_csv('../data/eval-datasets/5_e2e_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3e4c6",
   "metadata": {},
   "source": [
    "### 2.2 Setup retrieval task\n",
    "\n",
    "We will reuse the retrieval task from step 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "077a3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some experiment variables\n",
    "EMBEDDING_MODEL_ID: str = 'amazon.titan-embed-text-v2:0'\n",
    "EXPERIMENT_1_COLLECTION_NAME: str = 'experiment_1_collection'\n",
    "\n",
    "# Initialize the custom embedding function\n",
    "bedrock_ef = CustomBedrockEmbeddingFunction(model_id=EMBEDDING_MODEL_ID, client=bedrock)\n",
    "\n",
    "# Create our retrieval task. All retrieval tasks in this tutorial implement BaseRetrievalTask which has the method retrieve()\n",
    "# If you'd like to extend this to a different retrieval configuration, all you have to do is create a class that that implements\n",
    "# this abstract class and the rest is the same\n",
    "experiment_1_retrieval_task: BaseRetrievalTask = ChromaDBRetrievalTask(\n",
    "    chroma_client = chroma_client, \n",
    "    collection_name = EXPERIMENT_1_COLLECTION_NAME,\n",
    "    embedding_function = bedrock_ef,\n",
    "    chunks = chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e481cb",
   "metadata": {},
   "source": [
    "### 2.3 Setup RAG with Bedrock\n",
    "In the RAGClient, we're making a retrieval call to populate the context. We store it in context for the LLM-As-A-Judge evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "328f1732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBedrockClient:\n",
    "    def __init__(self, bedrock_client, user_prompt: str, system_prompt: str, model_id: str, hyper_params: dict):\n",
    "        self.client = bedrock_client\n",
    "        self.user_prompt = user_prompt\n",
    "        self.system_prompt = system_prompt\n",
    "        self.model_id = model_id\n",
    "        self.hyper_params = hyper_params\n",
    "\n",
    "    def create_chat_payload(self, inputs: dict) -> list[dict]:\n",
    "        prompt = self.user_prompt.format(**inputs)\n",
    "        return [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "    def call(self, messages: list[dict]) -> str:\n",
    "        response = self.client.converse(\n",
    "            modelId=self.model_id,\n",
    "            messages=messages,\n",
    "            inferenceConfig=self.hyper_params,\n",
    "            system=[{\"text\": self.system_prompt}]\n",
    "        )\n",
    "        return response['output']['message']['content'][0]['text']\n",
    "\n",
    "    def call_threaded(self, message_lists: List[List[Dict[str, Any]]]) -> List[str]:\n",
    "        future_to_position = {}\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            for i, request in enumerate(message_lists):\n",
    "                future = executor.submit(self.call, request)\n",
    "                future_to_position[future] = i\n",
    "            \n",
    "            responses = [None] * len(message_lists)\n",
    "            for future in as_completed(future_to_position):\n",
    "                position = future_to_position[future]\n",
    "                try:\n",
    "                    response: str = future.result()\n",
    "                    responses[position] = response\n",
    "                except Exception as exc:\n",
    "                    print(f\"Request at position {position} generated an exception: {exc}\")\n",
    "                    responses[position] = None\n",
    "        return responses\n",
    "\n",
    "class RAGClient(BaseBedrockClient):\n",
    "    def __init__(self, bedrock_client, user_prompt: str, system_prompt: str, model_id: str, hyper_params: dict, retrieval_task: BaseRetrievalTask):\n",
    "        super().__init__(bedrock_client, user_prompt, system_prompt, model_id, hyper_params)\n",
    "        self.retrieval_task = retrieval_task\n",
    "\n",
    "    def extract_response(self, llm_output: str) -> str:\n",
    "        response_match = re.search(r'<response>(.*?)</response>', llm_output, re.DOTALL)\n",
    "        return response_match.group(1).strip() if response_match else \"No response found\"\n",
    "\n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "\n",
    "        message_lists = []\n",
    "        contexts = []  # Store context as it's passed into the prompt\n",
    "        # context_lists = [] # Store context for RAGAS evaluation\n",
    "        for _, row in df.iterrows():\n",
    "            # # Get passages for context\n",
    "            passages: List[RetrievalResult] = self.retrieval_task.retrieve(row[\"query_text\"])\n",
    "            # Combine into single context\n",
    "            context = \"\\n\\n\".join(f\"###File name:\\n{p.metadata}\\n###Passage:\\n{p.document}\" for p in passages)\n",
    "\n",
    "            # Store contexts for downstream dependencies\n",
    "            contexts.append(context)\n",
    "            # context_lists.append(json.dumps([p.chunk for p in passages]))\n",
    "            \n",
    "            # Construct message list using the query text and relevant passages retrieved.\n",
    "            message_lists.append(self.create_chat_payload({\n",
    "                \"query_text\": row[\"query_text\"],\n",
    "                \"context\": context\n",
    "            }))\n",
    "        \n",
    "        responses = self.call_threaded(message_lists)\n",
    "\n",
    "        df['context'] = contexts\n",
    "        # df['context_chunks'] = context_lists\n",
    "        df['llm_response'] = [self.extract_response(r) for r in responses]\n",
    "        return df\n",
    "\n",
    "class EvaluationClient(BaseBedrockClient):\n",
    "    def __init__(self, bedrock_client, user_prompt: str, system_prompt: str, model_id: str, hyper_params: dict):\n",
    "        super().__init__(bedrock_client, user_prompt, system_prompt, model_id, hyper_params)\n",
    "\n",
    "    def extract_score_and_thinking(self, llm_output: str) -> tuple:\n",
    "        thinking_match = re.search(r'<thinking>(.*?)</thinking>', llm_output, re.DOTALL)\n",
    "        score_match = re.search(r'<score>(.*?)</score>', llm_output, re.DOTALL)\n",
    "\n",
    "        thinking = thinking_match.group(1).strip() if thinking_match else \"No thinking found\"\n",
    "        score = float(score_match.group(1)) if score_match else None\n",
    "        \n",
    "        return score, thinking\n",
    "\n",
    "    def evaluate(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        message_lists = [self.create_chat_payload({\n",
    "            \"query_text\": row[\"query_text\"],\n",
    "            \"context\": row[\"context\"],\n",
    "            \"llm_response\": row[\"llm_response\"],\n",
    "            \"ground_truth\": row[\"ground_truth\"]\n",
    "        }) for _, row in df.iterrows()]\n",
    "        \n",
    "        responses = self.call_threaded(message_lists)\n",
    "\n",
    "        llm_scores = []\n",
    "        llm_thinking = []\n",
    "\n",
    "        for response in responses:\n",
    "            if response is not None:\n",
    "                score, thinking = self.extract_score_and_thinking(response)\n",
    "                llm_scores.append(score)\n",
    "                llm_thinking.append(thinking)\n",
    "            else:\n",
    "                llm_scores.append(None)\n",
    "                llm_thinking.append(\"Error occurred during processing\")\n",
    "\n",
    "        df['grade'] = llm_scores\n",
    "        df['reasoning'] = llm_thinking\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca7dcc6",
   "metadata": {},
   "source": [
    "### 2.4 Define RAG prompt\n",
    "Before we evaluate anything, we need to construct a prompt that can take in context and generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b6d0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an advanced AI assistant specialized in Retrieval Augmented Generation (RAG).\n",
    "Your primary function is to provide accurate, concise, and relevant answers based solely on the given context.\n",
    "Follow these guidelines strictly:\n",
    "\n",
    "1. Use only information from the provided context. Do not introduce external knowledge or make assumptions.\n",
    "2. Ensure your answers are complete, addressing all aspects of the question using available information.\n",
    "3. Be extremely concise. Use as few words as possible while maintaining clarity and completeness.\n",
    "4. Maintain 100% accuracy based on the given context. If the context doesn't contain enough information to answer fully, state this clearly.\n",
    "5. Structure your responses for maximum clarity. Use bullet points or numbered lists when appropriate.\n",
    "6. If the context contains technical information, explain it in simple terms as if speaking to a non-technical person.\n",
    "7. Do not apologize or use phrases like \"Based on the context provided\" or \"According to the information given\".\n",
    "8. If asked about something not in the context, simply state \"The provided context does not contain information about [topic].\"\n",
    "\n",
    "Your goal is to achieve the highest possible score on context utilization, completeness, conciseness, accuracy, and clarity.\"\"\"\n",
    "\n",
    "# User Prompt\n",
    "RAG_USER_PROMPT = \"\"\"Answer the following question using only the provided context:\n",
    "\n",
    "<query>\n",
    "{query_text}\n",
    "</query>\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Instructions:\n",
    "1. Read the question and context carefully.\n",
    "2. Formulate a concise and accurate answer based solely on the given context.\n",
    "3. Ensure your response is clear and easily understandable to a non-technical person.\n",
    "4. Do not include any information not present in the context.\n",
    "5. If the context doesn't contain relevant information, state this clearly and concisely.\n",
    "6. Place your response in <response></response> tags.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868ab671",
   "metadata": {},
   "source": [
    "### 2.5 Define evaluation rubric \n",
    "We generate a score from 0-5 (arbitrary) for each of the defined evaluation criteria and we incorporate \"ground truth\" in the evaluation to assess context relevancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d2049a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Prompt\n",
    "RUBRIC_SYSTEM_PROMPT = \"\"\"You are an expert judge evaluating Retrieval Augmented Generation (RAG) applications.\n",
    "Your task is to evaluate given answers based on context and questions using the criteria provided.\n",
    "Evaluation Criteria (Score either 0 or 1 for each, total score is the sum):\n",
    "1. Context Utilization: Does the answer use only information provided in the context, without introducing external or fabricated details?\n",
    "2. Completeness: Does the answer thoroughly address all key elements of the question based on the available context, without significant omissions?\n",
    "3. Conciseness: Does the answer efficiently use words to address the question and avoid unnecessary redundancy?\n",
    "4. Context Relevancy: Is the context returned sufficient to provide an answer like the gold standard answer.\n",
    "5. Clarity: Is the answer easy to understand and follow?\n",
    "Your role is to provide a fair and thorough evaluation for each criterion, explaining your reasoning clearly.\"\"\"\n",
    "\n",
    "# User Prompt\n",
    "RUBRIC_USER_PROMPT = \"\"\"Please evaluate the following RAG response:\n",
    "\n",
    "Question:\n",
    "<query_text>\n",
    "{query_text}\n",
    "</query_text>\n",
    "\n",
    "Ground Truth Answer\n",
    "<llm_response>\n",
    "{ground_truth}\n",
    "</llm_response>\n",
    "\n",
    "\n",
    "Generated answer:\n",
    "<llm_response>\n",
    "{llm_response}\n",
    "</llm_response>\n",
    "\n",
    "Context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Evaluation Steps:\n",
    "1. Carefully read the provided context, question, and answer.\n",
    "2. For each evaluation criterion, assign a score of either 0 or 1:\n",
    "   - Context Utilization\n",
    "   - Completeness\n",
    "   - Conciseness\n",
    "   - Context Relevancy\n",
    "   - Clarity\n",
    "3. Provide a clear explanation for each score, referencing specific aspects of the response.\n",
    "4. Calculate the total score by adding up the points awarded (minimum 0, maximum 5).\n",
    "5. Present your evaluation inside <thinking></thinking> tags.\n",
    "6. Include individual criterion scores (0 or 1) in the thinking tags and the total score inside <score></score> tags.\n",
    "7. Ensure your response is valid XML and provides a comprehensive evaluation.\n",
    "8. Use the ground truth to evaluate whether the information returned was not relevant to answer the question fully. If not, \n",
    "\n",
    "Example Output Format:\n",
    "<thinking>\n",
    "Context Utilization: 1 - The answer strictly uses information from the context without introducing external details.\n",
    "Completeness: 1 - The response covers all key elements of the question based on the available context.\n",
    "Conciseness: 1 - The answer is helpful and doesn't repeat the same information more than once.\n",
    "Context Relevancy: 0 - The context was not relevant to the question.\n",
    "Clarity: 1 - The response is clear and easy to follow.\n",
    "</thinking>\n",
    "<score>4</score>\n",
    "\n",
    "Please provide your detailed evaluation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d2802ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different LLMs by changing the model ID here\n",
    "HAIKU_ID = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "# Initialize RAG Client\n",
    "rag_client: RAGClient = RAGClient(\n",
    "    bedrock,\n",
    "    RAG_USER_PROMPT, \n",
    "    RAG_SYSTEM_PROMPT, \n",
    "    HAIKU_ID,\n",
    "    {\"temperature\": 0.5, \"maxTokens\": 2096},\n",
    "    experiment_1_retrieval_task\n",
    ")\n",
    "\n",
    "# Initialize Eval Client\n",
    "eval_client = EvaluationClient(\n",
    "    bedrock,\n",
    "    RUBRIC_USER_PROMPT, \n",
    "    RUBRIC_SYSTEM_PROMPT, \n",
    "    HAIKU_ID, \n",
    "    {\"temperature\": 0.7, \"maxTokens\": 4096}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "618ec78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RAG responses\n",
    "rag_df = rag_client.process(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd64fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RAG Responses\n",
    "llm_as_a_judge_results_df = eval_client.evaluate(rag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c75101fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Summary View of Results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textwrap import fill\n",
    "\n",
    "class E2EEvaluator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.grades = df['grade'].astype(float)\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        return {\n",
    "            'Mean': np.mean(self.grades),\n",
    "            'Median': np.median(self.grades),\n",
    "            'Standard Deviation': np.std(self.grades),\n",
    "            'Minimum Grade': np.min(self.grades),\n",
    "            'Maximum Grade': np.max(self.grades)\n",
    "        }\n",
    "    \n",
    "    def generate_report(self):\n",
    "        metrics = self.calculate_metrics()\n",
    "        report = \"E2E Validation Result\\n\"\n",
    "        report += \"========================\\n\\n\"\n",
    "        \n",
    "        for metric, value in metrics.items():\n",
    "            report += f\"{metric}: {value:.2f}\\n\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def analyze_grade_distribution(self):\n",
    "        return self.df['grade'].value_counts().sort_index()\n",
    "\n",
    "    def pretty_print_lowest_results(self, n=3, width=80):\n",
    "        lowest_results = self.df.nsmallest(n, 'grade')\n",
    "        for index, row in lowest_results.iterrows():\n",
    "            print(f\"{'='*width}\\n\")\n",
    "            print(f\"Grade: {row['grade']}\\n\")\n",
    "            print(\"Query Text:\")\n",
    "            print(fill(row['query_text'], width=width))\n",
    "            print(\"\\nLLM Response:\")\n",
    "            print(fill(row['llm_response'], width=width))\n",
    "            print(\"\\nReasoning:\")\n",
    "            print(fill(row['reasoning'], width=width))\n",
    "            print(f\"\\n{'='*width}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3b5b9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2E Validation Result\n",
      "========================\n",
      "\n",
      "Mean: 4.46\n",
      "Median: 5.00\n",
      "Standard Deviation: 0.96\n",
      "Minimum Grade: 2.00\n",
      "Maximum Grade: 5.00\n",
      "\n",
      "grade\n",
      "2.0     2\n",
      "3.0     2\n",
      "4.0     3\n",
      "5.0    17\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming your dataframe is named 'df'\n",
    "evaluator = E2EEvaluator(llm_as_a_judge_results_df)\n",
    "\n",
    "# Generate and print the report\n",
    "print(evaluator.generate_report())\n",
    "\n",
    "# Analyze grade distribution\n",
    "print(evaluator.analyze_grade_distribution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "675e3da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_text</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>context</th>\n",
       "      <th>llm_response</th>\n",
       "      <th>grade</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm using version 2.1 of open search and tryin...</td>\n",
       "      <td>The `zstd` and `zstd_no_dict` codecs were intr...</td>\n",
       "      <td>###File name:\\n{'file_type': 'text/markdown', ...</td>\n",
       "      <td>The context indicates that zstd compression is...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The answer directly u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm trying to set up this new aggregate view t...</td>\n",
       "      <td>To ensure you don't break your existing multi-...</td>\n",
       "      <td>###File name:\\n{'last_modified_date': '2025-03...</td>\n",
       "      <td>Important points about the aggregate view for ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The generated answer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What software license does Opensearch use?</td>\n",
       "      <td>According to the context provided, the OpenSea...</td>\n",
       "      <td>###File name:\\n{'file_type': 'text/markdown', ...</td>\n",
       "      <td>Apache License, Version 2.0. All components of...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The answer directly u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Does GPU accelerated nodes support Pytorch?</td>\n",
       "      <td>Yes, according to the documentation, GPU-accel...</td>\n",
       "      <td>###File name:\\n{'relative_path': '_ml-commons-...</td>\n",
       "      <td>Yes, GPU-accelerated nodes support PyTorch. Sp...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The answer directly u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Does opensearch support hugging face models? I...</td>\n",
       "      <td>Yes, OpenSearch supports various Hugging Face ...</td>\n",
       "      <td>###File name:\\n{'relative_path': '_vector-sear...</td>\n",
       "      <td>Yes, OpenSearch supports Hugging Face models, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Context Utilization: 0 - While the answer uses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I have a custom model, can I run it in Opensea...</td>\n",
       "      <td>Yes, OpenSearch supports running custom local ...</td>\n",
       "      <td>###File name:\\n{'file_path': '/Users/huthmac/D...</td>\n",
       "      <td>Yes, you can run a custom model in OpenSearch....</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The generated answer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I have a model and some ML nodes, how do I boo...</td>\n",
       "      <td>Based on the context provided, to boost the pe...</td>\n",
       "      <td>###File name:\\n{'file_size': 12788, 'last_modi...</td>\n",
       "      <td>To boost your ML model's performance:\\n\\n1. Us...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The generated answer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Can you show me an example of how to use lat/l...</td>\n",
       "      <td>Yes, the context provides several examples of ...</td>\n",
       "      <td>###File name:\\n{'creation_date': '2025-09-22',...</td>\n",
       "      <td>Here are several ways to use latitude/longitud...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The answer directly u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How do I use vector search?</td>\n",
       "      <td>According to the provided context, there are t...</td>\n",
       "      <td>###File name:\\n{'file_size': 3509, 'last_modif...</td>\n",
       "      <td>To use vector search:\\n\\n1. Understand the bas...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Context Utilization: 0 - The generated answer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I understand the memory requirements fo...</td>\n",
       "      <td>The memory requirements for using HNSW (Hierar...</td>\n",
       "      <td>###File name:\\n{'creation_date': '2025-09-22',...</td>\n",
       "      <td>To understand HNSW memory requirements, use th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 \\n- The answer directly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Can you show me some different examples of usi...</td>\n",
       "      <td>Sure, here are some examples of using differen...</td>\n",
       "      <td>###File name:\\n{'file_name': 'knn-vector-quant...</td>\n",
       "      <td>Here are different quantization methods for ve...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Context Utilization: 1 - The answer uses infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Would you recommend I use ReRank? If so, what ...</td>\n",
       "      <td>Based on the context provided, I don't have en...</td>\n",
       "      <td>###File name:\\n{'relative_path': '_search-plug...</td>\n",
       "      <td>Yes, I recommend using ReRank. Based on the co...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The answer directly u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I set up a tiered cache with an on-heap...</td>\n",
       "      <td>To set up a tiered cache with an on-heap and d...</td>\n",
       "      <td>###File name:\\n{'last_modified_date': '2025-03...</td>\n",
       "      <td>To set up a tiered cache with on-heap and disk...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Context Utilization: 1 - The generated answer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I need to combine relevance scores from differ...</td>\n",
       "      <td>Based on the context provided, it seems that y...</td>\n",
       "      <td>###File name:\\n{'file_path': '/Users/huthmac/D...</td>\n",
       "      <td>To combine relevance scores from different typ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 \\n- The answer draws di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What metrics does query metrics give me out of...</td>\n",
       "      <td>According to the documentation, query metrics ...</td>\n",
       "      <td>###File name:\\n{'file_size': 3321, 'file_type'...</td>\n",
       "      <td>Query metrics provide the following measuremen...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The answer uses only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Does opensearch work with open telemetry?</td>\n",
       "      <td>Yes, OpenSearch does work with OpenTelemetry (...</td>\n",
       "      <td>###File name:\\n{'file_size': 11585, 'file_type...</td>\n",
       "      <td>Yes, OpenSearch works with OpenTelemetry. The ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The generated answer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How do I update an existing Slack channel conf...</td>\n",
       "      <td>To update an existing Slack channel configurat...</td>\n",
       "      <td>###File name:\\n{'file_path': '/Users/huthmac/D...</td>\n",
       "      <td>To update an existing Slack channel configurat...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Context Utilization: 0 - While the answer uses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What specific Java permission is required to r...</td>\n",
       "      <td>According to the example output when installin...</td>\n",
       "      <td>###File name:\\n{'file_size': 17560, 'file_type...</td>\n",
       "      <td>The specific Java permission required for regi...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The answer directly u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What environment variable and YAML structure m...</td>\n",
       "      <td>According to the context provided, to customiz...</td>\n",
       "      <td>###File name:\\n{'relative_path': '_security/co...</td>\n",
       "      <td>For Helm charts in OpenSearch 2.12 or later, y...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The answer directly u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I group my data by a nested field but ...</td>\n",
       "      <td>Based on the context provided, you can use the...</td>\n",
       "      <td>###File name:\\n{'file_size': 5033, 'file_name'...</td>\n",
       "      <td>The context provides insights into nested and ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Context Utilization: 0 - While the answer uses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Make it simpler like what happens when I have ...</td>\n",
       "      <td>When you have missing values in an aggregation...</td>\n",
       "      <td>###File name:\\n{'relative_path': '_aggregation...</td>\n",
       "      <td>When you have missing values in an aggregation...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The answer uses infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Can you explain compound queries to me?</td>\n",
       "      <td>Compound queries in Elasticsearch are used to ...</td>\n",
       "      <td>###File name:\\n{'relative_path': '_query-dsl/c...</td>\n",
       "      <td>Compound queries are special search queries th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The answer uses infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What is the difference between vector search a...</td>\n",
       "      <td>The context provided does not mention vector s...</td>\n",
       "      <td>###File name:\\n{'relative_path': '_query-dsl/s...</td>\n",
       "      <td>The key differences between vector search and ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Context Utilization: 0 - While the answer prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Why can't my coworker see my async searches, b...</td>\n",
       "      <td>Based on the context provided, it seems that t...</td>\n",
       "      <td>###File name:\\n{'file_name': 'security.md', 'f...</td>\n",
       "      <td>The context explains that asynchronous search ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Context Utilization: 1 - The generated answer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           query_text  \\\n",
       "0   I'm using version 2.1 of open search and tryin...   \n",
       "1   I'm trying to set up this new aggregate view t...   \n",
       "2         What software license does Opensearch use?    \n",
       "3         Does GPU accelerated nodes support Pytorch?   \n",
       "4   Does opensearch support hugging face models? I...   \n",
       "5   I have a custom model, can I run it in Opensea...   \n",
       "6   I have a model and some ML nodes, how do I boo...   \n",
       "7   Can you show me an example of how to use lat/l...   \n",
       "8                         How do I use vector search?   \n",
       "9   How do I understand the memory requirements fo...   \n",
       "10  Can you show me some different examples of usi...   \n",
       "11  Would you recommend I use ReRank? If so, what ...   \n",
       "12  How do I set up a tiered cache with an on-heap...   \n",
       "13  I need to combine relevance scores from differ...   \n",
       "14  What metrics does query metrics give me out of...   \n",
       "15          Does opensearch work with open telemetry?   \n",
       "16  How do I update an existing Slack channel conf...   \n",
       "17  What specific Java permission is required to r...   \n",
       "18  What environment variable and YAML structure m...   \n",
       "19  How can I group my data by a nested field but ...   \n",
       "20  Make it simpler like what happens when I have ...   \n",
       "21           Can you explain compound queries to me?    \n",
       "22  What is the difference between vector search a...   \n",
       "23  Why can't my coworker see my async searches, b...   \n",
       "\n",
       "                                         ground_truth  \\\n",
       "0   The `zstd` and `zstd_no_dict` codecs were intr...   \n",
       "1   To ensure you don't break your existing multi-...   \n",
       "2   According to the context provided, the OpenSea...   \n",
       "3   Yes, according to the documentation, GPU-accel...   \n",
       "4   Yes, OpenSearch supports various Hugging Face ...   \n",
       "5   Yes, OpenSearch supports running custom local ...   \n",
       "6   Based on the context provided, to boost the pe...   \n",
       "7   Yes, the context provides several examples of ...   \n",
       "8   According to the provided context, there are t...   \n",
       "9   The memory requirements for using HNSW (Hierar...   \n",
       "10  Sure, here are some examples of using differen...   \n",
       "11  Based on the context provided, I don't have en...   \n",
       "12  To set up a tiered cache with an on-heap and d...   \n",
       "13  Based on the context provided, it seems that y...   \n",
       "14  According to the documentation, query metrics ...   \n",
       "15  Yes, OpenSearch does work with OpenTelemetry (...   \n",
       "16  To update an existing Slack channel configurat...   \n",
       "17  According to the example output when installin...   \n",
       "18  According to the context provided, to customiz...   \n",
       "19  Based on the context provided, you can use the...   \n",
       "20  When you have missing values in an aggregation...   \n",
       "21  Compound queries in Elasticsearch are used to ...   \n",
       "22  The context provided does not mention vector s...   \n",
       "23  Based on the context provided, it seems that t...   \n",
       "\n",
       "                                              context  \\\n",
       "0   ###File name:\\n{'file_type': 'text/markdown', ...   \n",
       "1   ###File name:\\n{'last_modified_date': '2025-03...   \n",
       "2   ###File name:\\n{'file_type': 'text/markdown', ...   \n",
       "3   ###File name:\\n{'relative_path': '_ml-commons-...   \n",
       "4   ###File name:\\n{'relative_path': '_vector-sear...   \n",
       "5   ###File name:\\n{'file_path': '/Users/huthmac/D...   \n",
       "6   ###File name:\\n{'file_size': 12788, 'last_modi...   \n",
       "7   ###File name:\\n{'creation_date': '2025-09-22',...   \n",
       "8   ###File name:\\n{'file_size': 3509, 'last_modif...   \n",
       "9   ###File name:\\n{'creation_date': '2025-09-22',...   \n",
       "10  ###File name:\\n{'file_name': 'knn-vector-quant...   \n",
       "11  ###File name:\\n{'relative_path': '_search-plug...   \n",
       "12  ###File name:\\n{'last_modified_date': '2025-03...   \n",
       "13  ###File name:\\n{'file_path': '/Users/huthmac/D...   \n",
       "14  ###File name:\\n{'file_size': 3321, 'file_type'...   \n",
       "15  ###File name:\\n{'file_size': 11585, 'file_type...   \n",
       "16  ###File name:\\n{'file_path': '/Users/huthmac/D...   \n",
       "17  ###File name:\\n{'file_size': 17560, 'file_type...   \n",
       "18  ###File name:\\n{'relative_path': '_security/co...   \n",
       "19  ###File name:\\n{'file_size': 5033, 'file_name'...   \n",
       "20  ###File name:\\n{'relative_path': '_aggregation...   \n",
       "21  ###File name:\\n{'relative_path': '_query-dsl/c...   \n",
       "22  ###File name:\\n{'relative_path': '_query-dsl/s...   \n",
       "23  ###File name:\\n{'file_name': 'security.md', 'f...   \n",
       "\n",
       "                                         llm_response  grade  \\\n",
       "0   The context indicates that zstd compression is...    5.0   \n",
       "1   Important points about the aggregate view for ...    5.0   \n",
       "2   Apache License, Version 2.0. All components of...    5.0   \n",
       "3   Yes, GPU-accelerated nodes support PyTorch. Sp...    5.0   \n",
       "4   Yes, OpenSearch supports Hugging Face models, ...    3.0   \n",
       "5   Yes, you can run a custom model in OpenSearch....    5.0   \n",
       "6   To boost your ML model's performance:\\n\\n1. Us...    5.0   \n",
       "7   Here are several ways to use latitude/longitud...    5.0   \n",
       "8   To use vector search:\\n\\n1. Understand the bas...    2.0   \n",
       "9   To understand HNSW memory requirements, use th...    5.0   \n",
       "10  Here are different quantization methods for ve...    4.0   \n",
       "11  Yes, I recommend using ReRank. Based on the co...    5.0   \n",
       "12  To set up a tiered cache with on-heap and disk...    4.0   \n",
       "13  To combine relevance scores from different typ...    5.0   \n",
       "14  Query metrics provide the following measuremen...    5.0   \n",
       "15  Yes, OpenSearch works with OpenTelemetry. The ...    5.0   \n",
       "16  To update an existing Slack channel configurat...    4.0   \n",
       "17  The specific Java permission required for regi...    5.0   \n",
       "18  For Helm charts in OpenSearch 2.12 or later, y...    5.0   \n",
       "19  The context provides insights into nested and ...    3.0   \n",
       "20  When you have missing values in an aggregation...    5.0   \n",
       "21  Compound queries are special search queries th...    5.0   \n",
       "22  The key differences between vector search and ...    2.0   \n",
       "23  The context explains that asynchronous search ...    5.0   \n",
       "\n",
       "                                            reasoning  \n",
       "0   Context Utilization: 1 - The answer directly u...  \n",
       "1   Context Utilization: 1 - The generated answer ...  \n",
       "2   Context Utilization: 1 - The answer directly u...  \n",
       "3   Context Utilization: 1 - The answer directly u...  \n",
       "4   Context Utilization: 0 - While the answer uses...  \n",
       "5   Context Utilization: 1 - The generated answer ...  \n",
       "6   Context Utilization: 1 - The generated answer ...  \n",
       "7   Context Utilization: 1 - The answer directly u...  \n",
       "8   Context Utilization: 0 - The generated answer ...  \n",
       "9   Context Utilization: 1 \\n- The answer directly...  \n",
       "10  Context Utilization: 1 - The answer uses infor...  \n",
       "11  Context Utilization: 1 - The answer directly u...  \n",
       "12  Context Utilization: 1 - The generated answer ...  \n",
       "13  Context Utilization: 1 \\n- The answer draws di...  \n",
       "14  Context Utilization: 1 - The answer uses only ...  \n",
       "15  Context Utilization: 1 - The generated answer ...  \n",
       "16  Context Utilization: 0 - While the answer uses...  \n",
       "17  Context Utilization: 1 - The answer directly u...  \n",
       "18  Context Utilization: 1 - The answer directly u...  \n",
       "19  Context Utilization: 0 - While the answer uses...  \n",
       "20  Context Utilization: 1 - The answer uses infor...  \n",
       "21  Context Utilization: 1 - The answer uses infor...  \n",
       "22  Context Utilization: 0 - While the answer prov...  \n",
       "23  Context Utilization: 1 - The generated answer ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the results and spot check them\n",
    "llm_as_a_judge_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea01bfde",
   "metadata": {},
   "source": [
    "### 2.6 E2E test results\n",
    "The E2E test results are pretty good! However, it doesn't account for scenarios where you simply don't have the correct context. \n",
    "\n",
    "In this notebook we combined our embeddings and prompt together to run E2E tests on our entire RAG system. \n",
    "Based on our findings, document level chunking worked very well for this use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81a8cf",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "By adding validation at each touchpoint in the RAG system, we can get a comprehensive view of what's happening and where the bottlenecks to better performance are. \n",
    "\n",
    "Another important takeaway is that how you chunk your data matters (aguably) more than what model you choose to vend the RAG results. We only explored a very basic chunking strategy and did not cover more advanced retrieval strategies or other important components such as different embedding models, rerankers that could unlock greater performance. If you want to dive deeper into any of these other evaluation touch points, please refer to the [GenAI System Evaluation repository](https://github.com/aws-samples/genai-system-evaluation/tree/main).\n",
    "\n",
    "Lastly, no dataset or information retrieval problem is exactly the same. It's important to evaluate it to understand how your RAG system is performing and update your chunking and validation sets over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
